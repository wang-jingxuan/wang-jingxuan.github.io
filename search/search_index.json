{"config":{"lang":["en","ja"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome! I'm Jingxuan (Jocelyn) Wang.","text":""},{"location":"#education-experience","title":"Education &amp; Experience","text":"<p>I learned at Tsinghua University (undergraduate) and the University of Hong Kong (Master). Through my studies/internships, I've explored finance, biomedical engineering, and I\u2019m always curious about how data shapes decision-making. (View my CV)</p>"},{"location":"#fun-facts","title":"Fun Facts","text":"<ul> <li>I love spicy food\u2014the kind makes you question your life choices but keeps you coming back for more.</li> <li>I once volunteered at the Beijing Winter Olympics, which was a great mimicking of snowball.</li> <li>I have a habit of trying new Milk Tea shops but always end up ordering the same thing.</li> </ul>"},{"location":"#lets-connect","title":"Let\u2019s Connect","text":"<p>Happy to connect! You can reach me via email or LinkedIn. Spicy food recommendations are always welcome!</p>"},{"location":"AboutMe/","title":"About Me","text":"<p>To be continued...</p>"},{"location":"CV/","title":"CV","text":"<p>Download PDF</p> <p>Last updated: Mar 18, 2025</p>"},{"location":"Programming/","title":"Programming with Python","text":""},{"location":"Programming/MachineLearning/","title":"Machine Learning","text":""},{"location":"Programming/MachineLearning/mnist_image_classification_DNN/","title":"Mnist Image Classification by DNN","text":""},{"location":"Programming/MachineLearning/mnist_image_classification_DNN/#1-fire-up","title":"1. Fire Up","text":"<pre><code>import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport tensorflow as tf\nfrom tensorflow import keras\nimport sympy\nfrom sympy import Matrix\n</code></pre>"},{"location":"Programming/MachineLearning/mnist_image_classification_DNN/#2-loss-functions","title":"2. Loss Functions","text":"<pre><code>out = torch.FloatTensor([[0.1, 0.2, 0.7], [0.2, 0.3, 0.5]]).reshape(2, 3)\nprint(out)\nlabel = torch.LongTensor([2, 1])\nprint(label)\n</code></pre> <pre><code>loss_function = n.CrossEntropyLoss()\nprint(loss_function(out, label))\n</code></pre> <pre><code>a = p.log (np.exp(0.1) + np.exp(0.7) + np.exp(0.2)) - 0.7\nprint(a)\nb = np.log(np.exp(0.2) + np.exp(0.3) + np.exp(0.5)) - 0.3\nprint (b)\nprint(\"loss =\", (a + b) / 2)\n</code></pre>"},{"location":"Programming/MachineLearning/mnist_image_classification_DNN/#3-data-generating","title":"3. Data Generating","text":"<pre><code>mnist_dataset = keras.datasets.mnist\n(data_train, label_train), (data_test, label_test) = mist_dataset.load_data()\n</code></pre> <pre><code># Number of Training Samples\nprint(data_train.__len__())\nprint (label_train.__len__())\n# Number of Test Samples\nprint(data_test.__len__())\nprint(label_test.__len__())\n</code></pre> <pre><code>Matrix(data_train[0])\n</code></pre> <pre><code>plt.figure()\n\nplt.subplot(121)\nplt.imshow(data_train[0])\nprint(\"The label for image[0] is\", label_train[0])\n\nplt.subplot(122)\nplt.imshow(data_train[59999])\nprint (\"The label for image[59999] is\", label_train [59999]) \n\nplt.show()\n</code></pre> <pre><code># Convert numpy.ndarry data to tensor type in Pytorch\ndata_train = Variable(torch.FloatTensor(data_train)) / 255.0\ndata_test = Variable(torch.FloatTensor(data_test)) / 255.0\nlabel_train = Variable(torch.LongTensor(label_train))\nlabel_test = Variable(torch.LongTensor(label_test))\n</code></pre> <pre><code># Convert a 28 \u00d7 28 Matrix into a 784 \u00d7 1 Vector \ndata_train[0].reshape(1, -1).size()\n</code></pre>"},{"location":"Programming/MachineLearning/mnist_image_classification_DNN/#4-model-training","title":"4. Model Training","text":"<pre><code>class DeepNeuralNetworkModel(nn.Module):\n    # Constructor of the class\n    def __init__(self, input_dim1, output_dim1, input_dim2, output_dim2,\n               input_dim3, output_dim3):\n        super(DeepNeuralNetworkModel, self).__init__()\n\n        # Fully Connected Layer 1\n        self.FC_layer1 = nn.Linear(input_dim1, output_dim1)\n        #nn.init.constant_(self.FC_layer1.weight, 0.1)\n        #nn.init.constant_(self.FC_layer1.bias, -0.1)\n\n        # Fully Connected Layer 2\n        self.FC_layer2 = nn.Linear(input_dim2, output_dim2)\n\n        # Fully Connected Layer 3\n        self.FC_layer3 = nn.Linear(input_dim3, output_dim3)\n\n        # Activation Function Sigmoid()\n        self.act_sig = nn.Sigmoid()\n\n        # Activation Function ReLU()\n        self.act_relu = nn.ReLU()\n\n    # Forward propagation function\n    def forward(self, x):    # dim of x: N \u00d7 input_diml\n\n        z1_ = self.FC_layer1(x)\n        z1 = self.act_sig(z1_)\n\n        z2_ = self.FC_layer2(z1)\n        z2 = self.act_sig(z2_)\n\n        z3_ = self.FC_layer3(z2)\n        # z3 = self.act_relu(z3_)\n\n        return z3\n</code></pre> <pre><code>alpha = 0.5\nDNN_Model = DeepNeuraletworkModel(784, 128, 128, 64, 64, 10)\noptimizer = torch.optim.SGD(DNN_Model.parameters(), lr = alpha)\nloss_function = nn.CrossEntropyLoss()\n\n# Dynamically Change the learning rate\ndef adjust_learning_rate(optimizer, epoch):\n    if epoch &lt;= 100:\n        lr = alpha\n\n    elif epoch &gt; 100:\n        lr = alpha / (1 + 0.01 * (epoch - 100))\n\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n</code></pre> <pre><code>epochs = 10000\nbatch size = 10000\nbatches = int((data_train.__Len__ ()) / batch size)\ncol = 28 * 28\nloss_list = []\n</code></pre> <pre><code>for epoch in range(epochs):\n    for batch in range(batches):\n\n        # Get the input data matrix: dim = 100 \u00d7 784\n        input_data = Variable(torch. FloatTensor(batch_size, col))\n        for j in range(batch_size):\n            input_data[j] = data_train[j + batch_size * batch].reshape(1, -1)\n\n            # Forward propagation: output_data has dim = 100 \u00d7 10\n            output_data = DNN_Model.forward(input_data)\n\n            # Compute cross entropy loss\n            loss = loss_function(output_data, label_train[batch_size * batch : batch_size * (batch + 1)])\n\n            # backward propagation\n            loss.backward()\n\n            # update parameters\n            optimizer.step()\n\n            # Reset grad to 0\n            optimizer.zero_grad()\n\n            # Save loss for this batch\n            loss_list.append(loss)\n\n            # Print details for the gradient descent\n            if(epoch) % 10 == 0 and (batch + 1) % 1 == 0:\n                print(\"epoch =\", epoch + 1, \"; \", \"batch =\", batch + 1, \":\")\n                print(\"      -&gt; Now loss =\", loss.item())\n                print(\"-------------------------------------------------------------\")\n\n        adjust_learning_rate(optimizer, epoch) \n\n        if(epoch) % 10 == 0:\n            print(\"*********************** Epoch\",epoch + 1, \"Over **********************\")\n            print(\" \")\n            print(\" \")\n\n        if loss &lt; 0.74:\n            break\n</code></pre>"},{"location":"Programming/MachineLearning/mnist_image_classification_DNN/#5-visualization-of-the-cross-entropy-loss-function","title":"5. Visualization of the Cross Entropy Loss Function","text":"<pre><code>plt.figure(figsize = (14, 6))\nlength = loss_list.__len__()\nprint(\"The length of loss_list is:\", length) \nplt.plot(np.arange (1, length + 1, 1), loss_list, \"black\") \nplt.xlabel(\"epoch\") \nplt.ylabel (\"loss\") \nplt.show()\n</code></pre>"},{"location":"Programming/MachineLearning/mnist_image_classification_DNN/#6-prediction-on-the-test-set-and-model-evaluation","title":"6. Prediction on the Test Set and Model Evaluation","text":"<pre><code>data_test.__len__()\n</code></pre> <pre><code>plt.imshow(data_test[0])\nprint(\"The label of this image is:\", label_test[0])\n</code></pre> <pre><code>pred_vec = DNN_Model.forward(data_test[0]. reshape(1, -1))\nprint(\"Prediction for data_test[0]:\")\nMatrix(pred_vec.detach().numpy())\n</code></pre> <pre><code>proba_distribution = F.softmax(DNN_Model.forward(data_test[0].reshape(1, -1)), dim = 1)\nprint(\"Probability distribution for the prediction of data_test[0]:\")\nprint(\"    -&gt;The argmax of the probability distribution vector is:\", \n      torch.argmax(proba_distribution).detach().numpy())\nprint(\"    -&gt;Sum of the probability distribution vector is:\", torch.sum(proba_distribution).detach().numpy())\nMatrix(proba_distribution.detach().numpy())\n</code></pre> <pre><code>pred = []\nfor i in range(data_test.__len__()):\n    temp_distribution = F.softmax(DNN_Model.forward(data_test[i].reshape(1, -1)), dim = 1)\n    pred.append(torch.argmax(temp_distribution).detach().numpy ())\n</code></pre> <pre><code>print(\"The accuracy score is:\", 100.0 * accuracy_score(label_test, pred), \"%\")\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_DNN_strategy/","title":"Pytorch DNN Stategy","text":""},{"location":"Programming/MachineLearning/pytorch_DNN_strategy/#1-fire-up","title":"1. Fire Up","text":"<pre><code>import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport sympy\nfrom sympy import Matrix\nimport tushare as ts\n\n%config InlineBackend. figure_format = \"svg\" # In order to make the figures clearly shown in the notebook\n</code></pre> <pre><code>import tensorflow as tf\nfrom tensorflow import keras\n</code></pre> <pre><code>df = ts.lpr_ma_data() #\u53d6\u5f53\u524d\u5e74\u4efd\u7684\u6570\u636e\n#df = ts.lpr_ma_data(2014) #\u53d62014\u5e74\u7684\u6570\u636e\n</code></pre> <pre><code>df\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_DNN_strategy/#2-data-generating","title":"2. Data Generating","text":"<pre><code>data = ts.get_k_data('hs300', start = '2014-07-01', end = '2017-03-20')\ndata.set_index(\"date\", inplace = True)\ndata\n</code></pre> <pre><code>\u672c\u63a5\u53e3\u5373\u5c06\u505c\u6b62\u66f4\u65b0\uff0c\u8bf7\u5c3d\u5feb\u4f7f\u7528Pro\u7248\u63a5\u53e3\uff1ahttps://tushare.pro/document/2\n\n\n\n---------------------------------------------------------------------------\n\nAttributeError                            Traceback (most recent call last)\n\n/var/folders/j0/k9_1_n411h3ct9_rf7__b9hh0000gn/T/ipykernel_7429/3567945227.py in ?()\n----&gt; 1 data = ts.get_k_data('hs300', start = '2014-07-01', end = '2017-03-20')\n      2 data.set_index(\"date\", inplace = True)\n      3 data\n\n\n/opt/anaconda3/lib/python3.9/site-packages/tushare/stock/trading.py in ?(code, start, end, ktype, autype, index, retry_count, pause)\n    702     else:\n    703         raise TypeError('ktype input error.')\n    704     data = pd.DataFrame()\n    705     for url in urls:\n--&gt; 706         data = data.append(_get_k_data(url, dataflag, \n    707                                        symbol, code,\n    708                                        index, ktype,\n    709                                        retry_count, pause),\n\n\n/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py in ?(self, name)\n   5985             and name not in self._accessors\n   5986             and self._info_axis._can_hold_identifiers_and_holds_name(name)\n   5987         ):\n   5988             return self[name]\n-&gt; 5989         return object.__getattribute__(self, name)\n\n\nAttributeError: 'DataFrame' object has no attribute 'append'\n</code></pre> <pre><code>data_cleaned = pd.DataFrame()\ndata_cleaned[\"Close0\"] = np.log(data[\"close\"])\ndata_cleaned[\"H_L\"] = data[\"high\"] - data[\"low\"]\ndata_cleaned[\"C_0\"] = data[\"close\"] - data [\"open\"]\ndata_cleaned[\"volume\"] = np.log10(data[\"volume\"])\n\ndata_cleaned[\"Closel\"] = data_cleaned[\"Close0\"].shift(1)\ndata_cleaned[\"Close2\"] = data_cleaned[\"Close0\"].shift(2)\ndata_cleaned[\"Close3\"] = data_cleaned[\"Close0\"].shift(3)\ndata_cleaned[\"Close4\"] = data_cleaned[\"Close0\"].shift(4)\ndata_cleaned[\"Close5\"] = data_cleaned[\"Close0\"].shift(5)\n\ndata_cleaned[\"H_L1\"] = data_cleaned[\"H_L\"].shift(1)\ndata_cleaned[\"H_L2\"] = data_cleaned[\"H_L\"].shift(2)\ndata_cleaned[\"H_L3\"] = data_cleaned[\"H_L\"].shift(3)\ndata_cleaned[\"H_L4\"] = data_cleaned[\"H_L\"].shift(4)\ndata_cleaned[\"H_L5\"] = data_cleaned[\"H_L\"].shift(5)\n\ndata_cleaned[\"C_01\"] = data_cleaned[\"C_0\"].shift(1)\ndata_cleaned[\"C_02\"] = data_cleaned[\"C_0\"].shift(2)\ndata_cleaned[\"C_03\"] = data_cleaned[\"C_0\"].shift(3)\ndata_cleaned[\"C_04\"] = data_cleaned[\"C_0\"].shift(4)\ndata_cleaned[\"C_05\"] = data_cleaned[\"C_0\"].shift(5)\n\ndata_cleaned[\"volumel\"] = data_cleaned[\"volume\"].shift(1)\ndata_cleaned[\"volume2\"] = data_cleaned[\"volume\"].shift(2)\ndata_cleaned[\"volume3\"] = data_cleaned[\"volume\"].shift(3)\ndata_cleaned[\"volume4\"] = data_cleaned[\"volume\"].shift(4)\ndata_cleaned[\"volume5\"] = data_cleaned[\"volume\"].shift(5)\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\nCell In[5], line 2\n      1 data_cleaned = pd.DataFrame()\n----&gt; 2 data_cleaned[\"Close0\"] = np.log(data[\"close\"])\n      3 data_cleaned[\"H_L\"] = data[\"high\"] - data[\"low\"]\n      4 data_cleaned[\"C_0\"] = data[\"close\"] - data [\"open\"]\n\n\nNameError: name 'data' is not defined\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_DNN_strategy/#remarks","title":"Remarks","text":"<p>Shortcomings of DNN strategy</p> <ul> <li>Cannot represent all the historical data in the model.</li> </ul> <p>Add long data window? Maybe it will work, but we have better instruments.</p> <ul> <li>RNN: GRU/ LSTM</li> </ul> <p>Shortcomings of the feature engineering</p> <ul> <li> <p>Only contains price data: technical analysis</p> </li> <li> <p>Many data can be further introduced</p> </li> </ul> <p>Fundamental data</p> <p>text data: (NLP) sentiment analysis, sequence models(RNN: GRU/ LSTM, BRNN)</p> <p>Many practitioners in China are finding good factors according to a wealth of data available to them.</p> <p>Shortcomings of the idea</p> <ul> <li> <p>Frequently trading will cause large trading cost</p> </li> <li> <p>No risk management</p> </li> <li> <p>Only trade one asset(HS300), cannot be applied to portfolio management</p> </li> <li> <p>Short selling</p> </li> </ul> <p>Other considerations</p> <ul> <li> <p>Write all your strategies into a class</p> </li> <li> <p>Most code in the financial industry is OOP</p> </li> <li> <p>My investment philosophy: fundamental or event-driven(NLP and sentiment analysis)</p> </li> </ul>"},{"location":"Programming/MachineLearning/pytorch_SVM_algorithm/","title":"Support Vector Machine(SVM) Algorithm","text":""},{"location":"Programming/MachineLearning/pytorch_SVM_algorithm/#1-fire-up","title":"1. Fire up","text":"<pre><code># import sklearn SVC\n\nimport torch\nfrom torch.autograd import Variable \nimport torch.nn as nn \nimport torch.nn.functional as F \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nfrom sklearn.metrics import accuracy_score \nfrom torchvision import datasets \nfrom torchvision import transforms \nimport tensorflow as tf \nfrom tensorflow import keras \nimport sympy\nfrom sympy import Matrix\nfrom numpy import sin, cos, pi, tanh, exp\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_SVM_algorithm/#2-training-data-generation","title":"2. Training data generation","text":"<p>Why 2-dim data? For support vector visualization</p> <pre><code>max r = 0.45\nx1 = []\nx2 = []\ny = []\nfor i in range(20):\n    r = max_r * np.random.rand()\n    rand_num = np.random.rand ()\n    x1.append(0.5 + r * cos(2 * pi * rand_num))\n    x2.append(0.5 + r * sin(2 * pi * rand_num))\n    y.append(1)\n\nfor i in range(20):\n    r = max_r * np.random.rand()\n    rand_num = np.random.rand()\n    x1.append(-0.5 + r * cos(2 * pi * rand_num))\n    x2.append(-0.5 + r * sin(2 * pi * rand_num))\n    y.append (-1)\n\ndict1 = {\"xl\": x1, \"x2\": x2}\ntraining_X = pd.DataFrame(dict1)\ndict2 = {\"y\": y}\ntraining_y = pd.DataFrame(dict2)\ntraining_X.head(5)\n</code></pre> <pre><code>training_y.tail(5)\n</code></pre> <pre><code>plt. figure(figsize = (10, 5))\n\nplt. scatter(x1[0 : 20], x2[0 : 20], color = \"black\", label = \"Class 1\")\n#plt.scatter(x1[5], x2[5], color = \"blue\", label = \"Class 1 S V 1\")\n#plt.scatter(x1[10], x2[10], cotor =\u201cbtuel', label = \"Class 1 S V 2\")\n\nplt.scatter(x1[20: 40], x2[20 : 40], color = \"red\", label = \"Class -1\")\n#plt.scatter(x1[39], x2[39], color = \"pink\", label = \"Class -1 S V 1\")\n\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_SVM_algorithm/#3-kernel-fuction-design","title":"3. Kernel fuction design","text":"<pre><code>def K(vecl, vecz, kernel_function = \"rbf\"\uff0c Gamma = 0.2, r = 0.2):#vecl and vec 2 are transformed into row vect\n    vec1 = np.array(vec1).reshape(1,-1)\n    vec2 = np.array(vec2).reshape(1,-1)\n\n    if kernel_function == \"rbf\":\n        return torch.FloatTensor([np.exp(- Gamma * np.linalg.norm(vec1 - vec2, ord = 2) ** 2)])\n\n    elif kernel_fuction == \"sigmoid\":\n        return torch.FloatTensor([tanh(Gamma * np.matmul(vec1, vec2.T) = r)[0][0]])\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_SVM_algorithm/#4-training-the-svm-model-by-gradient-descent-method","title":"4. Training the SVM model by gradient descent method","text":"<p>(with the help of the Automatic Differentiation function in Pytorch)</p> <p>(Including how to solve the trouble of gradient disruption in Pytorch)</p> <pre><code>C = 10.0 # penalty factor\nM = training_X.__len__()\nalpha_lack = Variable(torch.FloatTensor(n.zeros((1, M - 1))), requires_grad = True) # alpha_new\ny = torch.FloatTensor(training_y.iloc[:, 0]).reshape(-1, 1)\n</code></pre> <pre><code>print(alpha_lack.shape)\nalpha_lack\n</code></pre> <pre><code>print(y.shape)\ny.T\n</code></pre> <pre><code>Iter times = 10000000\nlr = 0.22 # learning rate\nObj_List = []\n\nfor it in range(Iter times):\n\n    alpha_last = - torch.mm(alpha_lack, y[0 : M - 1, 0].reshape(-1, 1)) / y[M - 1]\n\n    sum_ = 0.0\n\n    for i in range(M):\n        for j in range (M):\n\n            if i != M - 1 and j != M - 1:\n                sum_ = sum_ + 0.5 * alpha_lack[0, i] * alpha_lack[0, j] * y[i, 0] * y[j, 0] * K(training_X.iloc[i, :], training_X.iloc[j, :])\n\n            elif i == M - 1 and j !=M - 1:\n                sum_ = sum_ + 0.5 * alpha_last * alpha_lack[0, j] * y[i, 0] * y[j, 0] * K(training_X.iloc[i, :],training_X.iloc[j, :])\n\n            elif i != M - 1 and j == M - 1:\n                sum_ = sum_ + 0.5 * alpha_last * alpha_lack[0, i] * y[i, 0] * y[j, 0] * K(training_X.iloc[i, :],training_X.iloc[j, :])\n\n            else:\n                sum_ = sum_ + 0.5 * alpha_last * alpha_last * y[i, 0] * y[j, 0] * K(training_X.iloc[i, :], training_X.iloc[j, :])\n\n    Obj = sum_ - (torch.sum(alpha_lack) + alpha_last)\n\n    Obj.backward()\n\n    grad = alpha_lack.grad.data\n\n    lr_temp = lr / (1 + it * 0.0008)\n\n    alpha_lack.data = alpha_lack.data - lr_temp * grad\n\n    alpha_lack.grad.data.zero_()\n\n    for col in range(alpha_lack.shape[1]):  # clipping\n\n        if alpha_lack[0, col] &gt; C:\n            alpha_lack[0, col] = C\n\n        elif alpha_lack[0, col] &lt; 0.0:\n            alpha_lack[0, col] = 0.0\n\n    alpha_lack = Variable(torch.FloatTensor(alpha_lack), requires_grad = True)\n\n    Obj_List.append(Obj)\n\n    print(\"-----------------------------------------------------------------------\")\n    print(it + 1, \"iterations has been completed!\")\n    print(\"    -&gt; Current Obj = \", Obj)\n    print(\"    -&gt; Current alpha_lack = \", alpha_lack)\n    print(\"-----------------------------------------------------------------------\")\n</code></pre> <pre><code>alpha_lack\n</code></pre> <pre><code>alpha_last = - torch.mm(alpha_lack, y[0 : M - 1, 0].reshape(-1, 1)) / y [M - 1]\nalpha_last\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_SVM_algorithm/#5-support-vectors-visualization","title":"5. Support vectors visualization","text":""},{"location":"Programming/MachineLearning/pytorch_SVM_algorithm/#6-computing-the-solutions-of-the-original-problem-w-b","title":"6. Computing the solutions of the original problem: W &amp; b","text":"<pre><code>s = 5\nys = y[s, 0]\nsum_record = 0.0\n\nfor i in range(M - 1):\n    sum_record = sum_record + alpha_lack[0, i] * y[i, 0] * K(training_X.iloc[i, :], training_X.iloc[s, :1])\n\nsum_record = sum_record + alpha_last * y [M - 1, 0] * K(training_X.iloc[M - 1, :], training_X.iloc[s, :])\n\nb = (1 / ys) - sum_record\nb\n</code></pre> <pre><code>W = torch.FloatTensor(np.zeros ((2, 1)))\nW\n</code></pre> <pre><code>for i in range(M - 1):\n    W = W + alpha_lack[0, i] * y[i, 0] * torch.FloatTensor(n.array(training_X.iloc[i, :]).reshape (-1, 1))\n\nW = W + alpha_last * y [M - 1, 0] * torch.FloatTensor(np.array(training_X.array.iloc[M - 1, :]).reshape(-1, 1))\nW\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_SVM_algorithm/#7-prediction-and-model-evaluation","title":"7. Prediction and model evaluation","text":"<pre><code>def prediction(x):\n    sum_record = 0.0\n    for i in range(M - 1):\n        sum_record = sum_record + alpha_lack[0, i] * y[i, 0] * K(training_X.iloc[i, :], x)\n        sum_record = sum_record + alpha_last * y [M - 1, 0] * K(training_X.iloc[M - 1, :], x)\n\n    return sum record + b\n</code></pre> <pre><code>max_r = 0.45\nx1 = []\nx2 = []\ny test = [] \n\nfor i in range(50):\n    r = max_r * np.random.rand()\n    rand_num = np.random.rand ()\n    x1.append(%.5 + r * cos(2 * pi * rand_num))\n    x2.append(0.5 + r * sin(2 * pi * rand_num))\n    y_test.append(1)\n\nfor i in range(50):\n    r = max_r * np.random.rand()\n    rand_num = np random.rand()\n    x1.append(-0.5 + r * cos (2 * pi * rand_num)) \n    x2.append(-0.5 + r * sin(2 * pi * rand num))\n    y_test.append(-1)\n\ndict1 = {\"xl\": x1, \"X2\": x2}\ntest_X = pd.DataFrame(dict1)\n\ndict2 = {\"y\": y_test}\ntest_y = pd.DataFrame(dict2)\n\ntest_X.head (5)\n</code></pre> <pre><code>prediction_results = []\n\nfor i in range(test_X.__len__()):\n    y_pred_i = prediction(test_X.iloc[i, :])\n    prediction_results.append(y_pred_i)\n\nfor i in range(prediction_results.__len__()):\n\n    if prediction_results[i] &gt; 0.0:\n        prediction_results[i] = 1.0\n\n    else:\n        prediction_results[i] = -1.0\n\nMatrix(np.array(prediction_results).reshape(-1, 10))\n</code></pre> <pre><code>from sklearn.metrics import accuracy_score\nprint(\"The accuracy score in this prediction is\", \n      accuracy_score(np.array (prediction_results), test_y) * 100, \"%\")\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_convolutional_neural_network%28CNN%29/","title":"Mist Image Classification by CNN","text":""},{"location":"Programming/MachineLearning/pytorch_convolutional_neural_network%28CNN%29/#1-fire-up","title":"1. Fire Up","text":"<pre><code>import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport tensorflow as tf\nfrom tensorflow import keras\nimport sympy\nfrom sympy import Matrix\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_convolutional_neural_network%28CNN%29/#2-data-generating","title":"2. Data Generating","text":"<pre><code>mist dataset = keras.datasets.mnist\n(data_train, label_train), (data_test, label_test) = mnist_dataset.load_data ()\n</code></pre>"},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/","title":"1. Load the data-set","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing \nfrom sklearn.cluster import KMeans \nimport matplotlib.pyplot as plt\n</code></pre> <p>We need to lead the perfume dataset from sklearn</p> <pre><code>perfume_preference = pd.read_csv(\"Perfume preference.csv\")\nperfume_preference\n</code></pre> Narcissus Agrumen Oud Jasmine Amber Neroli Indole Vanilla Frankincense Bergamot Galbanum Magnolia Sandalwood Cashmeran Citron Opopanax Aliphatic Aldehydes Vetiver 0 1353.0 1252.0 4066.0 3838.0 2144.0 4404.0 32082.0 3866.0 2505.0 3972.0 4485.0 6441.0 4106.0 1722.0 4287.0 4820.0 4140.0 1463.0 1 1089.0 2152.0 4045.0 3710.0 2235.0 4352.0 30398.0 4769.0 2995.0 4720.0 4532.0 10931.0 3794.0 1638.0 4648.0 4472.0 4184.0 1071.0 2 4177.0 3592.0 3596.0 1745.0 3234.0 2116.0 21678.0 4864.0 3178.0 3381.0 1376.0 18153.0 2502.0 1733.0 1747.0 2728.0 4580.0 4742.0 3 4899.0 3738.0 2454.0 3976.0 4945.0 3853.0 17963.0 3040.0 2943.0 2870.0 4016.0 18819.0 1990.0 5118.0 2391.0 2012.0 3470.0 3057.0 4 4822.0 4030.0 3447.0 4225.0 4078.0 3772.0 23988.0 3389.0 2415.0 2695.0 3887.0 20367.0 2118.0 4530.0 2427.0 3205.0 4319.0 2289.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9492 4857.0 3654.0 2788.0 3830.0 4272.0 4349.0 20816.0 2524.0 2675.0 2550.0 4620.0 18506.0 1999.0 5329.0 2140.0 2568.0 4358.0 2361.0 9493 2040.0 2561.0 3913.0 3375.0 2058.0 4722.0 19399.0 4765.0 3796.0 2949.0 2686.0 13008.0 4201.0 1830.0 1534.0 2272.0 3348.0 2992.0 9494 4846.0 4883.0 4153.0 2108.0 4164.0 1881.0 20551.0 5030.0 2683.0 4001.0 1450.0 24684.0 3979.0 1187.0 2107.0 2508.0 4581.0 4731.0 9495 4310.0 3916.0 3937.0 2488.0 3343.0 2219.0 22914.0 5104.0 2640.0 3864.0 1730.0 19874.0 3654.0 499.0 1920.0 2971.0 4476.0 4654.0 9496 2698.0 3174.0 3984.0 3541.0 2522.0 4946.0 18512.0 5165.0 4167.0 2704.0 2536.0 16069.0 3851.0 1620.0 1830.0 2084.0 3240.0 3644.0 <p>9497 rows \u00d7 18 columns</p> <pre><code>perfume_score = pd.read_csv(\"Perfume Score.csv\")\nperfume_score\n</code></pre> Narcissus Agrumen Oud Jasmine Amber Neroli Indole Vanilla Frankincense Bergamot Galbanum Magnolia Sandalwood Cashmeran Citron Opopanax Aliphatic Aldehydes Vetiver Scent Quality Score 0 489.766 343.510 638.519 315.377 966.417 913.256 1015.036 479.027 485.797 2918.050062 108.538 727.438 936.842 4801.306119 261.952 148.593 783.264 809.541 1.302700e+07 1 472.841 218.288 642.332 210.582 995.068 989.447 958.614 507.113 242.015 2119.074840 246.654 755.477 840.936 4896.315590 149.498 44.490 906.204 815.512 1.159073e+07 2 472.620 323.480 696.770 288.379 1006.334 875.163 987.398 611.463 410.451 2679.139347 281.022 729.155 825.386 5350.521973 177.980 141.612 705.294 794.394 1.367693e+07 3 503.155 397.632 644.533 151.414 960.097 905.462 1031.227 469.357 388.405 1784.035393 280.953 711.906 786.198 5029.939322 29.515 149.231 678.681 837.614 7.997427e+06 4 499.780 344.096 643.764 353.518 1033.988 978.976 871.312 439.266 311.002 3236.214279 272.058 737.003 898.238 4988.788504 138.884 122.238 622.090 824.174 1.113290e+07 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 4999 449.162 353.896 680.031 220.188 940.716 851.543 951.874 600.726 383.718 2209.955448 122.345 852.887 973.806 5122.835105 50.474 87.481 662.834 821.282 1.072665e+07 5000 526.781 392.868 652.819 268.901 983.403 700.787 1031.042 583.384 414.174 2453.815268 368.325 770.897 825.038 5009.288848 195.544 83.047 819.217 830.439 1.426609e+07 5001 475.160 256.740 655.360 204.422 905.181 1055.073 1008.550 539.192 399.411 2007.515839 192.985 774.179 747.784 4925.275302 205.319 143.601 741.248 780.727 9.882660e+06 5002 481.422 278.652 647.467 147.307 1033.814 880.379 1053.847 510.981 410.661 1762.999938 144.866 802.051 890.813 4992.597380 116.158 80.665 804.591 792.583 9.200338e+06 5003 476.130 364.371 659.429 224.550 1207.776 837.054 882.858 625.714 361.077 2175.815594 231.943 621.123 737.455 4939.983539 231.950 178.103 710.752 867.652 1.195802e+07 <p>5004 rows \u00d7 19 columns</p>"},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/#2-review-the-data-quatitatively","title":"2. Review the data quatitatively","text":"<pre><code>perfume_score.describe()\n</code></pre> Narcissus Agrumen Oud Jasmine Amber Neroli Indole Vanilla Frankincense Bergamot Galbanum Magnolia Sandalwood Cashmeran Citron Opopanax Aliphatic Aldehydes Vetiver Scent Quality Score count 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5004.000000 5.004000e+03 mean 470.922058 346.112136 674.849797 209.349424 988.363780 888.163961 944.930020 564.445091 378.302281 2186.942959 250.475366 769.066442 866.283868 5172.149509 121.493370 118.018880 703.506534 802.327559 1.079593e+07 std 23.038942 59.788428 36.724524 80.932744 74.334501 84.265546 70.080494 72.778522 60.988463 575.210895 76.697327 89.079681 87.392412 275.760510 70.881267 62.237022 99.694353 25.963051 2.867554e+06 min 383.651000 121.396000 543.403000 0.000000 652.234000 539.166000 683.213000 287.286000 142.905000 487.811886 0.000000 457.725000 545.930000 4119.640577 0.000000 0.000000 317.364000 714.678000 3.860472e+06 25% 455.292000 305.453750 650.423250 154.964250 938.166000 830.146500 896.683000 514.985750 337.244250 1804.401369 198.506500 707.927750 809.248500 4990.344908 69.145750 74.265250 635.601500 784.980750 8.777066e+06 50% 470.695500 346.160500 675.707000 209.278000 989.786500 889.092500 945.116500 563.099000 378.364000 2191.656699 249.945000 770.214000 867.822500 5174.842876 118.840500 117.010500 703.989000 802.968000 1.057777e+07 75% 486.797750 386.261000 699.350250 262.900250 1038.281500 945.673750 993.838500 612.617000 418.213250 2571.954572 301.876250 830.076000 924.809000 5358.193002 170.583000 160.389500 769.465500 819.953500 1.255241e+07 max 548.708000 562.238000 817.061000 481.593000 1258.446000 1183.733000 1214.694000 825.775000 646.347000 4079.337285 535.614000 1116.747000 1195.179000 6231.998892 396.534000 348.029000 1049.738000 888.987000 2.203800e+07 <pre><code>perfume_preference.describe()\n</code></pre> Narcissus Agrumen Oud Jasmine Amber Neroli Indole Vanilla Frankincense Bergamot Galbanum Magnolia Sandalwood Cashmeran Citron Opopanax Aliphatic Aldehydes Vetiver count 9494.000000 9493.000000 9489.000000 9495.000000 9494.000000 9492.000000 9489.000000 9494.000000 9491.000000 9490.000000 9489.000000 9493.000000 9492.000000 9492.000000 9487.000000 9489.000000 9492.000000 9487.000000 mean 3265.382663 3121.755083 3763.242597 3106.079726 3005.079524 3821.780341 22736.683001 4312.222878 3113.745970 3426.057113 3139.691538 15808.726325 3217.781184 2446.029709 2479.632023 2947.475182 4202.323957 3008.485190 std 1390.405062 924.872149 552.936664 686.367481 991.974076 1059.493773 5232.947390 861.309113 661.345376 740.826433 1223.697310 4625.023111 785.173638 1342.850878 1191.003546 1046.691982 409.025964 1219.619389 min 515.000000 584.000000 1998.000000 1373.000000 946.000000 642.000000 13318.000000 1685.000000 1239.000000 1703.000000 551.000000 3087.000000 1237.000000 58.000000 30.000000 1057.000000 2256.000000 283.000000 25% 1962.000000 2334.000000 3383.000000 2538.500000 2119.000000 3176.000000 18746.000000 3642.000000 2628.000000 2728.000000 2071.000000 11880.000000 2545.750000 1581.000000 1620.000000 2149.000000 3935.750000 2093.000000 50% 2911.500000 3372.000000 3780.000000 3201.000000 2941.000000 4193.500000 20910.000000 4573.000000 2908.000000 3525.000000 2991.000000 17055.000000 3515.000000 1859.500000 2069.000000 2581.000000 4215.000000 2873.000000 75% 4598.000000 3913.000000 4142.000000 3616.000000 3764.000000 4569.000000 28136.000000 4953.000000 3716.000000 4036.000000 4293.000000 19765.000000 3773.000000 2775.250000 3965.500000 4037.000000 4486.000000 3863.000000 max 5761.000000 5119.000000 5811.000000 4936.000000 5798.000000 5826.000000 35793.000000 6136.000000 4814.000000 5267.000000 5998.000000 25873.000000 4879.000000 6348.000000 5061.000000 5562.000000 5547.000000 6072.000000 <p>We should also review yje data to see if there are any missing values.</p> <pre><code>pd.isnull(perfume_score).any()\n</code></pre> <pre><code>Narcissus              False\nAgrumen                False\nOud                    False\nJasmine                False\nAmber                  False\nNeroli                 False\nIndole                 False\nVanilla                False\nFrankincense           False\nBergamot               False\nGalbanum               False\nMagnolia               False\nSandalwood             False\nCashmeran              False\nCitron                 False\nOpopanax               False\nAliphatic Aldehydes    False\nVetiver                False\nScent Quality Score    False\ndtype: bool\n</code></pre> <pre><code>pd.isnull(perfume_preference).any()\n</code></pre> <pre><code>Narcissus              True\nAgrumen                True\nOud                    True\nJasmine                True\nAmber                  True\nNeroli                 True\nIndole                 True\nVanilla                True\nFrankincense           True\nBergamot               True\nGalbanum               True\nMagnolia               True\nSandalwood             True\nCashmeran              True\nCitron                 True\nOpopanax               True\nAliphatic Aldehydes    True\nVetiver                True\ndtype: bool\n</code></pre> <p>Turns out there's a flaw in the perfume preference data.</p>"},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/#3-clean-and-tidy-the-data","title":"3. Clean and tidy the data","text":"<p>Display a count of missing data</p> <pre><code>print(perfume_preference.isnull().sum().sum())\n</code></pre> <pre><code>104\n</code></pre> <p>Visualising missing data</p> <pre><code>import missingno as msno\nperfume_preference_columns = perfume_preference.iloc[:,:]\nmsno.matrix(perfume_preference_columns)\n</code></pre> <pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a85e7c40c8&gt;\n</code></pre> <p></p> <p>Draw a bar-plot to indicate the amount of missingdata in each feature</p> <pre><code>msno.bar(perfume_preference_columns)\n</code></pre> <pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a85eb2b208&gt;\n</code></pre> <p></p> <p>Impute missing data using the mean of other data from the same feature</p> <pre><code>perfume_preference.fillna(perfume_preference.mean(), inplace = True) \nperfume_preference\n</code></pre> Narcissus Agrumen Oud Jasmine Amber Neroli Indole Vanilla Frankincense Bergamot Galbanum Magnolia Sandalwood Cashmeran Citron Opopanax Aliphatic Aldehydes Vetiver 0 1353.0 1252.0 4066.0 3838.0 2144.0 4404.0 32082.0 3866.0 2505.0 3972.0 4485.0 6441.0 4106.0 1722.0 4287.0 4820.0 4140.0 1463.0 1 1089.0 2152.0 4045.0 3710.0 2235.0 4352.0 30398.0 4769.0 2995.0 4720.0 4532.0 10931.0 3794.0 1638.0 4648.0 4472.0 4184.0 1071.0 2 4177.0 3592.0 3596.0 1745.0 3234.0 2116.0 21678.0 4864.0 3178.0 3381.0 1376.0 18153.0 2502.0 1733.0 1747.0 2728.0 4580.0 4742.0 3 4899.0 3738.0 2454.0 3976.0 4945.0 3853.0 17963.0 3040.0 2943.0 2870.0 4016.0 18819.0 1990.0 5118.0 2391.0 2012.0 3470.0 3057.0 4 4822.0 4030.0 3447.0 4225.0 4078.0 3772.0 23988.0 3389.0 2415.0 2695.0 3887.0 20367.0 2118.0 4530.0 2427.0 3205.0 4319.0 2289.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9492 4857.0 3654.0 2788.0 3830.0 4272.0 4349.0 20816.0 2524.0 2675.0 2550.0 4620.0 18506.0 1999.0 5329.0 2140.0 2568.0 4358.0 2361.0 9493 2040.0 2561.0 3913.0 3375.0 2058.0 4722.0 19399.0 4765.0 3796.0 2949.0 2686.0 13008.0 4201.0 1830.0 1534.0 2272.0 3348.0 2992.0 9494 4846.0 4883.0 4153.0 2108.0 4164.0 1881.0 20551.0 5030.0 2683.0 4001.0 1450.0 24684.0 3979.0 1187.0 2107.0 2508.0 4581.0 4731.0 9495 4310.0 3916.0 3937.0 2488.0 3343.0 2219.0 22914.0 5104.0 2640.0 3864.0 1730.0 19874.0 3654.0 499.0 1920.0 2971.0 4476.0 4654.0 9496 2698.0 3174.0 3984.0 3541.0 2522.0 4946.0 18512.0 5165.0 4167.0 2704.0 2536.0 16069.0 3851.0 1620.0 1830.0 2084.0 3240.0 3644.0 <p>9497 rows \u00d7 18 columns</p> <p>Do a final check by re-counting the amount of missing data</p> <pre><code>print(perfume_preference.isnull().sum().sum()) \n</code></pre> <pre><code>0\n</code></pre>"},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/#4-review-the-data-visually","title":"4. Review the data visually","text":"<pre><code>from pandas.plotting import scatter_matrix\nscatter_matrix(perfume_preference, figsize = (12,12));\n</code></pre> <pre><code>scatter_matrix(perfume_score, figsize = (12,12));\n</code></pre> <p>Correlation matrix:</p> <pre><code>correlation_matrix = np.absolute(perfume_preference.corr().round(2))\nsns.set(rc = {'figure.figsize': (10,10)})\nax = sns.heatmap(correlation_matrix, annot = True, cmap = 'Reds')\nbottom,top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n</code></pre> <pre><code>(18.5, -0.5)\n</code></pre> <p></p> <p>From this correlation map, we see that: - Vetiver is strongly correlated with Neroli and Citron - Opopanax is strongly correlated with Bergamot - ...</p> <pre><code>correlation_matrix = np.absolute(perfume_score.corr().round(2))\nsns.set(rc = {'figure.figsize': (10,10)})\nax = sns.heatmap(correlation_matrix, annot = True, cmap = 'Reds')\nbottom,top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n</code></pre> <pre><code>(19.5, -0.5)\n</code></pre> <p></p> <p>From this correlation map, we see that: - Scent Quality Score is strongly correlated with Bergamot and Jasmine - Scent Quality Score is also affected by Aliphatic Aldehydes and Vanilla to some extent - Jasmine and Bergamot are correlated with each other. We should not use both Jasmine and Vanillafor building the model considering the 'double counting'their impact on the result - Also Oud and Cashmeran are correlated.</p> <p>Therefore, we choose Jasmine, Vanilla and Aliphatic Aldhydes for building our first model.</p> <p>Three key features(Jasmine, Vanilla and Aliphatic Aldehydes) in a 3D plot:</p> <pre><code>from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(111, projection = '3d')\nax.scatter(perfume_score['Jasmine'], perfume_score['Vanilla'], perfume_score['Aliphatic Aldehydes'], c = perfume_score['Scent Quality Score'], cmap = 'gist_heat')\n</code></pre> <pre><code>&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x1a86c1b2908&gt;\n</code></pre> <p></p>"},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/#5-polynominal-regression","title":"5. Polynominal regression","text":""},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/#51-select-and-split-the-data-processing-the-perfume-score-data","title":"5.1 Select and split the data - processing the perfume score data","text":"<pre><code>X = pd.DataFrame(np.c_[perfume_score['Jasmine'], perfume_score['Vanilla'], perfume_score['Aliphatic Aldehydes']], columns = ['Jasmine','Vanilla','Aliphatic Aldehydes'])\nY = perfume_score['Scent Quality Score']\nX\n</code></pre> Jasmine Vanilla Aliphatic Aldehydes 0 315.377 479.027 783.264 1 210.582 507.113 906.204 2 288.379 611.463 705.294 3 151.414 469.357 678.681 4 353.518 439.266 622.090 ... ... ... ... 4999 220.188 600.726 662.834 5000 268.901 583.384 819.217 5001 204.422 539.192 741.248 5002 147.307 510.981 804.591 5003 224.550 625.714 710.752 <p>5004 rows \u00d7 3 columns</p> <pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code>X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2)\nprint(X_train[0:10])\n</code></pre> <pre><code>      Jasmine  Vanilla  Aliphatic Aldehydes\n4427  170.793  570.382              908.473\n3248  158.091  416.565              641.899\n2286  203.866  600.023              642.780\n581   232.697  436.631              720.934\n1188   90.112  538.792              814.159\n2535   17.924  585.713              618.380\n2151  252.522  624.270              621.184\n113   332.608  570.309              561.070\n3801   80.332  487.969              710.184\n3430  362.441  550.813              800.493\n</code></pre>"},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/#52-build-the-model","title":"5.2 Build the model","text":"<p>We first generate the features and take a look at the extended set of feature names:</p> <pre><code>from sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree = 3)\nX_train_poly = poly_features.fit_transform(X_train)\nprint(poly_features.get_feature_names([\"Jasmine\",\"Vanilla\",\"Aliphatic Aldehydes\"]))\n</code></pre> <pre><code>['1', 'Jasmine', 'Vanilla', 'Aliphatic Aldehydes', 'Jasmine^2', 'Jasmine Vanilla', 'Jasmine Aliphatic Aldehydes', 'Vanilla^2', 'Vanilla Aliphatic Aldehydes', 'Aliphatic Aldehydes^2', 'Jasmine^3', 'Jasmine^2 Vanilla', 'Jasmine^2 Aliphatic Aldehydes', 'Jasmine Vanilla^2', 'Jasmine Vanilla Aliphatic Aldehydes', 'Jasmine Aliphatic Aldehydes^2', 'Vanilla^3', 'Vanilla^2 Aliphatic Aldehydes', 'Vanilla Aliphatic Aldehydes^2', 'Aliphatic Aldehydes^3']\n</code></pre> <p>Actual data:</p> <pre><code>X_train_poly[0:10]\n</code></pre> <pre><code>array([[1.00000000e+00, 1.70793000e+02, 5.70382000e+02, 9.08473000e+02,\n        2.91702488e+04, 9.74172529e+04, 1.55160829e+05, 3.25335626e+05,\n        5.18176647e+05, 8.25323192e+05, 4.98207431e+06, 1.66381849e+07,\n        2.65003835e+07, 5.55650476e+07, 8.85009440e+07, 1.40959424e+08,\n        1.85565585e+08, 2.95558632e+08, 4.70749493e+08, 7.49783836e+08],\n       [1.00000000e+00, 1.58091000e+02, 4.16565000e+02, 6.41899000e+02,\n        2.49927643e+04, 6.58551774e+04, 1.01478455e+05, 1.73526399e+05,\n        2.67392657e+05, 4.12034326e+05, 3.95113110e+06, 1.04111109e+07,\n        1.60428304e+07, 2.74329620e+07, 4.22723725e+07, 6.51389187e+07,\n        7.22850245e+07, 1.11386422e+08, 1.71639079e+08, 2.64484422e+08],\n       [1.00000000e+00, 2.03866000e+02, 6.00023000e+02, 6.42780000e+02,\n        4.15613460e+04, 1.22324289e+05, 1.31040987e+05, 3.60027601e+05,\n        3.85682784e+05, 4.13166128e+05, 8.47294535e+06, 2.49377635e+07,\n        2.67148020e+07, 7.33973868e+07, 7.86276064e+07, 8.42305259e+07,\n        2.16024841e+08, 2.31418541e+08, 2.47909180e+08, 2.65574924e+08],\n       [1.00000000e+00, 2.32697000e+02, 4.36631000e+02, 7.20934000e+02,\n        5.41478938e+04, 1.01602724e+05, 1.67759179e+05, 1.90646630e+05,\n        3.14782133e+05, 5.19745832e+05, 1.26000524e+07, 2.36426490e+07,\n        3.90370577e+07, 4.43628989e+07, 7.32488581e+07, 1.20943296e+08,\n        8.32422288e+07, 1.37443638e+08, 2.26937143e+08, 3.74702442e+08],\n       [1.00000000e+00, 9.01120000e+01, 5.38792000e+02, 8.14159000e+02,\n        8.12017254e+03, 4.85516247e+04, 7.33654958e+04, 2.90296819e+05,\n        4.38662356e+05, 6.62854877e+05, 7.31724988e+05, 4.37508401e+06,\n        6.61111156e+06, 2.61592270e+07, 3.95287422e+07, 5.97311787e+07,\n        1.56409604e+08, 2.36347768e+08, 3.57140905e+08, 5.39669264e+08],\n       [1.00000000e+00, 1.79240000e+01, 5.85713000e+02, 6.18380000e+02,\n        3.21269776e+02, 1.04983198e+04, 1.10838431e+04, 3.43059718e+05,\n        3.62193205e+05, 3.82393824e+05, 5.75843947e+03, 1.88171884e+05,\n        1.98666804e+05, 6.14900239e+06, 6.49195101e+06, 6.85402691e+06,\n        2.00934537e+08, 2.12141269e+08, 2.23973034e+08, 2.36464693e+08],\n       [1.00000000e+00, 2.52522000e+02, 6.24270000e+02, 6.21184000e+02,\n        6.37673605e+04, 1.57641909e+05, 1.56862626e+05, 3.89713033e+05,\n        3.87786536e+05, 3.85869562e+05, 1.61026614e+07, 3.98080501e+07,\n        3.96112641e+07, 9.84111145e+07, 9.79246316e+07, 9.74405535e+07,\n        2.43286155e+08, 2.42083501e+08, 2.40886791e+08, 2.39695998e+08],\n       [1.00000000e+00, 3.32608000e+02, 5.70309000e+02, 5.61070000e+02,\n        1.10628082e+05, 1.89689336e+05, 1.86616371e+05, 3.25252355e+05,\n        3.19983271e+05, 3.14799545e+05, 3.67957850e+07, 6.30921906e+07,\n        6.20700978e+07, 1.08181535e+08, 1.06428996e+08, 1.04704847e+08,\n        1.85494346e+08, 1.82489339e+08, 1.79533014e+08, 1.76624581e+08],\n       [1.00000000e+00, 8.03320000e+01, 4.87969000e+02, 7.10184000e+02,\n        6.45323022e+03, 3.91995257e+04, 5.70505011e+04, 2.38113745e+05,\n        3.46547776e+05, 5.04361314e+05, 5.18400890e+05, 3.14897630e+06,\n        4.58298085e+06, 1.91281534e+07, 2.78388760e+07, 4.05163531e+07,\n        1.16192126e+08, 1.69104572e+08, 2.46112686e+08, 3.58189335e+08],\n       [1.00000000e+00, 3.62441000e+02, 5.50813000e+02, 8.00493000e+02,\n        1.31363478e+05, 1.99637215e+05, 2.90131483e+05, 3.03394961e+05,\n        4.40921951e+05, 6.40789043e+05, 4.76115105e+07, 7.23567117e+07,\n        1.05155545e+08, 1.09962773e+08, 1.59808193e+08, 2.32248222e+08,\n        1.67113889e+08, 2.42865542e+08, 3.52954935e+08, 5.12947143e+08]])\n</code></pre> <pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nlin_model = LinearRegression()\nlin_model.fit(X_train, Y_train)\n</code></pre> <pre><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n</code></pre> <pre><code>polynomial_model = LinearRegression()\npolynomial_model.fit(X_train_poly, Y_train)\nprint(\"Model coefficients = \", polynomial_model.coef_)\nprint(\"Constant term (bias) = \", polynomial_model.intercept_)\n</code></pre> <pre><code>Model coefficients =  [ 0.00000000e+00  8.44956564e+03 -7.03926860e+02  3.91781726e+03\n -7.27929661e+00 -8.46412648e+00 -1.17297723e+01  1.85161715e+00\n  9.12487891e+00 -5.89023638e+00  7.03479781e-04 -1.54576830e-03\n  9.41465525e-03  9.78622737e-03  6.73304083e-02  6.58873295e-03\n -5.15228130e-03  6.53665385e-03 -5.71172445e-03  4.06079530e-03]\nConstant term (bias) =  878612.6569970567\n</code></pre>"},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/#53-test-the-model","title":"5.3 Test the model","text":"<p>We can apply this model to both the original traning data and to the test data:</p> <pre><code>y_train_predicted = polynomial_model.predict(X_train_poly)\ny_test_predict = polynomial_model.predict(poly_features.fit_transform(X_test))\n</code></pre> <p>Then measure the model quality for each case:</p> <pre><code>rmse_train = np.sqrt(mean_squared_error(Y_train, y_train_predicted))\nr2_train = r2_score(Y_train, y_train_predicted)\n\nrmse_test = np.sqrt(mean_squared_error(Y_test,y_test_predict))\nr2_test = r2_score(Y_test, y_test_predict)\n\nprint(\"R2:\")\nprint(\"Train = \", r2_train)\nprint(\"Test = \", r2_test)\nprint(\"RMSE:\")\nprint(\"Train = \", rmse_train)\nprint(\"Test = \", rmse_test)\n</code></pre> <pre><code>R2:\nTrain =  0.9791796776657181\nTest =  0.9814345530715797\nRMSE:\nTrain =  412795.4591276655\nTest =  393860.84246070404\n</code></pre> <p>Then we get the results of two regression evaluation indicators - mean square error root (RMSE) and R squared (R2).</p>"},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/#6-clustering-customers","title":"6. Clustering Customers","text":""},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/#61-standardize-the-data-into-a-standard-size","title":"6.1 Standardize the data into a standard size","text":"<pre><code>perfume_preference[0:10]\n</code></pre> Narcissus Agrumen Oud Jasmine Amber Neroli Indole Vanilla Frankincense Bergamot Galbanum Magnolia Sandalwood Cashmeran Citron Opopanax Aliphatic Aldehydes Vetiver 0 1353.0 1252.0 4066.0 3838.0 2144.0 4404.0 32082.0 3866.0 2505.0 3972.0 4485.0 6441.0 4106.0 1722.0 4287.0 4820.0 4140.0 1463.0 1 1089.0 2152.0 4045.0 3710.0 2235.0 4352.0 30398.0 4769.0 2995.0 4720.0 4532.0 10931.0 3794.0 1638.0 4648.0 4472.0 4184.0 1071.0 2 4177.0 3592.0 3596.0 1745.0 3234.0 2116.0 21678.0 4864.0 3178.0 3381.0 1376.0 18153.0 2502.0 1733.0 1747.0 2728.0 4580.0 4742.0 3 4899.0 3738.0 2454.0 3976.0 4945.0 3853.0 17963.0 3040.0 2943.0 2870.0 4016.0 18819.0 1990.0 5118.0 2391.0 2012.0 3470.0 3057.0 4 4822.0 4030.0 3447.0 4225.0 4078.0 3772.0 23988.0 3389.0 2415.0 2695.0 3887.0 20367.0 2118.0 4530.0 2427.0 3205.0 4319.0 2289.0 5 2251.0 2305.0 4058.0 3330.0 1775.0 4882.0 16567.0 5148.0 4443.0 2472.0 2615.0 11655.0 3061.0 1549.0 1563.0 1709.0 3426.0 3003.0 6 1661.0 2199.0 4994.0 2795.0 2231.0 4108.0 31511.0 3584.0 2771.0 4153.0 4462.0 11061.0 3791.0 2123.0 4528.0 4716.0 4124.0 2016.0 7 4690.0 3674.0 3827.0 2130.0 3483.0 2544.0 21010.0 4284.0 2457.0 3610.0 1819.0 18601.0 3917.0 2129.0 1609.0 2614.0 3879.0 3962.0 8 4735.0 3236.0 3255.0 3349.0 4221.0 4038.0 22356.0 3202.0 2804.0 2754.0 2968.0 16407.0 1899.0 4813.0 2878.0 2862.0 3125.0 2692.0 9 1259.0 2679.0 3541.0 3159.0 1937.0 4619.0 31967.0 4662.0 2797.0 3840.0 5327.0 13610.0 3317.0 1889.0 4457.0 4795.0 4390.0 1690.0 <pre><code>standardized_customer_data = preprocessing.scale(perfume_preference)\nstandardized_customer_data_df = pd.DataFrame(standardized_customer_data,columns = perfume_preference.columns)\n</code></pre>"},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/#62-decide-the-number-of-clusters-elbow-method","title":"6.2 Decide the number of clusters - Elbow method","text":"<pre><code>import matplotlib.pyplot as plt\n</code></pre> <pre><code>sse = []\nfor k in range(1,11):\n    kmeans = KMeans(n_clusters = k,)\n    kmeans.fit(standardized_customer_data_df)\n    sse.append(kmeans.inertia_)\nx = range(1,11)\nplt.xlabel('K')\nplt.ylabel('SSE')\nplt.plot(x,sse,'o-')\nplt.show()\n</code></pre> <p>The 'elbow' value on the graph indicates the optimum number of clusters. The number of clusters here is 4.</p>"},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/#63-cluster-the-data-build-the-model","title":"6.3 Cluster the data (build the model)","text":"<p>First we create the object('machine') that we will use to build the model.</p> <pre><code>kmeans = KMeans(n_clusters = 4)\n</code></pre> <p>Then we use that object to identify clusters in the data.</p> <pre><code>kmeans.fit(standardized_customer_data_df)\n</code></pre> <pre><code>KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n       n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n       random_state=None, tol=0.0001, verbose=0)\n</code></pre> <pre><code>y_km = kmeans.fit_predict(standardized_customer_data_df)\n</code></pre>"},{"location":"Programming/MachineLearning/Perfume_Grouping/Perfume_Grouping/#64-reviw-the-results","title":"6.4 Reviw the Results","text":"<p>We want to know: for each group, how tightly clustered they are. That is, identify variances for each these groups.</p> <pre><code>km0_var = np.var(y_km ==0)\nkm1_var = np.var(y_km ==1)\nkm2_var = np.var(y_km ==2)\nkm3_var = np.var(y_km ==3)\nprint(\"Variance of group 0 is %f\" % km0_var)\nprint(\"Variance of group 1 is %f\" % km1_var)\nprint(\"Variance of group 2 is %f\" % km2_var)\nprint(\"Variance of group 3 is %f\" % km3_var)\n</code></pre> <pre><code>Variance of group 0 is 0.188117\nVariance of group 1 is 0.187171\nVariance of group 2 is 0.190040\nVariance of group 3 is 0.184611\n</code></pre> <p>Group 1 is the tightest, whereas group 2 is rather loose.</p> <p>Specifically, for any customer group identified indicate the spread (variance) for each group for each dimension (quantity of scent chemical)\uff1a</p> <pre><code>km0_che_var = np.var(perfume_preference[y_km == 0], axis=0) \nprint(\"Variance for each dimension of group 0 is: \")\nkm0_che_var\n</code></pre> <pre><code>Variance for each dimension of group 0 is:\n\n\n\n\n\nNarcissus              8.070874e+04\nAgrumen                1.860085e+05\nOud                    1.247092e+05\nJasmine                1.376372e+05\nAmber                  1.754141e+05\nNeroli                 9.981178e+04\nIndole                 1.814991e+06\nVanilla                1.461047e+05\nFrankincense           4.453063e+04\nBergamot               1.587021e+05\nGalbanum               5.220495e+04\nMagnolia               4.646825e+06\nSandalwood             8.379077e+04\nCashmeran              8.081125e+04\nCitron                 1.889976e+05\nOpopanax               7.244132e+04\nAliphatic Aldehydes    1.835542e+05\nVetiver                1.363927e+05\ndtype: float64\n</code></pre> <p>We can see that they are similar with regard to some chemicals but have a wide range of responses to other scent chemicals.</p> <p>The other three groups did much the same:</p> <pre><code>km1_che_var = np.var(perfume_preference[y_km == 1], axis=0) \nprint(\"Variance for each dimension of group 1 is: \")\nkm1_che_var\n</code></pre> <pre><code>Variance for each dimension of group 1 is:\n\n\n\n\n\nNarcissus              1.041274e+05\nAgrumen                6.039455e+04\nOud                    1.338378e+05\nJasmine                1.140873e+05\nAmber                  1.361587e+05\nNeroli                 1.831613e+05\nIndole                 2.477114e+06\nVanilla                9.723437e+04\nFrankincense           4.640931e+04\nBergamot               5.826429e+04\nGalbanum               2.036101e+05\nMagnolia               1.511061e+06\nSandalwood             4.278111e+04\nCashmeran              1.827669e+05\nCitron                 8.646526e+04\nOpopanax               9.921890e+04\nAliphatic Aldehydes    1.151117e+05\nVetiver                1.233736e+05\ndtype: float64\n</code></pre> <pre><code>km2_che_var = np.var(perfume_preference[y_km == 2], axis=0) \nprint(\"Variance for each dimension of group 2 is: \")\nkm2_che_var\n</code></pre> <pre><code>Variance for each dimension of group 2 is:\n\n\n\n\n\nNarcissus              9.723301e+04\nAgrumen                1.944770e+05\nOud                    1.962811e+05\nJasmine                1.289577e+05\nAmber                  6.918558e+04\nNeroli                 4.724214e+04\nIndole                 1.778136e+06\nVanilla                1.525133e+05\nFrankincense           1.786047e+05\nBergamot               7.893402e+04\nGalbanum               1.276341e+05\nMagnolia               4.858489e+06\nSandalwood             6.911784e+04\nCashmeran              6.365879e+04\nCitron                 4.415088e+04\nOpopanax               7.111018e+04\nAliphatic Aldehydes    9.197844e+04\nVetiver                1.581174e+05\ndtype: float64\n</code></pre> <pre><code>km3_che_var = np.var(perfume_preference[y_km == 3], axis=0) \nprint(\"Variance for each dimension of group 3 is: \")\nkm3_che_var\n</code></pre> <pre><code>Variance for each dimension of group 3 is:\n\n\n\n\n\nNarcissus              1.219228e+05\nAgrumen                9.138574e+04\nOud                    1.677518e+05\nJasmine                5.476186e+04\nAmber                  5.526990e+04\nNeroli                 1.466144e+05\nIndole                 3.613530e+06\nVanilla                1.238448e+05\nFrankincense           1.504654e+05\nBergamot               4.178902e+04\nGalbanum               7.548093e+04\nMagnolia               2.287027e+06\nSandalwood             1.405492e+05\nCashmeran              1.905020e+05\nCitron                 1.384131e+05\nOpopanax               1.444139e+05\nAliphatic Aldehydes    8.179831e+04\nVetiver                1.320200e+05\ndtype: float64\n</code></pre> <pre><code>print(y_km[0:20])\n</code></pre> <pre><code>[2 2 3 1 1 0 2 3 1 2 3 2 2 2 3 1 0 0 1 2]\n</code></pre> <pre><code>print(perfume_preference[y_km == 2][0:5])\n</code></pre> <pre><code>    Narcissus  Agrumen     Oud  Jasmine   Amber  Neroli   Indole  Vanilla  \\\n0      1353.0   1252.0  4066.0   3838.0  2144.0  4404.0  32082.0   3866.0   \n1      1089.0   2152.0  4045.0   3710.0  2235.0  4352.0  30398.0   4769.0   \n6      1661.0   2199.0  4994.0   2795.0  2231.0  4108.0  31511.0   3584.0   \n9      1259.0   2679.0  3541.0   3159.0  1937.0  4619.0  31967.0   4662.0   \n11     1683.0   2078.0  3989.0   2873.0  2038.0  4309.0  30393.0   3648.0\n\n    Frankincense  Bergamot  Galbanum  Magnolia  Sandalwood  Cashmeran  Citron  \\\n0         2505.0    3972.0    4485.0    6441.0      4106.0     1722.0  4287.0   \n1         2995.0    4720.0    4532.0   10931.0      3794.0     1638.0  4648.0   \n6         2771.0    4153.0    4462.0   11061.0      3791.0     2123.0  4528.0   \n9         2797.0    3840.0    5327.0   13610.0      3317.0     1889.0  4457.0   \n11        3011.0    4418.0    4092.0   10568.0      3768.0     1746.0  4483.0\n\n    Opopanax  Aliphatic Aldehydes  Vetiver  \n0     4820.0               4140.0   1463.0  \n1     4472.0               4184.0   1071.0  \n6     4716.0               4124.0   2016.0  \n9     4795.0               4390.0   1690.0  \n11    4495.0               4602.0   2307.0\n</code></pre> <p>We can look at the clusters in chart form.</p> <pre><code>#Jasmine vs Vanilla\nplt.figure(figsize = (7,7))\nplt.scatter(perfume_preference[y_km ==0]['Jasmine'], perfume_preference[y_km == 0]['Vanilla'],\n           s = 15,c = 'red',alpha = .5)\nplt.scatter(perfume_preference[y_km ==1]['Jasmine'], perfume_preference[y_km == 1]['Vanilla'],\n           s = 15,c = 'black',alpha = .5)\nplt.scatter(perfume_preference[y_km ==2]['Jasmine'], perfume_preference[y_km == 2]['Vanilla'],\n           s = 15,c = 'blue',alpha = .5)\nplt.scatter(perfume_preference[y_km ==3]['Jasmine'], perfume_preference[y_km == 3]['Vanilla'],\n           s = 15,c = 'cyan',alpha = .5)\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x1a86c2add48&gt;\n</code></pre> <p></p> <p>Take a '3D' view of the data</p> <pre><code>from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>%matplotlib\nfig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(111,projection = '3d')\nax.view_init(20,20)\nax.set_xlabel('Jasmine')\nax.set_ylabel('Vanilla')\nax.set_zlabel('Aliphatic Aldehydes')\n\nax.scatter(perfume_preference[y_km ==0]['Jasmine'], perfume_preference[y_km == 0]['Vanilla'],\n           perfume_preference[y_km ==0]['Aliphatic Aldehydes'], s = 15,c = 'red',alpha = .3)\nax.scatter(perfume_preference[y_km ==1]['Jasmine'], perfume_preference[y_km == 1]['Vanilla'],\n           perfume_preference[y_km ==1]['Aliphatic Aldehydes'], s = 15,c = 'black',alpha = .3)\nax.scatter(perfume_preference[y_km ==2]['Jasmine'], perfume_preference[y_km == 2]['Vanilla'],\n           perfume_preference[y_km ==2]['Aliphatic Aldehydes'], s = 15,c = 'blue',alpha = .3)\nax.scatter(perfume_preference[y_km ==3]['Jasmine'], perfume_preference[y_km == 3]['Vanilla'],\n           perfume_preference[y_km ==3]['Aliphatic Aldehydes'], s = 15,c = 'cyan',alpha = .3)\n</code></pre> <pre><code>Using matplotlib backend: Qt5Agg\n\n\n\n\n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x1a872c3bc88&gt;\n</code></pre> <p></p> <p>The central point of K-means:</p> <pre><code>centers = kmeans.cluster_centers_\ncenters\n</code></pre> <pre><code>array([[-0.63508675, -0.41641312,  0.1801924 ,  0.10214018, -0.71073531,\n         0.92559669, -0.91458644,  0.69966415,  1.49956699, -0.59666119,\n        -0.60720389, -0.41660319,  0.57117372, -0.48341316, -0.74077772,\n        -0.91436421, -0.42815036,  0.10413989],\n       [ 1.01512708,  0.75401605, -1.09476607,  1.04776978,  1.37499078,\n         0.14680867, -0.54262287, -1.52078422, -0.61896137, -1.13774394,\n         0.62329158,  0.75422437, -1.61128735,  1.66687438, -0.18799296,\n        -0.54276709, -0.53338134, -0.30061661],\n       [-1.2345529 , -1.27841445,  0.84571864,  0.21695373, -1.00251791,\n         0.4677676 ,  1.5634698 ,  0.12346678, -0.43796367,  1.19821614,\n         1.18028798, -1.27839257,  0.56702368, -0.50325856,  1.59486712,\n         1.56359936,  0.82022759, -1.19924176],\n       [ 0.90638695,  0.99381461,  0.04883221, -1.40107939,  0.37454187,\n        -1.59030928, -0.13842807,  0.70373206, -0.45305188,  0.52350583,\n        -1.24439817,  0.99377463,  0.46500412, -0.67859451, -0.71193968,\n        -0.13864473,  0.12810446,  1.45221773]])\n</code></pre> <p>We can visualize these arrays:</p> <pre><code>from matplotlib.pyplot import figure\n</code></pre> <pre><code>figure(figsize = (20, 10))\nx = np.array(range(0, 18))\ny = np.array([[ 0.90638695,  0.99381461,  0.04883221, -1.40107939,  0.37454187,\n        -1.59030928, -0.13842807,  0.70373206, -0.45305188,  0.52350583,\n        -1.24439817,  0.99377463,  0.46500412, -0.67859451, -0.71193968,\n        -0.13864473,  0.12810446,  1.45221773],\n       [-1.2345529 , -1.27841445,  0.84571864,  0.21695373, -1.00251791,\n         0.4677676 ,  1.5634698 ,  0.12346678, -0.43796367,  1.19821614,\n         1.18028798, -1.27839257,  0.56702368, -0.50325856,  1.59486712,\n         1.56359936,  0.82022759, -1.19924176],\n       [ 1.01512708,  0.75401605, -1.09476607,  1.04776978,  1.37499078,\n         0.14680867, -0.54262287, -1.52078422, -0.61896137, -1.13774394,\n         0.62329158,  0.75422437, -1.61128735,  1.66687438, -0.18799296,\n        -0.54276709, -0.53338134, -0.30061661],\n       [-0.63508675, -0.41641312,  0.1801924 ,  0.10214018, -0.71073531,\n         0.92559669, -0.91458644,  0.69966415,  1.49956699, -0.59666119,\n        -0.60720389, -0.41660319,  0.57117372, -0.48341316, -0.74077772,\n        -0.91436421, -0.42815036,  0.10413989]])\nplt.title(\"Plotting Central Points\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Preference\")\n\nfor i, array in enumerate(y):\n    plt.scatter(x, array, s = (150, ), color = np.random.rand(3, ), marker = \"o\", label = f\"Array #{i}\")\n\nplt.legend(loc = \"center left\", bbox_to_anchor=(1, 0.5))\nplt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],[r'$Narcissus$', r'$Agrumen$', r'$Oud$', r'$Jasmine$', r'$Amber$', \n                                                          r'$Neroli$', r'$Indole$', r'$Vanilla$', r'$Frankincen$', r'$Bergamot$', \n                                                          r'$Galbanum$', r'$Magnolia$', r'$Sandalwood$', r'$Cashmeran$', \n                                                          r'$Citron$', r'$Opopanax$', r'$Aliphatic Aldehydes$', r'$Vetiver$'])\nplt.show()\n</code></pre> <p></p> <p>We define that if the coefficient is positive, then the feature is preferred by group customers.</p> <p>Best mixture for each group:</p> <ul> <li>Group 0: Narcissus, Agrumen, Oud, Amber, Vanilla, Bergamot, Magnolia, Sandalwood, Aliphatic Aldehydes, Vetiver</li> <li>Group 1: Oud, Jasmine, Neroli, Indole, Vanilla, Bergamot, Galbanum, Sandalwood, Citron, Opopanax, Aliphatic Aldehydes</li> <li>Group 2: Narcissus, Agrumen, Jasmine, Amber, Neroli, Galbanum, Magnolia, Cashmeran</li> <li>Group 3: Oud, Jasmine, Neroli, Vanilla, Frankincen, Sandalwood, Vetiver</li> </ul> <pre><code>\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_DNN/pytorch_DNN/","title":"Pytourch Deep Neuron Network","text":""},{"location":"Programming/MachineLearning/pytorch_DNN/pytorch_DNN/#1-fire-up","title":"1. Fire up","text":"<pre><code>import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_DNN/pytorch_DNN/#2-data-generating","title":"2. Data generating","text":"<pre><code>x1_Pos = []\nx2_Pos = []\ny_Pos = []\n\nfor i in range(1000):\n    temp = 4.0 * np.random.rand() - 2.0\n    y_Pos.append(1)\n    x1_Pos.append(temp)\n    if i % 2 == 0:\n        x2_Pos.append(np.sqrt(4.0 - temp ** 2) + 0.3 * np.random.randn ( ))\n    elif i % 2 == 1:\n        x2_Pos.append(- np.sqrt(4.0 - temp ** 2) + 0.3 * np.random.randn () )\n</code></pre> <p>\u8fd9\u6bb5\u4ee3\u7801\u751f\u6210\u4e86\u4e00\u4e2a\u5305\u542b1000\u4e2a\u6570\u636e\u70b9\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6570\u636e\u70b9\u6709\u4e09\u4e2a\u7279\u5f81\uff1a<code>x1_Pos</code>\uff0c<code>x2_Pos</code>\u548c<code>y_Pos</code>\u3002</p> <ul> <li>\u5bf9\u4e8e\u6bcf\u4e2a\u6570\u636e\u70b9\uff0c\u9996\u5148\u751f\u6210\u4e00\u4e2a\u5728\u533a\u95f4[-2.0, 2.0]\u4e0a\u5747\u5300\u5206\u5e03\u7684\u968f\u673a\u6570<code>temp</code>\u3002</li> <li>\u5bf9\u4e8e<code>y_Pos</code>\u7279\u5f81\uff0c\u5c06\u5176\u8bbe\u7f6e\u4e3a\u5e38\u65701\uff0c\u8868\u793a\u6b63\u4f8b\u3002</li> <li>\u5bf9\u4e8e<code>x1_Pos</code>\u7279\u5f81\uff0c\u5c06\u5176\u8bbe\u7f6e\u4e3atemp\u7684\u503c\u3002</li> <li>\u5bf9\u4e8e<code>x2_Pos</code>\u7279\u5f81\uff0c\u6839\u636e\u4ee5\u4e0b\u89c4\u5219\u8fdb\u884c\u8bbe\u7f6e\uff1a</li> <li>\u5982\u679c\u5f53\u524d\u6570\u636e\u70b9\u7684\u7d22\u5f15i\u662f\u5076\u6570\uff0c\u5219\u5c06<code>x2_Pos</code>\u8bbe\u7f6e\u4e3a<code>sqrt(4.0 - temp^2) + 0.3 * \u968f\u673a\u6b63\u6001\u5206\u5e03\u7684\u968f\u673a\u6570</code>\u3002</li> <li>\u5982\u679c\u5f53\u524d\u6570\u636e\u70b9\u7684\u7d22\u5f15i\u662f\u5947\u6570\uff0c\u5219\u5c06<code>x2_Pos</code>\u8bbe\u7f6e\u4e3a<code>- sqrt(4.0 - temp^2) + 0.3 * \u968f\u673a\u6b63\u6001\u5206\u5e03\u7684\u968f\u673a\u6570</code>\u3002</li> </ul> <p>\u7efc\u5408\u8d77\u6765\uff0c\u8fd9\u6bb5\u4ee3\u7801\u751f\u6210\u4e86\u4e00\u4e2a\u5177\u6709\u4e24\u4e2a\u7279\u5f81\uff08x1_Pos\u548cx2_Pos\uff09\u548c\u4e00\u4e2a\u6807\u7b7e\uff08y_Pos\uff09\u7684\u4e8c\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5176\u4e2dx1_Pos\u548cx2_Pos\u7684\u53d6\u503c\u4e0e\u5f7c\u6b64\u76f8\u5173\uff0c\u5e76\u4e14\u5b58\u5728\u4e00\u5b9a\u7684\u566a\u58f0\u3002</p> <pre><code>x1_Neg = []\nx2_Neg = []\ny_Neg = []\n\nfor i in range (1000):\n    temp = 10.0 * np.random.rand() - 5.0\n    y_Neg.append(0)\n    x1_Neg. append (temp)\n    if i % 2 == 0:\n        x2_Neg.append(np.sqrt(25.0 - temp ** 2) + 0.3 * np.random.randn ())\n    elif i % 2 == 1:\n        x2_Neg.append(- np.sqrt (25.0 - temp ** 2) + 0.3 * np.random.randn ())\n</code></pre> <pre><code>plt.figure(figsize = (8, 6))\nplt.scatter(x1_Pos, x2_Pos, color = \"black\", label = \"Class 1\", alpha = 0.5)\nplt.scatter(x1_Neg, x2_Neg, color = \"red\", label = \"Class 0\", alpha = 0.5)\nplt.xlabel(\"xl\")\nplt.ylabel(\"x2\")\nplt.legend ()\nplt.grid()\nplt.show()\n</code></pre> <p></p> <pre><code>Dict = {\"x1\": x1_Pos + x1_Neg, \"x2\": x2_Pos + x2_Neg, \"y\": y_Pos + y_Neg}\nDataTrain = pd.DataFrame(Dict)\nDataTrain\n</code></pre> x1 x2 y 0 0.117529 1.440490 1 1 -1.255701 -1.325910 1 2 1.950944 0.462942 1 3 0.744222 -1.889267 1 4 -0.637758 1.741075 1 ... ... ... ... 1995 -4.094008 -3.365285 0 1996 -4.273857 3.205604 0 1997 1.081440 -4.366669 0 1998 3.146540 4.072895 0 1999 2.274089 -4.855089 0 <p>2000 rows \u00d7 3 columns</p> <pre><code>x1_Pos = []\nx2_Pos = []\ny_Pos = []\n\nfor i in range(250):\n    temp = 4.0 * np.random.rand() - 2.0\n    y_Pos.append(1)\n    x1_Pos.append(temp)\n    if i % 2 == 0:\n        x2_Pos.append(np.sqrt(4.0 - temp ** 2) + 0.3 * np.random.randn ( ))\n    elif i % 2 == 1:\n        x2_Pos.append(- np.sqrt(4.0 - temp ** 2) + 0.3 * np.random.randn () )\n</code></pre> <pre><code>x1_Neg = []\nx2_Neg = []\ny_Neg = []\n\nfor i in range (250):\n    temp = 10.0 * np.random.rand() - 5.0\n    y_Neg.append(0)\n    x1_Neg. append (temp)\n    if i % 2 == 0:\n        x2_Neg.append(np.sqrt(25.0 - temp ** 2) + 0.3 * np.random.randn ())\n    elif i % 2 == 1:\n        x2_Neg.append(- np.sqrt (25.0 - temp ** 2) + 0.3 * np.random.randn ())\n</code></pre> <pre><code>Dict = {\"x1\": x1_Pos + x1_Neg, \"x2\": x2_Pos + x2_Neg, \"y\": y_Pos + y_Neg}\nDataTest = pd.DataFrame(Dict)\nDataTest\n</code></pre> x1 x2 y 0 1.620214 1.175676 1 1 -0.509711 -1.657162 1 2 -0.471200 2.130239 1 3 0.906513 -1.790748 1 4 -1.163231 1.673213 1 ... ... ... ... 495 3.438152 -3.321079 0 496 -3.602392 3.356225 0 497 -3.557373 -3.948084 0 498 -0.313884 4.804572 0 499 4.260027 -2.314735 0 <p>500 rows \u00d7 3 columns</p>"},{"location":"Programming/MachineLearning/pytorch_DNN/pytorch_DNN/#3-model-training","title":"3. Model training","text":"\\[\\vec{x}\\in\\mathbb{R}^{1\\times 2}\\] \\[W_1\\in\\mathbb{R}^{2\\times 10}; b_1\\in\\mathbb{R}^{1\\times 10}\\] \\[W_2\\in\\mathbb{R}^{10\\times 2}; b_2\\in\\mathbb{R}^{1\\times 2}\\] \\[z_1 = \\sigma(\\vec{x}W_1+b_1)\\] \\[z_2 = \\sigma(z_1W_2+b_2)\\] \\[\\hat{y} = {\\rm{softmax}}(z_2)\\] \\[Loss (W_1, W_2, b_1, b_2) = -\\frac{1}{N}\\sum_{i=1}^{N}[y_i\\log\\hat{y} +\uff081-y_1)\\log(1-\\hat{y})]\\] \\[k = 0,1,2,...\\] \\[W_1^{(k+1)} = W_1^{(k)} - \\alpha_k\\frac{\\partial Loss(W_1^{(k)}, W_2^{(k)}, b_1^{(k)}, b_2^{(k})}{\\partial W_1}\\] \\[W_2^{(k+1)} = W_2^{(k)} - \\alpha_k\\frac{\\partial Loss(W_1^{(k)}, W_2^{(k)}, b_1^{(k)}, b_2^{(k})}{\\partial W_2}\\] \\[b_1^{(k+1)} = b_1^{(k)} - \\alpha_k\\frac{\\partial Loss(W_1^{(k)}, W_2^{(k)}, b_1^{(k)}, b_2^{(k})}{\\partial b_1}\\] \\[b_2^{(k+1)} = b_2^{(k)} - \\alpha_k\\frac{\\partial Loss(W_1^{(k)}, W_2^{(k)}, b_1^{(k)}, b_2^{(k})}{\\partial b_2}\\] <pre><code>class DeepNeuralNetworkModel(nn.Module):\n\n    # Constructor of the class\n    def __init__(self, input_dim1, output_dim1, input_dim2, output_dim2):   # output_dim1 = input_dim2\n        super(DeepNeuralNetworkModel, self).__init__()\n\n        # Fully Connected Layer 1\n        self.FC_layer1 = nn.Linear(input_dim1, output_dim1)\n\n        # nn.init.constant_(self.FC_layerl.weight, 0.1)\n        # nn.init.constant_(self.FC_layer1.bias, -0.1)\n\n        # Fully Connected Layer 2\n        self.FC_layer2 = nn.Linear(input_dim2, output_dim2)\n        # nn.init.constant_(self.FC_layer2.weight, 0.1)\n        # nn.init.constant_(self.FC_layer2.bias, -0.1)\n\n        # Activation Function Sigmoid()\n        self.act_sig = nn.Sigmoid()\n\n    # Forward propagation function\n    def forward(self, x):     # dim of x: N*2\n        z1_ = self.FC_layer1(x)\n        z1  = self.act_sig(z1_)\n\n        z2_ = self.FC_layer2(z1)\n        z2  = self.act_sig(z2_)\n\n        return z2\n</code></pre> <p>\u8fd9\u6bb5\u4ee3\u7801\u5b9a\u4e49\u4e86\u4e00\u4e2a\u540d\u4e3a<code>DeepNeuralNetworkModel</code>\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7c7b\u3002\u8be5\u7c7b\u7ee7\u627f\u81ea<code>nn.Module</code>\uff0c\u662fPyTorch\u4e2d\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u57fa\u7c7b\u3002</p> <p>\u5728\u8be5\u7c7b\u4e2d\uff0c\u5305\u542b\u4ee5\u4e0b\u51e0\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a</p> <ol> <li> <p>\u6784\u9020\u51fd\u6570<code>__init__</code>\uff1a\u521d\u59cb\u5316\u6a21\u578b\u7684\u53c2\u6570\u548c\u5c42\u7ed3\u6784\u3002\u5b83\u63a5\u6536\u8f93\u5165\u7ef4\u5ea6\u548c\u8f93\u51fa\u7ef4\u5ea6\u4f5c\u4e3a\u53c2\u6570\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42\uff08<code>self.FC_layer1</code>\u548c<code>self.FC_layer2</code>\uff09\uff0c\u4ee5\u53ca\u4e00\u4e2aSigmoid\u6fc0\u6d3b\u51fd\u6570\uff08<code>self.act_sig</code>\uff09\u3002</p> </li> <li> <p>\u524d\u5411\u4f20\u64ad\u51fd\u6570<code>forward</code>\uff1a\u5b9a\u4e49\u4e86\u6570\u636e\u5728\u6a21\u578b\u4e2d\u524d\u5411\u4f20\u64ad\u7684\u6d41\u7a0b\u3002\u8f93\u5165\u6570\u636ex\u9996\u5148\u901a\u8fc7\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\uff08<code>self.FC_layer1</code>\uff09\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\uff0c\u7136\u540e\u7ecf\u8fc7Sigmoid\u6fc0\u6d3b\u51fd\u6570\uff08<code>self.act_sig</code>\uff09\u3002\u63a5\u7740\uff0c\u8f93\u51fa\u7ed3\u679c\u518d\u901a\u8fc7\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\uff08<code>self.FC_layer2</code>\uff09\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\uff0c\u6700\u540e\u518d\u7ecf\u8fc7\u4e00\u6b21Sigmoid\u6fc0\u6d3b\u51fd\u6570\u5f97\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u7ed3\u679c\u3002</p> </li> </ol> <p>\u8be5\u6a21\u578b\u7684\u7ed3\u6784\u662f\u4e00\u4e2a\u4e24\u5c42\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc:</p> <ul> <li>\u7b2c\u4e00\u5c42\u8f93\u5165\u7ef4\u5ea6\u4e3a<code>input_dim1</code>\uff0c\u7b2c\u4e00\u5c42\u8f93\u51fa\u7ef4\u5ea6\u4e3a<code>output_dim1</code></li> <li>\u7b2c\u4e8c\u5c42\u7684\u8f93\u5165\u7ef4\u5ea6\u4e3a<code>output_dim1</code>\uff08\u5373\u7b2c\u4e00\u5c42\u7684\u8f93\u51fa\u7ef4\u5ea6\uff09\uff0c\u7b2c\u4e8c\u5c42\u7684\u8f93\u51fa\u7ef4\u5ea6\u4e3a<code>output_dim2</code>\u3002</li> </ul> <p>\u6574\u4e2a\u6a21\u578b\u7684\u4f5c\u7528\u662f\u5c06\u8f93\u5165\u6570\u636e\u6620\u5c04\u5230\u4e00\u4e2a\u8f93\u51fa\u7ed3\u679c\uff0c\u901a\u8fc7\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42\u548c\u6fc0\u6d3b\u51fd\u6570\u7684\u7ec4\u5408\u6765\u5b9e\u73b0\u975e\u7ebf\u6027\u6620\u5c04\u548c\u7279\u5f81\u63d0\u53d6\u3002</p> <pre><code>X_vec = torch.tensor(DataTrain[[\"x1\", \"x2\"]].values, dtype=torch.float32)  # N*2\ny_vec = torch.tensor(DataTrain[\"y\"].values, dtype=torch.int64).reshape(-1, 1)  # N*1\n</code></pre> <pre><code>alpha = 0.2\nDNN_Model = DeepNeuralNetworkModel(2, 10, 10, 2)\n\n# \u4f18\u5316DNN model\u91cc\u7684\u6240\u6709\u53c2\u6570\uff0clr:learning rate \u5b66\u4e60\u7387\uff08\u52a8\u6001\u53d8\u5316\uff09\noptimizer = torch.optim.SGD(DNN_Model.parameters(), lr = alpha)\n# \u635f\u5931\u51fd\u6570\nloss_function = nn.CrossEntropyLoss ()\n\n# Dynamically Change the learning rate\ndef adjust_learning_rate(optimizer, epoch):   # epoch\u5c31\u53ef\u4ee5\u7406\u89e3\u4e3a\u662f\u4e00\u6b21\u8fed\u4ee3\n    lr = alpha / (1 + 0.00001 * epoch)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n</code></pre> <pre><code>Iter_times = 200000\nloss_list = []\n\nfor i in range(Iter_times):\n\n    outputs = DNN_Model.forward(X_vec)  # forward propagation\n\n    loss = loss_function(outputs, torch.squeeze(y_vec))  # compute loss\n\n    loss.backward() # backward propagation \n\n    optimizer.step() # update parameters\n\n    optimizer.zero_grad() # Reset grad to 0\n\n    if (i + 1) % 500 == 0:\n        print(i + 1, \"iterations have been completed!\")\n        print(\"-&gt; Now loss =\", loss)\n        print(\"=========================================\")\n\n    adjust_learning_rate(optimizer, i)\n\n    loss_list.append(loss)\n    # length = loss_list.__len__()\n    # if(torch.abs(loss_listrlength - 11 - loss_list rlength - 21)&lt;10** 1-15) and length &gt;=2):\n        # break\n</code></pre> <pre><code>500 iterations have been completed!\n-&gt; Now loss = tensor(0.6814, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n1000 iterations have been completed!\n-&gt; Now loss = tensor(0.6406, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n1500 iterations have been completed!\n-&gt; Now loss = tensor(0.5620, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n2000 iterations have been completed!\n-&gt; Now loss = tensor(0.4912, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n2500 iterations have been completed!\n-&gt; Now loss = tensor(0.4461, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n3000 iterations have been completed!\n-&gt; Now loss = tensor(0.4169, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n3500 iterations have been completed!\n-&gt; Now loss = tensor(0.3933, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n4000 iterations have been completed!\n-&gt; Now loss = tensor(0.3756, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n4500 iterations have been completed!\n-&gt; Now loss = tensor(0.3634, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n5000 iterations have been completed!\n-&gt; Now loss = tensor(0.3549, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n5500 iterations have been completed!\n-&gt; Now loss = tensor(0.3486, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n6000 iterations have been completed!\n-&gt; Now loss = tensor(0.3439, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n6500 iterations have been completed!\n-&gt; Now loss = tensor(0.3403, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n7000 iterations have been completed!\n-&gt; Now loss = tensor(0.3374, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n7500 iterations have been completed!\n-&gt; Now loss = tensor(0.3350, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n8000 iterations have been completed!\n-&gt; Now loss = tensor(0.3331, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n8500 iterations have been completed!\n-&gt; Now loss = tensor(0.3314, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n9000 iterations have been completed!\n-&gt; Now loss = tensor(0.3300, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n9500 iterations have been completed!\n-&gt; Now loss = tensor(0.3289, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n10000 iterations have been completed!\n-&gt; Now loss = tensor(0.3278, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n10500 iterations have been completed!\n-&gt; Now loss = tensor(0.3269, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n11000 iterations have been completed!\n-&gt; Now loss = tensor(0.3261, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n11500 iterations have been completed!\n-&gt; Now loss = tensor(0.3254, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n12000 iterations have been completed!\n-&gt; Now loss = tensor(0.3248, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n12500 iterations have been completed!\n-&gt; Now loss = tensor(0.3242, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n13000 iterations have been completed!\n-&gt; Now loss = tensor(0.3237, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n13500 iterations have been completed!\n-&gt; Now loss = tensor(0.3232, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n14000 iterations have been completed!\n-&gt; Now loss = tensor(0.3228, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n14500 iterations have been completed!\n-&gt; Now loss = tensor(0.3224, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n15000 iterations have been completed!\n-&gt; Now loss = tensor(0.3220, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n15500 iterations have been completed!\n-&gt; Now loss = tensor(0.3217, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n16000 iterations have been completed!\n-&gt; Now loss = tensor(0.3214, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n16500 iterations have been completed!\n-&gt; Now loss = tensor(0.3211, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n17000 iterations have been completed!\n-&gt; Now loss = tensor(0.3208, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n17500 iterations have been completed!\n-&gt; Now loss = tensor(0.3206, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n18000 iterations have been completed!\n-&gt; Now loss = tensor(0.3204, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n18500 iterations have been completed!\n-&gt; Now loss = tensor(0.3202, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n19000 iterations have been completed!\n-&gt; Now loss = tensor(0.3200, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n19500 iterations have been completed!\n-&gt; Now loss = tensor(0.3198, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n20000 iterations have been completed!\n-&gt; Now loss = tensor(0.3196, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n20500 iterations have been completed!\n-&gt; Now loss = tensor(0.3194, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n21000 iterations have been completed!\n-&gt; Now loss = tensor(0.3193, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n21500 iterations have been completed!\n-&gt; Now loss = tensor(0.3191, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n22000 iterations have been completed!\n-&gt; Now loss = tensor(0.3190, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n22500 iterations have been completed!\n-&gt; Now loss = tensor(0.3188, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n23000 iterations have been completed!\n-&gt; Now loss = tensor(0.3187, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n23500 iterations have been completed!\n-&gt; Now loss = tensor(0.3186, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n24000 iterations have been completed!\n-&gt; Now loss = tensor(0.3185, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n24500 iterations have been completed!\n-&gt; Now loss = tensor(0.3184, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n25000 iterations have been completed!\n-&gt; Now loss = tensor(0.3182, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n25500 iterations have been completed!\n-&gt; Now loss = tensor(0.3181, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n26000 iterations have been completed!\n-&gt; Now loss = tensor(0.3180, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n26500 iterations have been completed!\n-&gt; Now loss = tensor(0.3180, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n27000 iterations have been completed!\n-&gt; Now loss = tensor(0.3179, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n27500 iterations have been completed!\n-&gt; Now loss = tensor(0.3178, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n28000 iterations have been completed!\n-&gt; Now loss = tensor(0.3177, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n28500 iterations have been completed!\n-&gt; Now loss = tensor(0.3176, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n29000 iterations have been completed!\n-&gt; Now loss = tensor(0.3175, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n29500 iterations have been completed!\n-&gt; Now loss = tensor(0.3175, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n30000 iterations have been completed!\n-&gt; Now loss = tensor(0.3174, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n30500 iterations have been completed!\n-&gt; Now loss = tensor(0.3173, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n31000 iterations have been completed!\n-&gt; Now loss = tensor(0.3173, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n31500 iterations have been completed!\n-&gt; Now loss = tensor(0.3172, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n32000 iterations have been completed!\n-&gt; Now loss = tensor(0.3171, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n32500 iterations have been completed!\n-&gt; Now loss = tensor(0.3171, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n33000 iterations have been completed!\n-&gt; Now loss = tensor(0.3170, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n33500 iterations have been completed!\n-&gt; Now loss = tensor(0.3170, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n34000 iterations have been completed!\n-&gt; Now loss = tensor(0.3169, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n34500 iterations have been completed!\n-&gt; Now loss = tensor(0.3169, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n35000 iterations have been completed!\n-&gt; Now loss = tensor(0.3168, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n35500 iterations have been completed!\n-&gt; Now loss = tensor(0.3168, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n36000 iterations have been completed!\n-&gt; Now loss = tensor(0.3167, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n36500 iterations have been completed!\n-&gt; Now loss = tensor(0.3167, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n37000 iterations have been completed!\n-&gt; Now loss = tensor(0.3166, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n37500 iterations have been completed!\n-&gt; Now loss = tensor(0.3166, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n38000 iterations have been completed!\n-&gt; Now loss = tensor(0.3165, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n38500 iterations have been completed!\n-&gt; Now loss = tensor(0.3165, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n39000 iterations have been completed!\n-&gt; Now loss = tensor(0.3165, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n39500 iterations have been completed!\n-&gt; Now loss = tensor(0.3164, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n40000 iterations have been completed!\n-&gt; Now loss = tensor(0.3164, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n40500 iterations have been completed!\n-&gt; Now loss = tensor(0.3164, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n41000 iterations have been completed!\n-&gt; Now loss = tensor(0.3163, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n41500 iterations have been completed!\n-&gt; Now loss = tensor(0.3163, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n42000 iterations have been completed!\n-&gt; Now loss = tensor(0.3163, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n42500 iterations have been completed!\n-&gt; Now loss = tensor(0.3162, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n43000 iterations have been completed!\n-&gt; Now loss = tensor(0.3162, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n43500 iterations have been completed!\n-&gt; Now loss = tensor(0.3162, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n44000 iterations have been completed!\n-&gt; Now loss = tensor(0.3161, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n44500 iterations have been completed!\n-&gt; Now loss = tensor(0.3161, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n45000 iterations have been completed!\n-&gt; Now loss = tensor(0.3161, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n45500 iterations have been completed!\n-&gt; Now loss = tensor(0.3160, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n46000 iterations have been completed!\n-&gt; Now loss = tensor(0.3160, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n46500 iterations have been completed!\n-&gt; Now loss = tensor(0.3160, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n47000 iterations have been completed!\n-&gt; Now loss = tensor(0.3160, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n47500 iterations have been completed!\n-&gt; Now loss = tensor(0.3159, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n48000 iterations have been completed!\n-&gt; Now loss = tensor(0.3159, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n48500 iterations have been completed!\n-&gt; Now loss = tensor(0.3159, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n49000 iterations have been completed!\n-&gt; Now loss = tensor(0.3159, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n49500 iterations have been completed!\n-&gt; Now loss = tensor(0.3158, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n50000 iterations have been completed!\n-&gt; Now loss = tensor(0.3158, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n50500 iterations have been completed!\n-&gt; Now loss = tensor(0.3158, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n51000 iterations have been completed!\n-&gt; Now loss = tensor(0.3158, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n51500 iterations have been completed!\n-&gt; Now loss = tensor(0.3157, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n52000 iterations have been completed!\n-&gt; Now loss = tensor(0.3157, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n52500 iterations have been completed!\n-&gt; Now loss = tensor(0.3157, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n53000 iterations have been completed!\n-&gt; Now loss = tensor(0.3157, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n53500 iterations have been completed!\n-&gt; Now loss = tensor(0.3157, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n54000 iterations have been completed!\n-&gt; Now loss = tensor(0.3156, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n54500 iterations have been completed!\n-&gt; Now loss = tensor(0.3156, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n55000 iterations have been completed!\n-&gt; Now loss = tensor(0.3156, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n55500 iterations have been completed!\n-&gt; Now loss = tensor(0.3156, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n56000 iterations have been completed!\n-&gt; Now loss = tensor(0.3156, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n56500 iterations have been completed!\n-&gt; Now loss = tensor(0.3156, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n57000 iterations have been completed!\n-&gt; Now loss = tensor(0.3155, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n57500 iterations have been completed!\n-&gt; Now loss = tensor(0.3155, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n58000 iterations have been completed!\n-&gt; Now loss = tensor(0.3155, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n58500 iterations have been completed!\n-&gt; Now loss = tensor(0.3155, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n59000 iterations have been completed!\n-&gt; Now loss = tensor(0.3155, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n59500 iterations have been completed!\n-&gt; Now loss = tensor(0.3155, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n60000 iterations have been completed!\n-&gt; Now loss = tensor(0.3154, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n60500 iterations have been completed!\n-&gt; Now loss = tensor(0.3154, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n61000 iterations have been completed!\n-&gt; Now loss = tensor(0.3154, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n61500 iterations have been completed!\n-&gt; Now loss = tensor(0.3154, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n62000 iterations have been completed!\n-&gt; Now loss = tensor(0.3154, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n62500 iterations have been completed!\n-&gt; Now loss = tensor(0.3154, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n63000 iterations have been completed!\n-&gt; Now loss = tensor(0.3153, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n63500 iterations have been completed!\n-&gt; Now loss = tensor(0.3153, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n64000 iterations have been completed!\n-&gt; Now loss = tensor(0.3153, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n64500 iterations have been completed!\n-&gt; Now loss = tensor(0.3153, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n65000 iterations have been completed!\n-&gt; Now loss = tensor(0.3153, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n65500 iterations have been completed!\n-&gt; Now loss = tensor(0.3153, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n66000 iterations have been completed!\n-&gt; Now loss = tensor(0.3153, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n66500 iterations have been completed!\n-&gt; Now loss = tensor(0.3153, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n67000 iterations have been completed!\n-&gt; Now loss = tensor(0.3152, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n67500 iterations have been completed!\n-&gt; Now loss = tensor(0.3152, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n68000 iterations have been completed!\n-&gt; Now loss = tensor(0.3152, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n68500 iterations have been completed!\n-&gt; Now loss = tensor(0.3152, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n69000 iterations have been completed!\n-&gt; Now loss = tensor(0.3152, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n69500 iterations have been completed!\n-&gt; Now loss = tensor(0.3152, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n70000 iterations have been completed!\n-&gt; Now loss = tensor(0.3152, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n70500 iterations have been completed!\n-&gt; Now loss = tensor(0.3152, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n71000 iterations have been completed!\n-&gt; Now loss = tensor(0.3151, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n71500 iterations have been completed!\n-&gt; Now loss = tensor(0.3151, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n72000 iterations have been completed!\n-&gt; Now loss = tensor(0.3151, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n72500 iterations have been completed!\n-&gt; Now loss = tensor(0.3151, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n73000 iterations have been completed!\n-&gt; Now loss = tensor(0.3151, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n73500 iterations have been completed!\n-&gt; Now loss = tensor(0.3151, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n74000 iterations have been completed!\n-&gt; Now loss = tensor(0.3151, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n74500 iterations have been completed!\n-&gt; Now loss = tensor(0.3151, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n75000 iterations have been completed!\n-&gt; Now loss = tensor(0.3151, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n75500 iterations have been completed!\n-&gt; Now loss = tensor(0.3151, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n76000 iterations have been completed!\n-&gt; Now loss = tensor(0.3150, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n76500 iterations have been completed!\n-&gt; Now loss = tensor(0.3150, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n77000 iterations have been completed!\n-&gt; Now loss = tensor(0.3150, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n77500 iterations have been completed!\n-&gt; Now loss = tensor(0.3150, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n78000 iterations have been completed!\n-&gt; Now loss = tensor(0.3150, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n78500 iterations have been completed!\n-&gt; Now loss = tensor(0.3150, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n79000 iterations have been completed!\n-&gt; Now loss = tensor(0.3150, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n79500 iterations have been completed!\n-&gt; Now loss = tensor(0.3150, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n80000 iterations have been completed!\n-&gt; Now loss = tensor(0.3150, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n80500 iterations have been completed!\n-&gt; Now loss = tensor(0.3150, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n81000 iterations have been completed!\n-&gt; Now loss = tensor(0.3150, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n81500 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n82000 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n82500 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n83000 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n83500 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n84000 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n84500 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n85000 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n85500 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n86000 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n86500 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n87000 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n87500 iterations have been completed!\n-&gt; Now loss = tensor(0.3149, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n88000 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n88500 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n89000 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n89500 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n90000 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n90500 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n91000 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n91500 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n92000 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n92500 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n93000 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n93500 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n94000 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n94500 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n95000 iterations have been completed!\n-&gt; Now loss = tensor(0.3148, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n95500 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n96000 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n96500 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n97000 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n97500 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n98000 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n98500 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n99000 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n99500 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n100000 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n100500 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n101000 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n101500 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n102000 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n102500 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n103000 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n103500 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n104000 iterations have been completed!\n-&gt; Now loss = tensor(0.3147, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n104500 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n105000 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n105500 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n106000 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n106500 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n107000 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n107500 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n108000 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n108500 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n109000 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n109500 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n110000 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n110500 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n111000 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n111500 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n112000 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n112500 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n113000 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n113500 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n114000 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n114500 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n115000 iterations have been completed!\n-&gt; Now loss = tensor(0.3146, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n115500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n116000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n116500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n117000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n117500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n118000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n118500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n119000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n119500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n120000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n120500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n121000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n121500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n122000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n122500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n123000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n123500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n124000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n124500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n125000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n125500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n126000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n126500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n127000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n127500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n128000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n128500 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n129000 iterations have been completed!\n-&gt; Now loss = tensor(0.3145, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n129500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n130000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n130500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n131000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n131500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n132000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n132500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n133000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n133500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n134000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n134500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n135000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n135500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n136000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n136500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n137000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n137500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n138000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n138500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n139000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n139500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n140000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n140500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n141000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n141500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n142000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n142500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n143000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n143500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n144000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n144500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n145000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n145500 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n146000 iterations have been completed!\n-&gt; Now loss = tensor(0.3144, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n146500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n147000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n147500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n148000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n148500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n149000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n149500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n150000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n150500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n151000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n151500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n152000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n152500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n153000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n153500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n154000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n154500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n155000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n155500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n156000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n156500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n157000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n157500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n158000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n158500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n159000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n159500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n160000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n160500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n161000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n161500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n162000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n162500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n163000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n163500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n164000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n164500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n165000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n165500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n166000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n166500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n167000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n167500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n168000 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n168500 iterations have been completed!\n-&gt; Now loss = tensor(0.3143, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n169000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n169500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n170000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n170500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n171000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n171500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n172000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n172500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n173000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n173500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n174000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n174500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n175000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n175500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n176000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n176500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n177000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n177500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n178000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n178500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n179000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n179500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n180000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n180500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n181000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n181500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n182000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n182500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n183000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n183500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n184000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n184500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n185000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n185500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n186000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n186500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n187000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n187500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n188000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n188500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n189000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n189500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n190000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n190500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n191000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n191500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n192000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n192500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n193000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n193500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n194000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n194500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n195000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n195500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n196000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n196500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n197000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n197500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n198000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n198500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n199000 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n199500 iterations have been completed!\n-&gt; Now loss = tensor(0.3142, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n200000 iterations have been completed!\n-&gt; Now loss = tensor(0.3141, grad_fn=&lt;NllLossBackward0&gt;)\n=========================================\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_DNN/pytorch_DNN/#4-visualization-of-the-cross-entropy-loss-function","title":"4. Visualization of the Cross Entropy Loss Function","text":"<pre><code>plt.figure(figsize = (12, 6))\nlength = len(loss_list)\nprint(\"The length of loss_list is:\", length)\nplt.plot(np.arange(1, 20001, 1), [loss.detach().numpy() for loss in loss_list[0:20000]], \"black\") \nplt.xlabel(\"epoch\") \nplt.ylabel(\"loss\")\nplt.show()\n</code></pre> <pre><code>The length of loss_list is: 200000\n</code></pre> <pre><code># \u5f00\u59cb\u6d4b\u8bd5\nX_vec_test = torch.tensor(DataTest[[\"x1\",\"x2\"]].values, dtype=torch.float32)  # N*2\ny_vec_test = torch.tensor(DataTest[\"y\"].values, dtype=torch.int64).reshape(-1, 1)  # N*1\n\npred = DNN_Model.forward(X_vec_test)\npred_vec = pred [:, 1]\npred_vec[pred_vec &gt; 0.50] = 1\npred_vec[pred_vec &lt;= 0.50 ] = 0\n</code></pre> <pre><code>pred_vec\n</code></pre> <pre><code>tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       grad_fn=&lt;AsStridedBackward0&gt;)\n</code></pre> <pre><code>y_pred_np = y_vec_test.detach().numpy()\ny_pred_np = np.squeeze(y_pred_np)\nprint(\"Shape of y_pred_p:\", y_pred_np.shape)\n\npred_vec_np = pred_vec.detach().numpy()\npred_vec_np = np. squeeze (pred_vec_np)\nprint (\"Shape of y_pred_p:\", pred_vec_np.shape)\n</code></pre> <pre><code>Shape of y_pred_p: (500,)\nShape of y_pred_p: (500,)\n</code></pre> <pre><code>accuracy = accuracy_score (y_vec_test, y_pred_np)\nprint (\"The accuracy score is:\", accuracy)\n</code></pre> <pre><code>The accuracy score is: 1.0\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_logistic_regerssion/pytorch_logistic_regerssion/","title":"Pytorch logistic regression","text":""},{"location":"Programming/MachineLearning/pytorch_logistic_regerssion/pytorch_logistic_regerssion/#1-fire-up","title":"1. Fire up","text":"<pre><code>import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_logistic_regerssion/pytorch_logistic_regerssion/#2-data-generating","title":"2. Data generating","text":"<pre><code>from sklearn.datasets import load_breast_cancer\n</code></pre> <pre><code>data = load_breast_cancer()\n</code></pre> <pre><code>X = data.data # extract target\ny = data.target.reshape(-1, 1) # extract label\n</code></pre> <p><code>reshape(-1, 1)</code>\u5c06\u76ee\u6807\u53d8\u91cf\u4ece\u4e00\u7ef4\u6570\u7ec4\u8f6c\u6362\u4e3a\u4e8c\u7ef4\u6570\u7ec4\u7684\u5217\u5411\u91cf\u5f62\u5f0f,\u662fNumPy\u6570\u7ec4\u7684\u4e00\u79cd\u5f62\u72b6\u53d8\u6362\u64cd\u4f5c\u3002\u5b83\u7684\u4f5c\u7528\u662f\u5c06\u6570\u7ec4\u4ece\u539f\u59cb\u5f62\u72b6\u8f6c\u6362\u4e3a\u65b0\u7684\u5f62\u72b6\uff0c\u5176\u4e2d\u7684 -1 \u8868\u793a\u81ea\u52a8\u63a8\u65ad\u7ef4\u5ea6\u5927\u5c0f\u3002</p> <p>\u8fd9\u6837\u5904\u7406\u662f\u56e0\u4e3a\u5728scikit-learn\u4e2d\uff0c\u76ee\u6807\u53d8\u91cf\u901a\u5e38\u9700\u8981\u662f\u4e00\u4e2a\u4e8c\u7ef4\u6570\u7ec4\u7684\u5217\u5411\u91cf\uff0c\u5176\u4e2d\u6bcf\u884c\u5bf9\u5e94\u4e00\u4e2a\u6837\u672c\u7684\u6807\u7b7e\u3002</p> <p>\u5177\u4f53\u5730\uff0c<code>reshape(-1, 1)</code>\u4f1a\u5c06\u6570\u7ec4\u7684\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u8bbe\u4e3a\u81ea\u52a8\u63a8\u65ad\u7684\u503c\uff0c\u800c\u5c06\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u8bbe\u4e3a1\u3002\u8fd9\u6837\u7684\u64cd\u4f5c\u5b9e\u9645\u4e0a\u662f\u5c06\u539f\u59cb\u6570\u7ec4\u8f6c\u6362\u4e3a\u4e00\u4e2a\u5217\u5411\u91cf\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5143\u7d20\u72ec\u7acb\u5360\u636e\u4e00\u884c\u3002</p> <p>\u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5047\u8bbe\u6709\u4e00\u4e2a\u4e00\u7ef4\u6570\u7ec4 <code>arr = [1, 2, 3, 4, 5]</code>\uff0c\u901a\u8fc7 <code>arr.reshape(-1, 1)</code> \u64cd\u4f5c\uff0c\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u5217\u5411\u91cf\uff1a</p> <pre><code>[[1],\n [2],\n [3],\n [4],\n [5]]\n</code></pre> <pre><code>print(X.shape) # 569\u4e2a30\u7ef4\u5411\u91cf\nprint(y.shape) # 569\u4e2a1\u7ef4\u5411\u91cf\n</code></pre> <pre><code>(569, 30)\n(569, 1)\n</code></pre> <pre><code># \u5c06\u6570\u7ec4\u6216\u77e9\u9635\u8fdb\u884c\u5207\u7247\u64cd\u4f5c\uff0c\u7528\u4e8e\u5c06\u6570\u636e\u96c6\u5206\u5272\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\nX_train = X[:500,:] # \u5bf9\u77e9\u9635X\u8fdb\u884c\u5207\u7247\uff0c\u9009\u62e9\u7b2c0\u884c\u5230\u7b2c499\u884c\uff08\u5171500\u884c\uff09\u7684\u6240\u6709\u5217\uff0c\u5373\u9009\u53d6X\u7684\u524d500\u884c\u4f5c\u4e3a\u8bad\u7ec3\u96c6\ny_train = y[:500,:]\n\nX_test = X[500:,:] # \u5bf9\u77e9\u9635X\u8fdb\u884c\u5207\u7247\uff0c\u9009\u62e9\u4ece\u7b2c500\u884c\u5f00\u59cb\u5230\u6700\u540e\u4e00\u884c\u7684\u6240\u6709\u5217\uff0c\u5373\u9009\u53d6X\u7684\u7b2c500\u884c\u53ca\u4ee5\u540e\u7684\u6240\u6709\u884c\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\ny_test = y[500:,:]\n</code></pre> <p>\u5728\u8fd9\u91cc\uff0c<code>:</code>\u8868\u793a\u9009\u53d6\u6240\u6709\u5143\u7d20\u7684\u8303\u56f4\uff0c\u7528\u4e8e\u6307\u5b9a\u5207\u7247\u7684\u8d77\u59cb\u548c\u7ed3\u675f\u4f4d\u7f6e\uff0c\u5982<code>a[:500]</code>\u8868\u793a\u9009\u53d6\u6570\u7ec4a\u7684\u524d500\u4e2a\u5143\u7d20\u3002</p> <p>\u5bf9\u4e8e\u4e8c\u7ef4\u6570\u7ec4\u6216\u77e9\u9635\uff0c\u53ef\u4ee5\u4f7f\u7528<code>:</code>\u5bf9\u884c\u548c\u5217\u540c\u65f6\u8fdb\u884c\u5207\u7247\u64cd\u4f5c\uff0c\u5982<code>a[:500, :]</code>\u8868\u793a\u9009\u53d6\u524d500\u884c\u7684\u6240\u6709\u5217\u3002</p> <pre><code># \u5c06 NumPy \u6570\u7ec4\u8f6c\u6362\u4e3a Pandas DataFrame \u5bf9\u8c61\ntraining_X = pd.DataFrame(X_train)\ntraining_y = pd.DataFrame(y_train)\ntest_X = pd.DataFrame(X_test)\ntest_y = pd.DataFrame(y_test)\n</code></pre> <p>DataFrame\u7c7b\u4f3c\u4e8e\u8868\u683c\u6216\u7535\u5b50\u8868\u683c\uff0c\u53ef\u4ee5\u5b58\u50a8\u548c\u64cd\u4f5c\u4e8c\u7ef4\u6570\u636e\u3002</p> <p>\u8fd9\u6837\u505a\u7684\u76ee\u7684\u53ef\u80fd\u662f\u4e3a\u4e86\u65b9\u4fbf\u540e\u7eed\u4f7f\u7528 Pandas \u63d0\u4f9b\u7684\u6570\u636e\u5206\u6790\u548c\u5904\u7406\u529f\u80fd\uff0c\u6bd4\u5982\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\u3001\u7279\u5f81\u5de5\u7a0b\u3001\u6a21\u578b\u8bad\u7ec3\u7b49\u3002</p> <p>Pandas \u7684 DataFrame \u63d0\u4f9b\u4e86\u66f4\u591a\u7075\u6d3b\u7684\u6570\u636e\u64cd\u4f5c\u548c\u5206\u6790\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u65b9\u4fbf\u5730\u5904\u7406\u548c\u63a2\u7d22\u6570\u636e\u3002</p> <pre><code>training_X\n</code></pre> 0 1 2 3 4 5 6 7 8 9 ... 20 21 22 23 24 25 26 27 28 29 0 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.30010 0.14710 0.2419 0.07871 ... 25.38 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 1 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.08690 0.07017 0.1812 0.05667 ... 24.99 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 2 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.19740 0.12790 0.2069 0.05999 ... 23.57 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 3 11.42 20.38 77.58 386.1 0.14250 0.28390 0.24140 0.10520 0.2597 0.09744 ... 14.91 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 4 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.19800 0.10430 0.1809 0.05883 ... 22.54 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 495 14.87 20.21 96.12 680.9 0.09587 0.08345 0.06824 0.04951 0.1487 0.05748 ... 16.01 28.48 103.90 783.6 0.1216 0.1388 0.1700 0.1017 0.2369 0.06599 496 12.65 18.17 82.69 485.6 0.10760 0.13340 0.08017 0.05074 0.1641 0.06854 ... 14.38 22.15 95.29 633.7 0.1533 0.3842 0.3582 0.1407 0.3230 0.10330 497 12.47 17.31 80.45 480.1 0.08928 0.07630 0.03609 0.02369 0.1526 0.06046 ... 14.06 24.34 92.82 607.3 0.1276 0.2506 0.2028 0.1053 0.3035 0.07661 498 18.49 17.52 121.30 1068.0 0.10120 0.13170 0.14910 0.09183 0.1832 0.06697 ... 22.75 22.88 146.40 1600.0 0.1412 0.3089 0.3533 0.1663 0.2510 0.09445 499 20.59 21.24 137.80 1320.0 0.10850 0.16440 0.21880 0.11210 0.1848 0.06222 ... 23.86 30.76 163.20 1760.0 0.1464 0.3597 0.5179 0.2113 0.2480 0.08999 <p>500 rows \u00d7 30 columns</p> <pre><code>test_X\n</code></pre> 0 1 2 3 4 5 6 7 8 9 ... 20 21 22 23 24 25 26 27 28 29 0 15.040 16.74 98.73 689.4 0.09883 0.13640 0.07721 0.06142 0.1668 0.06869 ... 16.760 20.43 109.70 856.9 0.11350 0.21760 0.1856 0.10180 0.2177 0.08549 1 13.820 24.49 92.33 595.9 0.11620 0.16810 0.13570 0.06759 0.2275 0.07237 ... 16.010 32.94 106.00 788.0 0.17940 0.39660 0.3381 0.15210 0.3651 0.11830 2 12.540 16.32 81.25 476.3 0.11580 0.10850 0.05928 0.03279 0.1943 0.06612 ... 13.570 21.40 86.67 552.0 0.15800 0.17510 0.1889 0.08411 0.3155 0.07538 3 23.090 19.83 152.10 1682.0 0.09342 0.12750 0.16760 0.10030 0.1505 0.05484 ... 30.790 23.87 211.50 2782.0 0.11990 0.36250 0.3794 0.22640 0.2908 0.07277 4 9.268 12.87 61.49 248.7 0.16340 0.22390 0.09730 0.05252 0.2378 0.09502 ... 10.280 16.38 69.05 300.2 0.19020 0.34410 0.2099 0.10250 0.3038 0.12520 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 64 21.560 22.39 142.00 1479.0 0.11100 0.11590 0.24390 0.13890 0.1726 0.05623 ... 25.450 26.40 166.10 2027.0 0.14100 0.21130 0.4107 0.22160 0.2060 0.07115 65 20.130 28.25 131.20 1261.0 0.09780 0.10340 0.14400 0.09791 0.1752 0.05533 ... 23.690 38.25 155.00 1731.0 0.11660 0.19220 0.3215 0.16280 0.2572 0.06637 66 16.600 28.08 108.30 858.1 0.08455 0.10230 0.09251 0.05302 0.1590 0.05648 ... 18.980 34.12 126.70 1124.0 0.11390 0.30940 0.3403 0.14180 0.2218 0.07820 67 20.600 29.33 140.10 1265.0 0.11780 0.27700 0.35140 0.15200 0.2397 0.07016 ... 25.740 39.42 184.60 1821.0 0.16500 0.86810 0.9387 0.26500 0.4087 0.12400 68 7.760 24.54 47.92 181.0 0.05263 0.04362 0.00000 0.00000 0.1587 0.05884 ... 9.456 30.37 59.16 268.6 0.08996 0.06444 0.0000 0.00000 0.2871 0.07039 <p>69 rows \u00d7 30 columns</p>"},{"location":"Programming/MachineLearning/pytorch_logistic_regerssion/pytorch_logistic_regerssion/#3-model-training","title":"3. Model training","text":"\\[\\vec{x}\\in\\mathbb{R}^{1\\times 30}\\] \\[W\\in\\mathbb{R}^{30\\times 1}\\] \\[b\\in\\mathbb{R}\\] \\[z = \\vec{x}W+b\\] \\[\\hat{y} = \\sigma(z) = \\frac{1}{1+e^{-z}}\\] <p>Cross Entropy Loss Function</p> \\[Loss(W\uff0cb) = -\\frac{1}{N}\\sum_{i = 1}^{N}[y_i\\log\\hat{y_i} + (1 - y_i)\\log (1-\\hat{y_i})]\\] \\[k = 0,1,2,...\\] \\[W^{(k+1)}= W^{(k)} - \\alpha_k\\frac{\\partial Loss (W^{(k)}, b^{(k)})}{\\partial W}\\] \\[b^{(k+1)}= b^{(k)} - \\alpha_k\\frac{\\partial Loss (W^{(k)}, b^{(k)})}{\\partial b}\\] <pre><code># \u4f7f\u7528 PyTorch \u5b9a\u4e49\u4e86\u4e24\u4e2a\u53ef\u8bad\u7ec3\u7684\u53d8\u91cf\uff08\u53c2\u6570\uff09\uff1aW \u548c b\nW = torch.tensor(np.zeros((30, 1)), dtype = torch.float32, requires_grad = True)\nb = torch.tensor(np.array([0]), dtype = torch.float32, requires_grad = True)\n</code></pre> <p>\u4f7f\u7528 <code>torch.tensor</code> \u521b\u5efa\u5f20\u91cf\uff0c\u5e76\u4f20\u5165 <code>np.zeros((30, 1))</code> \u6765\u521d\u59cb\u5316\u6743\u91cd W\uff0c\u8868\u793a\u4e00\u4e2a\u5927\u5c0f\u4e3a 30x1 \u7684\u5168\u96f6\u5f20\u91cf\u3002</p> <p>\u4f7f\u7528 <code>dtype=torch.float32</code> \u8bbe\u7f6e\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b\u4e3a float32\u3002</p> <p>\u8bbe\u7f6e <code>requires_grad = True</code>\uff0c\u4ee5\u4fbf\u5728\u8ba1\u7b97\u4e2d\u8ddf\u8e2a\u5e76\u8ba1\u7b97\u68af\u5ea6\u3002</p> <p>\u6ce8\u610f\uff1a\u5728\u65b0\u7248\u672c\u7684 PyTorch \u4e2d\uff0c<code>Variable</code> \u88ab\u5e9f\u5f03\u4e86\uff0c\u76f4\u63a5\u4f7f\u7528 <code>torch.tensor</code> \u521b\u5efa\u5f20\u91cf\u5373\u53ef\u3002</p> <pre><code>print(\"Initial value of w =\", W.T)\nprint(\"---------------------------------------------------------\")\nprint(\"Initial value of b =\", b)\n</code></pre> <pre><code>Initial value of w = tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0.]], grad_fn=&lt;PermuteBackward0&gt;)\n---------------------------------------------------------\nInitial value of b = tensor([0.], requires_grad=True)\n</code></pre> <pre><code># \u5c06 training_X \u548c training_y \u5411\u91cf\u5316\uff08vertorization\uff09\u5904\u7406\nX_vec = torch.tensor(training_X.values, dtype=torch.float32)  # N*30\ny_vec = torch.tensor(training_y.values, dtype=torch.float32).reshape(-1, 1)  # N*1\n</code></pre> <p>\u4f7f\u7528 <code>torch.tensor</code> \u521b\u5efa\u5f20\u91cf\uff0c\u5e76\u4f20\u5165 <code>training_X.values</code> \u5c06 <code>training_X</code> \u8f6c\u6362\u4e3a\u5f20\u91cf <code>X_vec</code>\uff0c\u6570\u636e\u7c7b\u578b\u8bbe\u7f6e\u4e3a float32\u3002</p> <p>\u4f7f\u7528 <code>torch.tensor</code> \u521b\u5efa\u5f20\u91cf\uff0c\u5e76\u4f20\u5165 <code>training_y.values</code> \u5c06 <code>training_y</code> \u8f6c\u6362\u4e3a\u5f20\u91cf <code>y_vec</code>\uff0c\u6570\u636e\u7c7b\u578b\u8bbe\u7f6e\u4e3a float32\uff0c\u5e76\u4f7f\u7528 <code>reshape(-1, 1)</code> \u5c06\u5176\u5f62\u72b6\u6539\u4e3a <code>N*1</code>\uff0c\u5176\u4e2d N \u8868\u793a\u6837\u672c\u6570\u91cf\u3002</p> <pre><code>X_vec.shape\n</code></pre> <pre><code>torch.Size([500, 30])\n</code></pre> <pre><code>y_vec.shape\n</code></pre> <pre><code>torch.Size([500, 1])\n</code></pre> <pre><code>Iter_times = 100000 # \u8fed\u4ee3\u6b21\u6570\nalpha = 0.000015 # \u8bbe\u7f6e\u521d\u59cb\u5b66\u4e60\u7387\nloss_list = [] # \u50a8\u5b58\u6bcf\u4e00\u6b21\u8fed\u4ee3\u540e\u7684\u635f\u5931\u51fd\u6570\uff08\u5982\u679c\u9012\u51cf\uff0c\u8bf4\u660e\u68af\u5ea6\u4e0b\u964d\u662f\u6210\u529f\u7684\uff09\n\nfor i in range(Iter_times):\n    z = torch.mm(X_vec, w) + b # mm\uff1a\u77e9\u9635\u4e58\u6cd5\uff0c500*1\n    y_hat = torch.sigmoid(z) # 500*1\n\n    # \u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\n    loss_vec = -(y_vec * torch.log(y_hat) + (1.0 - y_vec) * torch.log (1.0 - y_hat))\n    loss = torch.mean(loss_vec) # \u53d6\u5e73\u5747\n\n    # \u6c42\u5bfc\n    loss.backward()\n\n    # \u83b7\u53d6\u68af\u5ea6\n    grad_w = w.grad.data\n    grad_b = b.grad.data\n\n    # \u5b66\u4e60\u7387\n    alpha_temp = alpha / (1 + 0.001 * i)\n\n    # \u7528alpha\u8c03\u6574\u68af\u5ea6\n    w.data = w.data - alpha_temp * grad_w\n    b.data = b.data - alpha_temp * grad_b\n\n    # \u5f00\u59cb\u4e0b\u6b21\u68af\u5ea6\u4e4b\u524d\u8981\u6e05\u96f6\uff08\u5426\u5219\u4f1a\u4e0d\u6536\u655b\uff09\n    w.grad.data.zero_()\n    b.grad.data.zero_()\n\n    # \u8f93\u51fa\n    print(i + 1, \"iterations have been completed!\")\n    print(\"      -&gt; Now w1 =\", w[0,0])\n    print(\"      -&gt; Now w2 =\", w[1,0])\n    print(\"      -&gt; Now b =\", b[0])\n    print(\"      -&gt; Now Loss =\", loss)\n    print(\"---------------------------------------------------------\")\n\n    loss_list.append(loss)\n    length = len(loss_list)\n\n    # \u5f53\u635f\u5931\u51fd\u6570\u7684\u53d8\u5316 &lt; 10^-5 \u65f6\uff0c\u8ba4\u4e3a\u6a21\u578b\u5df2\u7ecf\u6536\u655b\u5230\u4e00\u4e2a\u8f83\u597d\u7684\u89e3\uff0c\u65e0\u9700\u7ee7\u7eed\u8fed\u4ee3\n    if(torch.abs(loss_list[length - 1] - loss_list[length - 2]) &lt; 10 ** (-5) and length &gt;= 2):\n        break\n</code></pre> <pre><code>1 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(1.2350e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(2.8345, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2 iterations have been completed!\n      -&gt; Now w1 = tensor(8.6309e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(1.2469e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7095, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n3 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(2.0564e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.4803, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n4 iterations have been completed!\n      -&gt; Now w1 = tensor(9.7714e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(1.5858e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.9152, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n5 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(2.4791e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(2.8352, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n6 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(2.5471e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6368, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n7 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(3.2530e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.0426, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n8 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(2.7652e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.9973, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n9 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(3.6527e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(2.7423, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n10 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(3.6926e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6151, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n11 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(4.3940e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.0319, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n12 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(3.9090e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.8925, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n13 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(4.7919e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(2.7136, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n14 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(4.8531e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5777, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n15 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(5.4751e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.8452, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n16 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(5.0012e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.7197, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n17 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(5.8802e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(2.7031, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n18 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(5.9709e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5445, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n19 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0010, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(6.4862e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6815, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n20 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(6.0524e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.3842, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n21 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0010, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(6.9268e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(2.6696, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n22 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0010, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(7.0253e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5230, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n23 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0011, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(7.4795e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6129, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n24 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0010, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(7.0936e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.1365, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n25 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0012, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(7.9587e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(2.4876, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n26 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0012, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(7.9610e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5413, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n27 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0013, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(8.5724e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.8255, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n28 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0012, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(8.1163e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.4633, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n29 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0013, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(8.9789e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(2.5229, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n30 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0013, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(9.0345e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5025, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n31 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0014, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(9.5107e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6230, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n32 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0013, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(9.1218e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.1088, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n33 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0015, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(9.9724e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(2.3022, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n34 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0014, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(9.9157e-05, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5447, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n35 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0015, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.9003, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n36 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0014, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.3358, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n37 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0016, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(2.3390, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n38 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0016, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5018, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n39 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0010, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0017, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6953, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n40 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0016, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.1394, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n41 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0010, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0017, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(2.1576, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n42 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0010, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0017, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5405, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n43 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0010, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0018, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.8840, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n44 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0010, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0017, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.1882, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n45 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0011, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0019, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(2.1067, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n46 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0010, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0018, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5315, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n47 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0011, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.8397, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n48 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0010, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0019, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.1149, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n49 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0011, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.9685, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n50 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0011, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5624, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n51 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0012, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.9378, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n52 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0011, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.0626, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n53 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0012, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.8434, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n54 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0012, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5908, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n55 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0013, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.0015, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n56 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0012, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0001, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.9963, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n57 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0013, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.7073, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n58 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0012, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6266, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n59 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0013, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.0660, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n60 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0013, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.9267, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n61 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0014, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.5658, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n62 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0013, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6652, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n63 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0014, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.1180, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n64 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0013, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.8610, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n65 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0014, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.4258, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n66 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0014, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7004, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n67 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0015, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.1473, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n68 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0014, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.8065, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n69 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0015, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.2985, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n70 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0014, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7239, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n71 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0015, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.1463, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n72 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0015, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7671, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n73 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0016, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.1932, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n74 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0015, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7316, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n75 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0016, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.1162, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n76 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0015, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7412, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n77 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0016, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.1103, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n78 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0016, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7261, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n79 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0017, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.0669, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n80 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0016, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7225, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n81 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0017, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.0420, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n82 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0016, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7133, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n83 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0017, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(1.0094, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n84 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0017, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7058, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n85 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0017, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.9801, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n86 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0017, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6972, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n87 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0018, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.9502, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n88 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0017, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6883, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n89 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0018, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.9208, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n90 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0018, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6791, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n91 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0018, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.8917, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n92 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0018, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6694, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n93 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0019, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.8629, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n94 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0018, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6592, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n95 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0019, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.8346, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n96 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0018, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6487, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n97 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0019, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.8066, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n98 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0019, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6377, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n99 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0019, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7790, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n100 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0019, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6263, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n101 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7519, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n102 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0019, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6145, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n103 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.7254, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n104 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0019, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6023, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n105 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6993, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n106 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5897, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n107 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6739, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n108 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0002, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5767, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n109 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6490, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n110 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5634, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n111 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6247, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n112 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5498, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n113 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.6011, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n114 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5359, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n115 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5781, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n116 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5216, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n117 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5558, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n118 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5072, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n119 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5342, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n120 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4926, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n121 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.5133, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n122 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0021, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4780, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n123 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4933, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n124 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4634, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n125 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4743, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n126 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4492, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n127 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4564, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n128 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4354, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n129 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4398, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n130 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4224, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n131 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4245, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n132 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4103, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n133 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.4109, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n134 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0022, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3992, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n135 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3988, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n136 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3894, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n137 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3882, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n138 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3807, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n139 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3792, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n140 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3732, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n141 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3715, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n142 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3668, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n143 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3650, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n144 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3613, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n145 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3597, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n146 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0023, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3568, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n147 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3553, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n148 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3530, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n149 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3517, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n150 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3499, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n151 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3488, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n152 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3474, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n153 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3464, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n154 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3453, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n155 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3445, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n156 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3436, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n157 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3429, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n158 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3422, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n159 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3416, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n160 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3411, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n161 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0024, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3405, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n162 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3401, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n163 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3396, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n164 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3392, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n165 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3388, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n166 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3384, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n167 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3380, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n168 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3377, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n169 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3373, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n170 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3370, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n171 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3367, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n172 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3363, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n173 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3360, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n174 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3357, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n175 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3354, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n176 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3351, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n177 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0025, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3348, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n178 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3345, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n179 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3341, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n180 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3338, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n181 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3335, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n182 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3332, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n183 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3330, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n184 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3327, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n185 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3324, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n186 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3321, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n187 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3318, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n188 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3315, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n189 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3312, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n190 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3309, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n191 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3306, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n192 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3303, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n193 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0026, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3301, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n194 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3298, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n195 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3295, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n196 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3292, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n197 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3289, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n198 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3286, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n199 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3284, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n200 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3281, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n201 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3278, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n202 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3275, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n203 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3273, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n204 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3270, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n205 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3267, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n206 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3265, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n207 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0003, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3262, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n208 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3259, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n209 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3257, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n210 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0027, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3254, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n211 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3251, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n212 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3249, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n213 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3246, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n214 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3243, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n215 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3241, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n216 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3238, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n217 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3236, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n218 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3233, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n219 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3230, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n220 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3228, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n221 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3225, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n222 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3223, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n223 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3220, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n224 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3218, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n225 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3215, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n226 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3213, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n227 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0028, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3210, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n228 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3208, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n229 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3205, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n230 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3203, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n231 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3200, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n232 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3198, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n233 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3196, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n234 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3193, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n235 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3191, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n236 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3188, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n237 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3186, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n238 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3184, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n239 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3181, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n240 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3179, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n241 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3176, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n242 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3174, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n243 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3172, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n244 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3169, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n245 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0029, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3167, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n246 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3165, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n247 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3163, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n248 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3160, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n249 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3158, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n250 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3156, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n251 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3153, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n252 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3151, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n253 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3149, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n254 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3147, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n255 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3145, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n256 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3142, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n257 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3140, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n258 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3138, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n259 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3136, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n260 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3134, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n261 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3131, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n262 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3129, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n263 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3127, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n264 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0030, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3125, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n265 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3123, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n266 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3121, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n267 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3119, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n268 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3116, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n269 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3114, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n270 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3112, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n271 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3110, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n272 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3108, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n273 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3106, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n274 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3104, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n275 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3102, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n276 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3100, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n277 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3098, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n278 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3096, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n279 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3094, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n280 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3092, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n281 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3090, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n282 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3088, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n283 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0031, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3086, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n284 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3084, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n285 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3082, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n286 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3080, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n287 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3078, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n288 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3076, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n289 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3074, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n290 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3072, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n291 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3070, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n292 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3068, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n293 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3066, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n294 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3064, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n295 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3062, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n296 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3061, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n297 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3059, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n298 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3057, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n299 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3055, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n300 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3053, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n301 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3051, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n302 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3049, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n303 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3047, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n304 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0032, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3046, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n305 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3044, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n306 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3042, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n307 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3040, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n308 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3038, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n309 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3037, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n310 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3035, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n311 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3033, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n312 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3031, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n313 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3029, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n314 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3028, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n315 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3026, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n316 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3024, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n317 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3022, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n318 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3021, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n319 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3019, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n320 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3017, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n321 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3016, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n322 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3014, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n323 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3012, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n324 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3010, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n325 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0033, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3009, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n326 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3007, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n327 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3005, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n328 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3004, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n329 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3002, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n330 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.3000, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n331 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2999, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n332 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2997, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n333 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2996, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n334 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2994, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n335 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2992, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n336 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2991, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n337 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2989, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n338 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2987, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n339 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2986, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n340 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2984, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n341 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2983, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n342 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2981, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n343 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2980, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n344 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2978, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n345 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2976, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n346 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2975, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n347 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2973, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n348 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0034, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2972, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n349 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0004, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2970, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n350 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2969, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n351 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2967, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n352 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2966, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n353 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2964, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n354 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2963, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n355 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2961, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n356 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2960, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n357 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2958, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n358 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2957, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n359 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2955, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n360 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2954, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n361 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2952, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n362 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2951, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n363 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2949, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n364 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2948, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n365 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2946, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n366 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2945, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n367 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2944, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n368 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2942, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n369 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2941, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n370 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2939, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n371 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0035, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2938, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n372 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2936, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n373 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2935, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n374 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2934, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n375 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2932, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n376 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2931, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n377 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2929, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n378 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2928, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n379 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2927, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n380 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2925, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n381 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2924, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n382 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2923, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n383 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2921, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n384 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2920, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n385 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2919, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n386 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2917, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n387 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2916, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n388 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2915, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n389 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2913, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n390 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2912, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n391 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2911, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n392 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2909, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n393 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2908, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n394 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2907, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n395 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2905, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n396 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0036, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2904, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n397 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2903, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n398 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2902, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n399 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2900, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n400 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2899, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n401 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2898, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n402 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2897, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n403 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2895, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n404 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2894, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n405 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2893, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n406 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2892, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n407 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2890, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n408 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2889, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n409 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2888, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n410 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2887, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n411 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2885, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n412 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2884, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n413 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2883, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n414 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2882, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n415 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2881, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n416 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2879, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n417 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2878, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n418 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2877, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n419 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2876, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n420 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2875, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n421 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2873, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n422 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0037, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2872, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n423 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2871, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n424 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2870, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n425 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2869, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n426 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2868, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n427 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2866, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n428 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2865, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n429 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2864, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n430 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2863, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n431 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2862, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n432 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2861, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n433 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2860, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n434 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0066, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2859, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n435 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2857, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n436 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2856, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n437 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2855, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n438 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2854, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n439 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2853, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n440 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2852, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n441 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2851, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n442 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2850, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n443 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2849, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n444 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2848, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n445 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2846, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n446 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2845, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n447 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2844, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n448 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2843, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n449 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0038, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2842, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n450 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2841, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n451 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2840, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n452 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0067, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2839, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n453 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2838, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n454 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2837, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n455 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2836, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n456 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2835, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n457 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2834, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n458 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2833, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n459 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2832, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n460 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2831, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n461 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2830, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n462 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2829, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n463 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2828, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n464 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2827, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n465 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2826, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n466 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2825, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n467 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2824, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n468 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2823, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n469 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2822, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n470 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0068, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2821, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n471 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2820, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n472 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2819, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n473 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2818, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n474 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2817, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n475 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2816, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n476 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2815, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n477 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0039, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2814, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n478 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2813, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n479 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2812, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n480 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2811, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n481 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2810, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n482 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2809, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n483 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2808, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n484 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2807, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n485 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2806, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n486 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2805, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n487 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2804, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n488 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2803, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n489 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0069, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2802, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n490 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2801, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n491 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2800, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n492 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2799, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n493 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2798, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n494 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2798, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n495 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2797, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n496 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2796, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n497 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2795, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n498 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2794, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n499 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2793, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n500 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2792, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n501 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2791, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n502 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2790, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n503 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2789, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n504 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2788, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n505 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2788, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n506 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2787, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n507 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0040, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2786, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n508 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2785, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n509 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0070, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2784, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n510 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2783, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n511 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2782, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n512 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2781, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n513 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2780, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n514 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2780, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n515 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2779, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n516 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2778, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n517 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2777, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n518 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2776, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n519 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2775, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n520 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2774, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n521 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2774, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n522 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2773, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n523 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2772, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n524 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2771, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n525 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2770, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n526 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2769, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n527 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2769, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n528 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2768, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n529 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2767, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n530 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0071, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2766, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n531 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2765, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n532 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2764, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n533 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2764, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n534 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2763, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n535 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2762, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n536 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2761, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n537 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2760, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n538 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2759, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n539 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0041, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2759, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n540 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2758, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n541 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2757, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n542 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2756, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n543 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2755, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n544 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2755, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n545 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0005, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2754, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n546 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2753, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n547 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2752, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n548 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2751, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n549 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2751, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n550 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2750, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n551 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2749, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n552 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0072, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2748, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n553 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2748, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n554 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2747, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n555 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2746, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n556 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2745, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n557 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2744, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n558 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2744, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n559 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2743, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n560 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2742, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n561 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2741, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n562 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2741, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n563 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2740, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n564 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2739, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n565 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2738, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n566 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2738, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n567 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2737, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n568 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2736, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n569 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2735, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n570 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2735, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n571 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2734, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n572 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0042, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2733, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n573 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2732, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n574 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0073, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2732, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n575 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2731, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n576 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2730, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n577 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2729, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n578 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2729, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n579 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2728, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n580 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2727, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n581 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2727, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n582 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2726, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n583 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2725, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n584 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2724, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n585 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2724, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n586 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2723, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n587 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2722, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n588 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2722, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n589 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2721, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n590 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2720, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n591 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2719, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n592 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2719, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n593 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2718, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n594 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2717, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n595 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2717, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n596 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2716, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n597 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2715, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n598 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0074, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2715, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n599 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2714, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n600 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2713, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n601 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2713, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n602 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2712, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n603 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2711, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n604 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2710, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n605 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2710, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n606 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2709, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n607 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0043, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2708, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n608 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2708, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n609 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2707, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n610 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2706, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n611 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2706, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n612 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2705, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n613 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2704, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n614 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2704, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n615 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2703, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n616 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2702, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n617 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2702, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n618 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2701, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n619 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2700, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n620 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2700, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n621 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2699, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n622 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2699, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n623 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0075, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2698, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n624 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2697, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n625 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2697, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n626 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2696, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n627 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2695, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n628 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2695, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n629 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2694, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n630 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2693, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n631 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2693, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n632 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2692, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n633 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2691, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n634 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2691, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n635 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2690, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n636 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2690, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n637 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2689, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n638 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2688, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n639 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2688, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n640 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2687, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n641 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2686, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n642 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2686, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n643 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2685, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n644 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0044, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2685, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n645 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2684, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n646 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2683, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n647 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2683, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n648 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2682, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n649 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0076, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2682, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n650 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2681, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n651 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2680, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n652 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2680, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n653 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2679, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n654 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2679, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n655 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2678, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n656 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2677, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n657 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2677, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n658 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2676, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n659 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2676, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n660 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2675, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n661 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2674, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n662 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2674, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n663 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2673, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n664 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2673, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n665 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2672, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n666 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2671, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n667 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2671, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n668 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2670, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n669 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2670, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n670 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2669, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n671 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2668, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n672 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2668, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n673 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2667, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n674 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2667, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n675 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2666, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n676 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0077, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2666, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n677 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2665, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n678 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2664, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n679 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2664, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n680 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2663, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n681 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2663, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n682 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2662, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n683 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0045, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2662, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n684 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2661, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n685 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2660, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n686 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2660, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n687 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2659, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n688 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2659, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n689 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2658, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n690 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2658, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n691 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2657, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n692 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2657, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n693 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2656, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n694 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2655, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n695 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2655, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n696 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2654, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n697 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2654, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n698 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2653, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n699 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2653, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n700 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2652, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n701 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2652, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n702 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2651, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n703 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2651, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n704 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2650, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n705 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0078, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2649, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n706 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2649, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n707 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2648, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n708 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2648, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n709 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2647, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n710 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2647, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n711 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2646, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n712 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2646, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n713 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2645, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n714 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2645, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n715 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2644, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n716 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2644, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n717 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2643, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n718 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2643, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n719 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2642, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n720 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2642, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n721 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2641, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n722 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2641, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n723 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2640, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n724 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0046, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2639, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n725 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2639, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n726 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2638, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n727 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2638, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n728 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2637, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n729 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2637, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n730 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2636, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n731 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2636, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n732 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2635, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n733 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2635, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n734 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2634, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n735 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0079, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2634, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n736 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2633, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n737 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2633, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n738 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2632, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n739 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2632, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n740 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2631, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n741 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2631, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n742 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2630, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n743 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2630, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n744 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2629, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n745 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2629, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n746 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2628, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n747 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2628, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n748 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2627, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n749 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2627, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n750 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2626, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n751 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2626, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n752 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2625, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n753 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2625, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n754 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2624, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n755 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2624, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n756 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2624, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n757 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2623, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n758 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2623, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n759 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2622, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n760 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2622, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n761 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2621, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n762 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2621, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n763 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2620, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n764 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2620, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n765 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2619, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n766 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2619, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n767 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0047, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0080, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2618, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n768 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2618, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n769 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2617, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n770 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2617, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n771 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2616, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n772 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2616, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n773 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2615, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n774 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2615, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n775 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2615, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n776 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2614, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n777 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2614, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n778 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2613, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n779 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2613, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n780 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2612, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n781 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2612, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n782 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2611, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n783 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2611, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n784 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2610, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n785 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2610, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n786 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2609, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n787 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2609, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n788 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2609, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n789 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2608, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n790 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2608, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n791 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2607, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n792 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2607, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n793 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2606, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n794 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2606, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n795 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2605, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n796 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2605, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n797 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2605, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n798 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2604, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n799 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2604, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n800 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0081, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2603, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n801 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2603, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n802 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2602, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n803 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2602, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n804 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2601, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n805 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2601, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n806 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2601, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n807 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2600, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n808 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2600, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n809 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2599, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n810 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2599, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n811 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2598, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n812 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0048, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2598, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n813 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2598, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n814 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2597, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n815 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2597, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n816 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2596, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n817 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2596, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n818 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2595, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n819 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2595, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n820 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2595, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n821 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2594, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n822 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2594, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n823 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2593, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n824 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0006, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2593, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n825 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2592, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n826 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2592, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n827 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2592, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n828 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2591, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n829 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2591, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n830 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2590, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n831 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2590, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n832 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2589, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n833 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2589, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n834 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2589, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n835 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0082, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2588, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n836 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2588, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n837 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2587, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n838 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2587, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n839 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2587, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n840 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2586, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n841 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2586, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n842 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2585, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n843 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2585, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n844 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2585, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n845 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2584, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n846 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2584, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n847 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2583, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n848 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2583, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n849 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2582, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n850 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2582, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n851 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2582, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n852 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2581, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n853 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2581, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n854 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2580, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n855 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2580, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n856 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2580, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n857 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2579, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n858 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2579, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n859 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0049, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2578, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n860 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2578, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n861 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2578, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n862 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2577, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n863 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2577, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n864 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2576, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n865 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2576, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n866 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2576, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n867 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2575, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n868 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2575, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n869 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2575, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n870 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2574, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n871 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2574, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n872 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0083, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2573, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n873 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2573, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n874 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2573, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n875 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2572, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n876 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2572, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n877 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2571, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n878 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2571, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n879 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2571, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n880 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2570, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n881 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2570, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n882 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2570, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n883 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2569, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n884 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2569, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n885 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2568, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n886 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2568, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n887 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2568, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n888 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2567, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n889 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2567, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n890 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2567, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n891 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2566, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n892 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2566, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n893 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2565, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n894 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2565, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n895 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2565, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n896 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2564, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n897 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2564, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n898 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2564, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n899 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2563, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n900 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2563, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n901 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2562, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n902 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2562, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n903 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2562, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n904 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2561, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n905 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2561, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n906 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2561, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n907 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2560, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n908 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2560, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n909 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0050, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2559, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n910 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2559, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n911 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0084, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2559, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n912 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2558, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n913 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2558, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n914 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2558, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n915 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2557, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n916 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2557, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n917 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2557, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n918 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2556, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n919 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2556, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n920 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2556, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n921 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2555, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n922 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2555, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n923 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2554, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n924 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2554, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n925 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2554, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n926 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2553, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n927 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2553, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n928 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2553, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n929 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2552, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n930 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2552, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n931 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2552, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n932 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2551, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n933 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2551, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n934 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2551, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n935 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2550, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n936 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2550, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n937 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2550, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n938 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2549, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n939 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2549, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n940 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2549, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n941 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2548, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n942 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2548, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n943 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2547, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n944 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2547, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n945 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2547, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n946 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2546, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n947 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2546, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n948 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2546, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n949 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2545, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n950 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2545, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n951 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2545, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n952 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2544, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n953 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0085, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2544, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n954 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2544, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n955 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2543, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n956 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2543, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n957 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2543, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n958 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2542, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n959 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2542, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n960 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2542, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n961 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2541, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n962 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0051, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2541, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n963 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2541, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n964 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2540, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n965 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2540, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n966 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2540, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n967 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2539, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n968 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2539, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n969 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2539, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n970 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2538, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n971 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2538, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n972 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2538, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n973 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2537, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n974 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2537, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n975 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2537, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n976 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2536, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n977 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2536, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n978 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2536, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n979 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2535, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n980 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2535, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n981 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2535, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n982 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2534, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n983 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2534, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n984 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2534, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n985 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2534, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n986 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2533, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n987 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2533, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n988 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2533, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n989 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2532, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n990 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2532, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n991 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2532, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n992 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2531, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n993 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2531, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n994 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2531, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n995 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2530, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n996 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0086, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2530, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n997 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2530, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n998 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2529, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n999 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2529, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1000 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2529, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1001 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2528, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1002 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2528, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1003 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2528, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1004 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2528, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1005 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2527, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1006 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2527, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1007 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2527, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1008 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2526, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1009 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2526, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1010 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2526, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1011 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2525, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1012 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2525, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1013 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2525, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1014 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2524, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1015 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2524, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1016 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2524, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1017 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2524, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1018 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0052, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2523, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1019 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2523, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1020 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2523, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1021 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2522, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1022 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2522, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1023 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2522, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1024 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2521, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1025 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2521, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1026 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2521, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1027 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2520, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1028 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2520, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1029 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2520, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1030 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2520, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1031 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2519, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1032 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2519, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1033 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2519, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1034 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2518, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1035 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2518, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1036 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2518, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1037 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2518, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1038 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2517, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1039 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2517, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1040 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2517, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1041 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2516, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1042 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2516, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1043 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0087, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2516, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1044 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2515, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1045 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2515, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1046 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2515, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1047 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2515, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1048 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2514, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1049 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2514, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1050 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2514, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1051 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2513, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1052 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2513, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1053 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2513, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1054 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2513, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1055 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2512, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1056 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2512, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1057 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2512, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1058 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2511, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1059 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2511, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1060 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2511, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1061 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2511, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1062 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2510, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1063 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2510, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1064 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2510, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1065 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2509, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1066 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2509, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1067 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2509, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1068 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2509, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1069 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2508, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1070 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2508, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1071 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2508, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1072 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2507, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1073 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2507, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1074 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2507, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1075 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2507, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1076 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2506, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1077 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0053, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2506, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1078 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2506, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1079 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2505, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1080 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2505, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1081 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2505, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1082 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2505, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1083 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2504, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1084 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2504, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1085 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2504, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1086 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2504, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1087 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2503, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1088 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2503, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1089 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2503, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1090 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2502, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1091 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2502, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1092 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0088, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2502, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1093 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2502, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1094 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2501, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1095 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2501, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1096 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2501, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1097 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2501, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1098 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2500, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1099 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2500, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1100 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2500, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1101 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2499, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1102 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2499, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1103 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2499, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1104 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2499, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1105 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2498, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1106 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2498, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1107 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2498, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1108 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2498, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1109 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2497, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1110 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2497, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1111 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2497, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1112 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2497, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1113 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2496, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1114 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2496, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1115 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2496, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1116 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2495, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1117 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2495, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1118 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2495, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1119 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2495, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1120 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2494, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1121 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2494, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1122 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2494, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1123 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2494, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1124 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2493, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1125 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2493, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1126 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2493, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1127 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2493, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1128 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2492, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1129 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2492, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1130 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2492, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1131 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2492, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1132 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2491, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1133 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2491, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1134 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2491, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1135 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2491, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1136 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2490, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1137 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2490, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1138 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2490, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1139 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0054, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2489, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1140 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2489, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1141 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2489, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1142 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2489, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1143 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2488, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1144 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2488, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1145 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0089, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2488, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1146 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2488, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1147 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2487, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1148 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2487, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1149 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2487, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1150 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2487, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1151 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2486, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1152 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2486, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1153 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2486, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1154 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2486, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1155 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2485, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1156 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2485, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1157 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2485, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1158 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2485, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1159 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2484, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1160 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2484, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1161 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2484, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1162 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2484, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1163 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2483, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1164 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2483, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1165 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2483, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1166 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2483, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1167 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2482, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1168 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2482, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1169 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2482, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1170 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2482, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1171 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2481, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1172 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2481, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1173 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2481, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1174 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2481, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1175 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2481, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1176 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2480, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1177 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2480, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1178 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2480, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1179 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2480, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1180 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2479, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1181 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2479, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1182 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2479, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1183 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2479, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1184 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2478, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1185 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2478, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1186 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2478, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1187 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2478, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1188 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2477, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1189 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2477, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1190 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2477, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1191 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2477, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1192 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2476, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1193 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2476, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1194 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2476, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1195 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2476, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1196 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2475, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1197 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2475, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1198 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2475, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1199 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2475, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1200 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2475, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1201 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0090, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2474, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1202 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2474, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1203 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2474, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1204 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0055, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2474, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1205 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2473, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1206 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2473, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1207 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2473, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1208 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2473, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1209 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2472, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1210 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2472, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1211 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2472, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1212 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2472, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1213 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2472, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1214 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2471, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1215 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2471, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1216 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2471, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1217 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2471, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1218 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2470, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1219 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2470, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1220 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2470, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1221 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2470, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1222 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2469, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1223 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2469, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1224 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0007, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2469, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1225 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2469, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1226 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2469, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1227 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2468, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1228 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2468, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1229 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2468, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1230 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2468, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1231 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2467, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1232 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2467, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1233 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2467, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1234 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2467, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1235 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2467, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1236 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2466, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1237 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2466, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1238 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2466, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1239 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2466, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1240 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2465, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1241 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2465, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1242 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2465, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1243 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2465, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1244 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2465, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1245 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2464, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1246 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2464, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1247 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2464, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1248 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2464, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1249 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2463, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1250 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2463, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1251 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2463, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1252 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2463, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1253 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2463, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1254 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2462, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1255 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2462, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1256 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2462, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1257 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2462, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1258 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2461, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1259 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2461, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1260 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2461, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1261 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0091, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2461, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1262 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2461, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1263 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2460, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1264 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2460, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1265 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2460, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1266 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2460, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1267 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2459, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1268 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2459, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1269 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2459, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1270 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2459, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1271 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2459, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1272 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2458, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1273 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2458, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1274 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0056, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2458, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1275 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2458, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1276 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2458, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1277 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2457, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1278 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2457, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1279 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2457, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1280 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2457, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1281 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2456, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1282 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2456, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1283 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2456, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1284 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2456, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1285 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2456, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1286 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2455, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1287 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2455, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1288 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2455, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1289 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2455, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1290 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2455, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1291 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2454, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1292 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2454, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1293 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2454, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1294 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2454, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1295 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2454, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1296 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2453, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1297 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2453, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1298 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2453, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1299 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2453, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1300 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2453, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1301 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2452, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1302 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2452, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1303 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2452, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1304 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2452, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1305 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2451, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1306 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2451, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1307 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2451, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1308 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2451, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1309 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2451, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1310 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2450, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1311 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2450, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1312 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2450, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1313 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2450, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1314 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2450, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1315 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2449, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1316 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2449, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1317 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2449, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1318 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2449, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1319 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2449, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1320 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2448, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1321 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2448, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1322 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2448, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1323 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2448, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1324 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2448, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1325 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0092, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2447, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1326 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2447, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1327 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2447, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1328 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2447, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1329 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2447, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1330 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2446, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1331 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2446, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1332 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2446, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1333 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2446, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1334 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2446, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1335 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2445, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1336 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2445, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1337 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2445, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1338 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2445, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1339 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2445, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1340 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2444, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1341 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2444, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1342 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2444, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1343 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2444, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1344 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2444, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1345 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2443, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1346 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2443, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1347 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0057, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2443, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1348 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2443, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1349 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2443, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1350 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2442, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1351 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2442, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1352 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2442, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1353 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2442, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1354 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2442, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1355 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2442, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1356 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2441, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1357 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2441, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1358 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2441, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1359 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2441, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1360 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2441, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1361 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2440, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1362 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2440, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1363 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2440, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1364 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2440, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1365 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2440, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1366 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2439, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1367 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2439, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1368 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2439, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1369 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2439, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1370 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2439, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1371 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2438, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1372 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2438, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1373 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2438, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1374 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2438, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1375 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2438, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1376 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2438, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1377 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2437, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1378 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2437, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1379 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2437, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1380 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2437, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1381 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2437, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1382 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2436, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1383 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2436, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1384 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2436, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1385 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2436, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1386 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2436, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1387 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2435, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1388 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2435, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1389 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2435, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1390 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2435, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1391 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2435, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1392 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2435, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1393 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2434, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1394 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0093, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2434, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1395 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2434, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1396 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2434, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1397 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2434, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1398 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2433, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1399 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2433, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1400 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2433, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1401 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2433, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1402 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2433, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1403 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2432, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1404 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2432, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1405 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2432, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1406 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2432, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1407 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2432, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1408 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2432, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1409 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2431, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1410 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2431, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1411 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2431, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1412 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2431, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1413 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2431, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1414 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2430, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1415 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2430, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1416 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2430, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1417 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2430, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1418 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2430, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1419 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2430, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1420 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2429, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1421 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2429, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1422 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2429, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1423 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2429, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1424 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0058, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2429, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1425 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2429, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1426 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2428, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1427 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2428, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1428 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2428, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1429 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2428, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1430 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2428, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1431 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2427, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1432 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2427, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1433 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2427, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1434 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2427, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1435 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2427, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1436 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2427, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1437 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2426, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1438 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2426, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1439 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2426, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1440 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2426, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1441 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2426, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1442 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2426, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1443 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2425, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1444 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2425, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1445 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2425, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1446 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2425, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1447 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2425, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1448 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2424, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1449 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2424, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1450 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2424, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1451 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2424, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1452 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2424, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1453 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2424, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1454 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2423, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1455 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2423, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1456 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2423, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1457 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2423, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1458 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2423, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1459 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2423, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1460 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2422, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1461 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2422, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1462 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2422, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1463 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2422, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1464 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2422, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1465 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2422, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1466 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2421, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1467 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2421, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1468 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0094, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2421, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1469 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2421, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1470 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2421, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1471 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2421, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1472 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2420, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1473 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2420, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1474 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2420, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1475 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2420, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1476 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2420, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1477 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2420, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1478 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2419, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1479 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2419, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1480 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2419, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1481 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2419, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1482 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2419, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1483 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2419, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1484 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2418, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1485 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2418, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1486 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2418, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1487 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2418, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1488 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2418, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1489 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2418, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1490 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2417, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1491 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2417, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1492 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2417, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1493 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2417, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1494 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2417, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1495 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2417, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1496 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2416, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1497 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2416, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1498 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2416, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1499 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2416, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1500 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2416, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1501 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2416, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1502 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2415, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1503 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2415, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1504 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2415, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1505 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2415, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1506 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0059, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2415, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1507 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2415, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1508 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2414, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1509 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2414, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1510 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2414, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1511 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2414, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1512 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2414, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1513 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2414, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1514 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2413, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1515 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2413, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1516 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2413, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1517 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2413, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1518 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2413, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1519 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2413, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1520 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2412, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1521 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2412, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1522 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2412, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1523 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2412, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1524 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2412, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1525 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2412, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1526 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2411, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1527 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2411, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1528 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2411, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1529 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2411, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1530 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2411, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1531 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2411, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1532 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2411, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1533 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2410, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1534 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2410, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1535 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2410, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1536 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2410, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1537 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2410, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1538 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2410, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1539 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2409, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1540 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2409, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1541 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2409, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1542 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2409, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1543 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2409, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1544 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2409, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1545 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2408, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1546 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2408, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1547 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2408, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1548 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2408, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1549 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0095, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2408, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1550 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2408, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1551 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2408, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1552 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2407, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1553 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2407, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1554 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2407, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1555 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2407, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1556 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2407, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1557 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2407, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1558 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2406, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1559 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2406, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1560 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2406, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1561 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2406, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1562 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2406, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1563 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2406, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1564 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2406, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1565 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2405, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1566 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2405, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1567 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2405, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1568 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2405, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1569 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2405, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1570 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2405, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1571 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2404, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1572 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2404, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1573 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2404, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1574 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2404, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1575 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2404, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1576 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2404, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1577 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2404, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1578 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2403, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1579 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2403, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1580 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2403, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1581 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2403, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1582 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2403, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1583 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2403, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1584 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2403, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1585 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2402, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1586 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2402, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1587 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2402, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1588 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2402, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1589 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2402, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1590 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2402, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1591 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2401, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1592 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2401, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1593 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0060, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2401, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1594 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2401, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1595 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2401, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1596 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2401, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1597 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2401, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1598 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2400, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1599 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2400, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1600 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2400, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1601 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2400, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1602 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2400, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1603 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2400, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1604 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2400, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1605 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2399, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1606 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2399, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1607 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2399, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1608 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2399, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1609 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2399, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1610 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2399, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1611 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2399, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1612 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2398, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1613 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2398, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1614 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2398, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1615 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2398, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1616 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2398, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1617 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2398, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1618 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2397, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1619 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2397, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1620 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2397, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1621 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2397, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1622 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2397, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1623 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2397, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1624 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2397, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1625 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2396, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1626 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2396, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1627 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2396, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1628 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2396, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1629 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2396, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1630 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2396, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1631 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2396, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1632 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2395, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1633 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2395, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1634 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2395, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1635 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2395, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1636 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0096, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2395, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1637 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2395, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1638 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2395, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1639 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2394, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1640 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2394, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1641 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2394, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1642 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2394, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1643 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2394, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1644 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2394, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1645 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2394, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1646 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2393, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1647 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2393, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1648 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2393, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1649 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2393, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1650 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2393, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1651 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2393, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1652 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2393, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1653 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2393, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1654 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2392, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1655 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2392, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1656 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2392, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1657 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2392, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1658 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2392, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1659 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2392, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1660 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2392, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1661 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2391, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1662 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2391, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1663 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2391, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1664 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2391, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1665 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2391, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1666 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2391, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1667 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2391, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1668 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2390, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1669 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2390, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1670 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2390, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1671 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2390, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1672 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2390, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1673 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2390, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1674 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2390, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1675 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2389, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1676 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2389, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1677 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2389, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1678 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2389, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1679 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2389, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1680 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2389, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1681 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2389, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1682 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2389, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1683 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2388, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1684 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2388, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1685 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0061, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2388, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1686 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2388, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1687 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2388, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1688 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2388, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1689 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2388, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1690 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2387, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1691 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2387, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1692 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2387, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1693 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2387, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1694 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2387, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1695 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2387, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1696 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2387, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1697 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2386, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1698 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2386, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1699 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2386, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1700 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2386, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1701 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2386, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1702 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2386, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1703 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2386, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1704 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2386, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1705 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2385, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1706 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2385, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1707 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2385, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1708 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2385, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1709 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2385, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1710 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2385, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1711 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2385, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1712 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2384, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1713 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2384, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1714 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2384, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1715 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2384, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1716 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2384, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1717 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2384, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1718 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2384, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1719 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2384, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1720 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2383, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1721 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2383, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1722 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2383, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1723 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2383, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1724 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2383, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1725 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2383, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1726 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2383, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1727 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2383, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1728 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2382, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1729 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2382, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1730 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2382, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1731 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2382, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1732 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0097, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2382, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1733 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2382, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1734 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2382, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1735 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2381, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1736 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2381, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1737 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2381, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1738 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2381, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1739 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2381, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1740 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2381, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1741 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2381, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1742 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2381, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1743 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2380, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1744 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2380, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1745 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2380, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1746 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2380, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1747 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2380, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1748 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2380, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1749 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2380, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1750 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2380, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1751 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2379, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1752 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2379, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1753 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2379, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1754 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2379, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1755 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2379, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1756 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2379, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1757 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2379, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1758 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2379, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1759 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2378, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1760 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2378, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1761 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2378, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1762 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2378, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1763 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2378, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1764 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2378, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1765 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2378, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1766 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2378, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1767 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2377, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1768 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2377, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1769 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2377, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1770 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2377, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1771 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2377, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1772 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2377, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1773 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2377, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1774 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2377, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1775 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2376, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1776 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2376, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1777 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2376, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1778 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2376, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1779 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2376, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1780 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2376, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1781 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2376, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1782 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0062, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2376, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1783 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2375, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1784 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2375, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1785 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2375, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1786 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2375, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1787 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2375, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1788 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2375, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1789 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2375, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1790 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2375, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1791 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2374, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1792 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2374, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1793 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2374, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1794 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2374, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1795 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2374, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1796 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2374, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1797 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2374, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1798 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2374, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1799 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2373, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1800 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2373, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1801 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2373, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1802 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2373, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1803 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2373, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1804 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2373, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1805 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2373, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1806 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2373, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1807 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2372, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1808 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2372, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1809 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0008, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2372, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1810 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2372, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1811 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2372, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1812 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2372, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1813 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2372, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1814 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2372, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1815 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2372, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1816 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2371, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1817 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2371, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1818 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2371, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1819 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2371, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1820 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2371, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1821 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2371, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1822 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2371, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1823 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2371, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1824 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2370, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1825 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2370, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1826 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2370, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1827 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2370, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1828 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2370, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1829 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2370, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1830 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2370, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1831 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2370, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1832 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2369, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1833 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2369, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1834 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2369, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1835 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2369, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1836 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0098, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2369, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1837 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2369, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1838 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2369, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1839 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2369, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1840 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2369, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1841 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2368, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1842 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2368, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1843 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2368, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1844 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2368, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1845 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2368, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1846 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2368, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1847 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2368, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1848 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2368, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1849 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2368, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1850 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2367, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1851 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2367, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1852 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2367, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1853 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2367, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1854 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2367, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1855 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2367, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1856 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2367, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1857 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2367, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1858 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2366, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1859 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2366, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1860 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2366, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1861 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2366, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1862 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2366, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1863 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2366, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1864 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2366, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1865 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2366, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1866 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2366, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1867 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2365, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1868 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2365, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1869 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2365, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1870 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2365, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1871 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2365, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1872 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2365, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1873 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2365, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1874 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2365, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1875 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2365, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1876 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2364, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1877 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2364, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1878 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2364, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1879 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2364, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1880 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2364, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1881 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2364, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1882 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2364, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1883 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2364, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1884 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2363, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1885 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0063, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2363, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1886 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2363, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1887 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2363, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1888 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2363, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1889 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2363, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1890 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2363, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1891 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2363, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1892 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2363, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1893 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2362, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1894 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2362, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1895 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2362, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1896 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2362, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1897 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2362, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1898 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2362, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1899 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2362, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1900 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2362, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1901 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2362, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1902 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2361, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1903 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2361, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1904 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2361, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1905 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2361, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1906 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2361, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1907 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2361, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1908 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2361, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1909 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2361, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1910 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2361, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1911 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2360, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1912 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2360, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1913 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2360, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1914 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2360, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1915 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2360, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1916 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2360, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1917 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2360, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1918 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2360, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1919 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2360, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1920 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2359, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1921 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2359, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1922 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2359, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1923 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2359, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1924 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2359, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1925 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2359, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1926 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2359, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1927 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2359, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1928 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2359, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1929 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2359, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1930 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2358, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1931 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2358, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1932 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2358, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1933 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2358, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1934 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2358, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1935 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2358, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1936 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2358, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1937 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2358, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1938 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2358, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1939 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2357, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1940 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2357, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1941 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2357, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1942 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2357, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1943 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2357, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1944 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2357, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1945 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2357, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1946 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2357, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1947 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2357, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1948 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2356, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1949 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2356, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1950 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2356, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1951 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2356, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1952 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0099, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2356, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1953 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2356, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1954 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2356, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1955 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2356, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1956 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2356, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1957 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2356, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1958 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2355, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1959 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2355, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1960 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2355, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1961 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2355, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1962 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2355, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1963 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2355, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1964 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2355, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1965 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2355, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1966 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2355, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1967 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2354, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1968 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2354, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1969 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2354, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1970 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2354, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1971 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2354, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1972 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2354, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1973 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2354, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1974 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2354, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1975 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2354, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1976 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2354, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1977 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2353, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1978 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2353, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1979 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2353, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1980 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2353, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1981 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2353, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1982 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2353, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1983 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2353, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1984 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2353, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1985 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2353, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1986 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2352, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1987 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2352, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1988 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2352, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1989 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2352, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1990 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2352, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1991 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2352, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1992 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2352, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1993 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2352, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1994 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2352, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1995 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0064, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2352, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1996 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2351, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1997 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2351, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1998 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2351, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n1999 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2351, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2000 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2351, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2001 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2351, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2002 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2351, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2003 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2351, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2004 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2351, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2005 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2351, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2006 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2350, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2007 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2350, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2008 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2350, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2009 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2350, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2010 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2350, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2011 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2350, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2012 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2350, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2013 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2350, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2014 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2350, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2015 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2350, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2016 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2349, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2017 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2349, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2018 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2349, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2019 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2349, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2020 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2349, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2021 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2349, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2022 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2349, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2023 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2349, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2024 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2349, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n2025 iterations have been completed!\n      -&gt; Now w1 = tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now w2 = tensor(0.0100, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now b = tensor(0.0009, grad_fn=&lt;SelectBackward0&gt;)\n      -&gt; Now Loss = tensor(0.2349, grad_fn=&lt;MeanBackward0&gt;)\n---------------------------------------------------------\n</code></pre>"},{"location":"Programming/MachineLearning/pytorch_logistic_regerssion/pytorch_logistic_regerssion/#4-visualization-of-the-cross-entropy-function","title":"4. Visualization of the Cross Entropy Function","text":"<pre><code>plt.figure(figsize=(16, 8))\nlength = len(loss_list)\nprint(\"The length of loss_list is:\", length)\nplt.plot(np.arange(1, 201, 1), [loss.detach().numpy() for loss in loss_list[0:200]], \"black\") # \u753b\u524d200\u4e2aiter\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.show()\n</code></pre> <pre><code>The length of loss_list is: 2025\n</code></pre> <p>\u5728\u7ed8\u5236\u56fe\u5f62\u65f6\uff0c\u5c1d\u8bd5\u8c03\u7528 <code>numpy()</code> \u51fd\u6570\u5c06\u5177\u6709\u68af\u5ea6\u7684\u5f20\u91cf\u8f6c\u6362\u4e3aNumPy\u6570\u7ec4\uff0c\u4f46\u662f\u5177\u6709\u68af\u5ea6\u7684\u5f20\u91cf\u4e0d\u652f\u6301\u76f4\u63a5\u8f6c\u6362\u4e3aNumPy\u6570\u7ec4\u3002</p> <p>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f7f\u7528 <code>detach().numpy()</code> \u65b9\u6cd5\u5c06\u5177\u6709\u68af\u5ea6\u7684\u5f20\u91cf\u5206\u79bb\u51fa\u6765\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3aNumPy\u6570\u7ec4\u3002</p>"},{"location":"Programming/MachineLearning/pytorch_logistic_regerssion/pytorch_logistic_regerssion/#5-prediction-on-the-test-set-and-model-evaluation","title":"5. Prediction on the test set and model evaluation","text":"<pre><code># \u8c03\u7528\u6d4b\u8bd5\u96c6\u7684\u6570\u636e\nX_vec_test = torch.tensor(test_X.values, dtype=torch.float32)  # N*30\ny_vec_test = torch.tensor(test_y.values, dtype=torch.float32).reshape(-1, 1)  # N*1\n\n# \u540c\u6837\uff0c\u8fd8\u662f\u5411\u91cf\u5316\u5904\u7406\uff0c\u5f97\u5230\u9884\u6d4b\u7684\u6982\u7387\u503c\nz_test = torch.mm(X_vec_test, w) + b  # 69*1\ny_pred = torch.sigmoid(z_test)  # 69*1\uff0c\u5f97\u5230\u7684\u662f\u6982\u7387\n</code></pre> <pre><code># \u628a\u8fde\u7eed\u7684\u6982\u7387\u503cy_pred\u8f6c\u6362\u6210 0 or 1\ny_pred[y_pred &lt; 0.5] = 0.0\ny_pred[y_pred &gt; 0.5] = 1.0\nprint(y_pred)\n</code></pre> <pre><code>tensor([[1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1.]],\n       grad_fn=&lt;AsStridedBackward0&gt;)\n</code></pre> <pre><code>test_y.T # \u770b\u770b\u771f\u5b9e\u7684\u6d4b\u8bd5\u96c6\u6570\u636e\uff0c.T\u8868\u793a\u8f6c\u7f6e\n</code></pre> 0 1 2 3 4 5 6 7 8 9 ... 59 60 61 62 63 64 65 66 67 68 0 1 0 1 0 1 1 1 1 1 0 ... 1 1 1 0 0 0 0 0 0 1 <p>1 rows \u00d7 69 columns</p> <pre><code>y_pred_np = y_pred.detach().numpy()\ny_pred_np = np.squeeze(y_pred_np)\nprint(\"Shape of y_pred_p:\", y_pred_np.shape)\n</code></pre> <pre><code>Shape of y_pred_p: (69,)\n</code></pre> <p>\u5c06\u5177\u6709\u68af\u5ea6\u7684\u5f20\u91cf <code>y_pred</code> \u5206\u79bb\u51fa\u6765\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3aNumPy\u6570\u7ec4\u3002</p> <p>\u9996\u5148\uff0c\u4f7f\u7528 <code>detach()</code> \u65b9\u6cd5\u5c06 <code>y_pred</code> \u5f20\u91cf\u5206\u79bb\u51fa\u6765\uff0c\u8fd9\u6837\u5c31\u5f97\u5230\u4e00\u4e2a\u6ca1\u6709\u68af\u5ea6\u7684\u65b0\u5f20\u91cf\u3002\u7136\u540e\uff0c\u4f7f\u7528 <code>numpy()</code> \u65b9\u6cd5\u5c06\u8be5\u65b0\u5f20\u91cf\u8f6c\u6362\u4e3aNumPy\u6570\u7ec4\u3002\u6700\u540e\uff0c\u4f7f\u7528 <code>np.squeeze()</code> \u51fd\u6570\u5c06\u6570\u7ec4\u4e2d\u7684\u7ef4\u5ea6\u4e3a1\u7684\u7ef4\u5ea6\u53bb\u9664\uff0c\u5f97\u5230\u4e00\u4e2a\u5f62\u72b6\u66f4\u7d27\u51d1\u7684\u6570\u7ec4\u3002</p> <pre><code># \u8ba1\u7b97\u9884\u6d4b\u7ed3\u679c y_pred_np \u548c\u6d4b\u8bd5\u96c6\u6807\u7b7e test_y \u7684\u51c6\u786e\u7387\naccuracy = accuracy_score(test_y, y_pred_np)\nprint(\"The accuracy score is:\", accuracy)\n</code></pre> <pre><code>The accuracy score is: 0.9130434782608695\n</code></pre> <p><code>accuracy_score</code> \u51fd\u6570\u7528\u4e8e\u8ba1\u7b97\u5206\u7c7b\u4efb\u52a1\u7684\u51c6\u786e\u7387\u3002</p> <p><code>accuracy = accuracy_score(y_true, y_pred)</code></p> <p>\u5176\u4e2d\uff1a</p> <p><code>y_true</code> \u662f\u771f\u5b9e\u7684\u6807\u7b7e\uff0c\u53ef\u4ee5\u662f\u4e00\u4e2a\u4e00\u7ef4\u6570\u7ec4\u6216\u5217\u8868\u3002</p> <p><code>y_pred</code> \u662f\u9884\u6d4b\u7684\u6807\u7b7e\uff0c\u53ef\u4ee5\u662f\u4e00\u4e2a\u4e00\u7ef4\u6570\u7ec4\u6216\u5217\u8868\uff0c\u4e0e <code>y_true</code> \u7684\u957f\u5ea6\u5fc5\u987b\u76f8\u7b49\u3002</p> <pre><code>\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/","title":"Two sigma stock market prediction","text":"<p>This project contains 4 files</p> <ul> <li> <p>Market EDA - This notebook contains an Extensive EDA describing how market data is and what are different attributes associated with it.</p> </li> <li> <p>News EDA - This notebook contains an Extensive EDA describing how news data is and what are different attributes associated with it.</p> </li> <li> <p>Merging and Data preprocessing - This notebook contains the code for data preprocesing and merging the market and news data.</p> </li> <li> <p>Modeling - This notebook contains code to all the models that are applied to the merged data.</p> </li> </ul>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20sigma%20Merging%20and%20Data%20prepocessing/","title":"Merging and Data preprocessing","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>market_data=pd.read_csv('marketdataprocesssed.csv')\nnews_data=pd.read_csv('newsdatapreprocessed.csv')\n</code></pre> <pre><code>market_data=market_data.iloc[:,1:-1]\n</code></pre> <pre><code>market_data.head(3)\n</code></pre> time assetCode assetName volume close open returnsClosePrevRaw1 returnsOpenPrevRaw1 returnsClosePrevMktres1 returnsOpenPrevMktres1 returnsClosePrevRaw10 returnsOpenPrevRaw10 returnsClosePrevMktres10 returnsOpenPrevMktres10 returnsOpenNextMktres10 universe 0 2009-01-02 A.N Agilent Technologies Inc 3030118.0 16.24 15.60 0.039028 0.045576 0.029112 0.042122 -0.005511 -0.037037 -0.026992 -0.033293 0.179633 True 1 2009-01-02 AAI.N AirTran Holdings Inc 1551494.0 4.51 4.36 0.015766 -0.035398 -0.018756 -0.047927 0.127500 0.141361 0.110937 0.144485 0.048476 False 2 2009-01-02 AAP.N Advance Auto Parts Inc 795900.0 34.14 33.86 0.014562 0.022652 -0.010692 0.009156 0.035283 0.047398 -0.005260 0.054363 0.029782 True <pre><code>for columnname in market_data.columns:\n    if market_data[columnname].dtype=='float64':\n        market_data[columnname]=market_data[columnname].astype('float32')\n</code></pre> <pre><code>market_data['time'] =  pd.to_datetime(market_data['time'], format='%Y-%m-%d')\n</code></pre> <pre><code>market_data['assetName']=market_data['assetName'].astype('category')\nmarket_data['assetCode']=market_data['assetCode'].astype('category')\nmarket_data['universe'] = market_data['universe'].astype('bool')\n</code></pre> <pre><code>news_data.drop(['noveltyCount12H','volumeCounts12H'], axis=1, inplace=True)\n</code></pre> <pre><code>for columnname in news_data.columns:\n    if news_data[columnname].dtype=='float64':\n        news_data[columnname]=news_data[columnname].astype('float32')\n    elif news_data[columnname].dtype=='int64':\n        news_data[columnname]=news_data[columnname].astype('int32')\nnews_data['urgency']=news_data['urgency'].astype('int8')\nnews_data['time'] =  pd.to_datetime(news_data['time'], format='%Y-%m-%d')\n</code></pre> <pre><code>news_data=news_data.iloc[:,1:]\n</code></pre> <pre><code>def preprocess_news(news_train):\n    # Remove {} and '' from assetCodes column\n    news_train['assetCodes'] = news_train['assetCodes'].apply(lambda x: x[1:-1].replace(\"'\", \"\"))\n    return news_train\n\nnews_data = preprocess_news(news_data)\n</code></pre> <pre><code>def unstack_asset_codes(news_train):\n    codes = []\n    indexes = []\n    for i, values in news_train['assetCodes'].iteritems():\n        explode = values.split(\", \")\n        codes.extend(explode)\n        repeat_index = [int(i)]*len(explode)\n        indexes.extend(repeat_index)\n    index_df = pd.DataFrame({'news_index': indexes, 'assetCode': codes})\n    del codes, indexes\n    return index_df\n\nindex_df = unstack_asset_codes(news_data)\n\ndef merge_news_on_index(news_train, index_df):\n    news_train['news_index'] = news_train.index.copy()\n\n    # Merge news on unstacked assets\n    news_unstack = index_df.merge(news_train, how='left', on='news_index')\n    news_unstack.drop(['news_index', 'assetCodes'], axis=1, inplace=True)\n    return news_unstack\n\nnews_data = merge_news_on_index(news_data, index_df)\ndel  index_df\n</code></pre> <pre><code>news_data.time.max()\n</code></pre> <pre><code>Timestamp('2016-12-30 22:00:00')\n</code></pre> <pre><code>news_data.time.min()\n</code></pre> <pre><code>Timestamp('2009-01-01 00:25:02')\n</code></pre> <pre><code>news_data.assetCode.nunique()\n</code></pre> <pre><code>13205\n</code></pre> <pre><code>news_data.drop_duplicates(subset=['time','assetCode','assetName'],inplace=True)\n</code></pre> <pre><code>market_data.assetCode.unique()\n</code></pre> <pre><code>[A.N, AAI.N, AAP.N, AAPL.O, AB.N, ..., MTGE.O, SITE.N, FCB.N, AMC.N, CVGW.O]\nLength: 3464\nCategories (3464, object): [A.N, AAI.N, AAP.N, AAPL.O, ..., SITE.N, FCB.N, AMC.N, CVGW.O]\n</code></pre> <pre><code>news_data.shape\n</code></pre> <pre><code>(14271734, 19)\n</code></pre> <pre><code>market_data.shape\n</code></pre> <pre><code>(3340140, 16)\n</code></pre> <pre><code>news_data=news_data[news_data['assetCode'].isin(market_data.assetCode.unique())]\n</code></pre> <pre><code>df = market_data.merge(news_data, how='left', on=['assetCode', 'time', 'assetName'])\ndel market_data, news_data\n</code></pre> <pre><code>df.shape\n</code></pre> <pre><code>(3340140, 32)\n</code></pre> <pre><code>df_train=df[df.time&lt;=pd.Timestamp(2015,6,1)]\ndf_test=df[df.time&gt;pd.Timestamp(2015,6,1)]\n</code></pre> <pre><code>del df\n</code></pre> <pre><code>np.shape(df_train)[0]*100/3340140\n</code></pre> <pre><code>78.51368505511745\n</code></pre> <pre><code>df_train.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 2622467 entries, 0 to 2622466\nData columns (total 32 columns):\ntime                        datetime64[ns]\nassetCode                   object\nassetName                   object\nvolume                      float32\nclose                       float32\nopen                        float32\nreturnsClosePrevRaw1        float32\nreturnsOpenPrevRaw1         float32\nreturnsClosePrevMktres1     float32\nreturnsOpenPrevMktres1      float32\nreturnsClosePrevRaw10       float32\nreturnsOpenPrevRaw10        float32\nreturnsClosePrevMktres10    float32\nreturnsOpenPrevMktres10     float32\nreturnsOpenNextMktres10     float32\nuniverse                    bool\nurgency                     float64\nbodySize                    float64\ncompanyCount                float64\nmarketCommentary            object\nsentenceCount               float64\nwordCount                   float64\nrelevance                   float32\nsentimentClass              float64\nsentimentNegative           float32\nsentimentNeutral            float32\nsentimentPositive           float32\nsentimentWordCount          float64\nnoveltyCount3D              float64\nvolumeCounts3D              float64\ndelay_time                  object\nheadlinelength              float64\ndtypes: bool(1), datetime64[ns](1), float32(16), float64(10), object(4)\nmemory usage: 482.7+ MB\n</code></pre> <pre><code>df_train.drop(['delay_time'],axis=1,inplace=True)\ndf_test.drop(['delay_time'],axis=1,inplace=True)\n</code></pre> <pre><code>nullcols=df_train.returnsClosePrevMktres1.isnull()\n</code></pre> <pre><code>df_train[['time','assetCode','returnsClosePrevMktres1']][nullcols].head(6)\n</code></pre> time assetCode returnsClosePrevMktres1 167 2009-01-02 BBND.O NaN 739 2009-01-02 IIVI.O NaN 1472 2009-01-02 ULTA.O NaN 1641 2009-01-05 AIPC.O NaN 2013 2009-01-05 DCOM.O NaN 3062 2009-01-05 ULTA.O NaN <pre><code>def imputation(df):\n    df['sentimentClass']=df['sentimentClass'].fillna(0)\n    df.urgency=df.urgency.fillna(3)\n\n    for cols in df.columns:\n        if(df[cols].dtype=='float64' or df[cols].dtype=='float32'):\n            df[cols] = df.groupby(\"assetCode\")[cols].transform(lambda x: x.fillna(x.median()))\n    for cols in df.columns:\n        if(df[cols].dtype=='float64' or df[cols].dtype=='float32'):\n            df[cols][df[cols].isnull()]=df[cols].median()\n        elif(df[cols].dtype=='bool' or df[cols].dtype.name=='category'):\n            df[cols][df[cols].isnull()]=df[cols].value_counts().argmax()        \n\n    df['marketCommentary'][df['marketCommentary'].isnull()]=df['marketCommentary'].value_counts().argmax()\n    return df\n</code></pre> <pre><code>df_train=imputation(df_train)\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1019: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\nC:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  # Remove the CWD from sys.path while we load stuff.\nC:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\nwill be corrected to return the positional maximum in the future.\nUse 'series.values.argmax' to get the position of the maximum now.\n  if sys.path[0] == '':\nC:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  if sys.path[0] == '':\nC:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\nwill be corrected to return the positional maximum in the future.\nUse 'series.values.argmax' to get the position of the maximum now.\n\nC:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n</code></pre> <pre><code>df_train[['time','assetCode','returnsClosePrevMktres1']][nullcols].head(6)\n</code></pre> time assetCode returnsClosePrevMktres1 167 2009-01-02 BBND.O -0.002200 739 2009-01-02 IIVI.O -0.000287 1472 2009-01-02 ULTA.O 0.000449 1641 2009-01-05 AIPC.O 0.003477 2013 2009-01-05 DCOM.O 0.001298 3062 2009-01-05 ULTA.O 0.000449 <pre><code>df_train.isnull().sum()\n</code></pre> <pre><code>time                        0\nassetCode                   0\nassetName                   0\nvolume                      0\nclose                       0\nopen                        0\nreturnsClosePrevRaw1        0\nreturnsOpenPrevRaw1         0\nreturnsClosePrevMktres1     0\nreturnsOpenPrevMktres1      0\nreturnsClosePrevRaw10       0\nreturnsOpenPrevRaw10        0\nreturnsClosePrevMktres10    0\nreturnsOpenPrevMktres10     0\nreturnsOpenNextMktres10     0\nuniverse                    0\nurgency                     0\nbodySize                    0\ncompanyCount                0\nmarketCommentary            0\nsentenceCount               0\nwordCount                   0\nrelevance                   0\nsentimentClass              0\nsentimentNegative           0\nsentimentNeutral            0\nsentimentPositive           0\nsentimentWordCount          0\nnoveltyCount3D              0\nvolumeCounts3D              0\nheadlinelength              0\ndtype: int64\n</code></pre> <pre><code>df_test=imputation(df_test)\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  # Remove the CWD from sys.path while we load stuff.\nC:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\nwill be corrected to return the positional maximum in the future.\nUse 'series.values.argmax' to get the position of the maximum now.\n  if sys.path[0] == '':\nC:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  if sys.path[0] == '':\nC:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\nwill be corrected to return the positional maximum in the future.\nUse 'series.values.argmax' to get the position of the maximum now.\n\nC:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n</code></pre> <pre><code>return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\n</code></pre> <pre><code>def generate_features(df,window_size=[3,7,14],shift_size=1):\n    grouped=df.groupby('assetCode')\n    #----------------------------lag features-------------------------------------------\n    for col in return_features:\n        for window in window_size:\n            df['%s_lag_%s_median'%(col,window)] = grouped[col].shift(shift_size).rolling(window=window).mean()\n            df['%s_lag_%s_max'%(col,window)] = grouped[col].shift(shift_size).rolling(window=window).max()\n            df['%s_lag_%s_min'%(col,window)] = grouped[col].shift(shift_size).rolling(window=window).min()\n\n   # df['betareturn1']=(df['returnsClosePrevRaw1'] - df['returnsClosePrevMktres1']) / (df[returnraw1] - df[returnMktres1]).groupby(\"time\").mean()\n    df['closeopentovolume']=(df['close']+df['open'])*df['volume']\n    df['meanvolume']=grouped['volume'].mean()\n    df['meanclose']=grouped['close'].mean()\n    df['stdclose']=grouped['close'].std()\n    #-----------------------------time features----------------------------------------------\n    df['dayofweek']=df.time.dt.dayofweek\n    df['quarter']=df.time.dt.quarter\n    df['month']=df.time.dt.month\n    df['year']=df.time.dt.year\n    #---------------------------quant features---------------------------------------------------\n    new_column = grouped.apply(lambda x: x['close'].ewm(span=30).mean())\n    df[\"close_30EMA\"] = new_column.reset_index(level=0, drop=True)\n    new_column = grouped.apply(lambda x: x['close'].ewm(span=26).mean())\n    df[\"close_26EMA\"] = new_column.reset_index(level=0, drop=True)\n    new_column = grouped.apply(lambda x: x['close'].ewm(span=12).mean())\n    df[\"close_12EMA\"] = new_column.reset_index(level=0, drop=True)\n    df['MACD'] = df['close_12EMA'] - df['close_26EMA']\n    no_of_std = 2\n    #--------------------------bolinger band---------------------------------------\n    new_column=grouped['close'].rolling(window=7).mean()\n    df['MA_7MA'] =  new_column.reset_index(level=0, drop=True)\n    new_column=grouped['close'].rolling(window=7).std()\n    df['MA_7MA_std'] =  new_column.reset_index(level=0, drop=True)\n    df['MA_7MA_BB_high'] = df['MA_7MA'] + no_of_std * df['MA_7MA_std']\n    df['MA_7MA_BB_low'] = df['MA_7MA'] - no_of_std * df['MA_7MA_std']\n    return df.fillna(-1)\n</code></pre> <pre><code>df_train=generate_features(df_train)\ndf_test=generate_features(df_test)\n</code></pre> <pre><code>def RSI(df, column=\"close\", period=14):\n    # wilder's RSI\n    delta = df.groupby('assetCode')[column].diff()\n    up, down = delta.copy(), delta.copy()\n    up[up &lt; 0] = 0\n    down[down &gt; 0] = 0\n    rUp = up.ewm(com=period - 1,  adjust=False).mean()\n    rDown = down.ewm(com=period - 1, adjust=False).mean().abs()\n    rsi = 100 - 100 / (1 + rUp / rDown)    \n    return df.join(rsi.to_frame('RSI'))\n</code></pre> <pre><code>df_train=RSI(df_train)\ndf_test=RSI(df_test)\n</code></pre> <pre><code>def beta(df):\n\n    df['raw_median'] = df.groupby('time').returnsOpenPrevRaw10.transform('median')\n    df['xy'] = df.returnsOpenPrevRaw10 * df.raw_median\n\n    roll = df.groupby('assetCode').rolling(window=20)\n\n    df['cov_xy'] = (\n      (roll.xy.mean() - roll.returnsOpenPrevRaw10.mean() * roll.raw_median.mean()) * 20 / 19\n      ).reset_index(0,drop=True)\n\n    df['var_y'] = roll.raw_median.var().reset_index(0,drop=True)\n    df['beta'] = (df['cov_xy'] /df['var_y'])\n    df['beta'] = df.groupby('assetCode')['beta'].shift(1)\n    df.drop(['var_y','xy','raw_median','cov_xy'],axis=1,inplace=True)\n    return df.fillna(-1)\n</code></pre> <pre><code>df_train=beta(df_train)\ndf_test=beta(df_test)\n</code></pre> <pre><code>df_train['sin_quarter'] = np.sin(2*np.pi*df_train.quarter/4)\n</code></pre> <pre><code>df_test['sin_quarter'] = np.sin(2*np.pi*df_test.quarter/4)\n</code></pre> <pre><code>df_test['sin_dayofweek']=np.sin(2*np.pi*df_test.dayofweek/7)\ndf_train['sin_dayofweek']=np.sin(2*np.pi*df_train.dayofweek/7)\n</code></pre> <pre><code>df_test['sin_month']=np.sin(2*np.pi*df_test.month/12)\ndf_train['sin_month']=np.sin(2*np.pi*df_train.month/12)\n</code></pre> <pre><code>df_train.columns\n</code></pre> <pre><code>Index(['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsOpenNextMktres10', 'universe', 'urgency', 'bodySize',\n       'companyCount', 'marketCommentary', 'sentenceCount', 'wordCount',\n       'relevance', 'sentimentClass', 'sentimentNegative', 'sentimentNeutral',\n       'sentimentPositive', 'sentimentWordCount', 'noveltyCount3D',\n       'volumeCounts3D', 'headlinelength',\n       'returnsClosePrevMktres10_lag_3_median',\n       'returnsClosePrevMktres10_lag_3_max',\n       'returnsClosePrevMktres10_lag_3_min',\n       'returnsClosePrevMktres10_lag_7_median',\n       'returnsClosePrevMktres10_lag_7_max',\n       'returnsClosePrevMktres10_lag_7_min',\n       'returnsClosePrevMktres10_lag_14_median',\n       'returnsClosePrevMktres10_lag_14_max',\n       'returnsClosePrevMktres10_lag_14_min',\n       'returnsClosePrevRaw10_lag_3_median', 'returnsClosePrevRaw10_lag_3_max',\n       'returnsClosePrevRaw10_lag_3_min', 'returnsClosePrevRaw10_lag_7_median',\n       'returnsClosePrevRaw10_lag_7_max', 'returnsClosePrevRaw10_lag_7_min',\n       'returnsClosePrevRaw10_lag_14_median',\n       'returnsClosePrevRaw10_lag_14_max', 'returnsClosePrevRaw10_lag_14_min',\n       'open_lag_3_median', 'open_lag_3_max', 'open_lag_3_min',\n       'open_lag_7_median', 'open_lag_7_max', 'open_lag_7_min',\n       'open_lag_14_median', 'open_lag_14_max', 'open_lag_14_min',\n       'close_lag_3_median', 'close_lag_3_max', 'close_lag_3_min',\n       'close_lag_7_median', 'close_lag_7_max', 'close_lag_7_min',\n       'close_lag_14_median', 'close_lag_14_max', 'close_lag_14_min',\n       'closeopentovolume', 'meanvolume', 'meanclose', 'stdclose', 'dayofweek',\n       'quarter', 'month', 'year', 'close_30EMA', 'close_26EMA', 'close_12EMA',\n       'MACD', 'MA_7MA', 'MA_7MA_std', 'MA_7MA_BB_high', 'MA_7MA_BB_low',\n       'RSI', 'beta', 'sin_quarter', 'sin_dayofweek', 'sin_month'],\n      dtype='object')\n</code></pre> <pre><code>num_cols=['volume', 'close', 'open',\n       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsClosePrevMktres10_lag_3_median',\n       'returnsClosePrevMktres10_lag_3_max',\n       'returnsClosePrevMktres10_lag_3_min',\n       'returnsClosePrevMktres10_lag_7_median',\n       'returnsClosePrevMktres10_lag_7_max',\n       'returnsClosePrevMktres10_lag_7_min',\n       'returnsClosePrevMktres10_lag_14_median',\n       'returnsClosePrevMktres10_lag_14_max',\n       'returnsClosePrevMktres10_lag_14_min',\n       'returnsClosePrevRaw10_lag_3_median', 'returnsClosePrevRaw10_lag_3_max',\n       'returnsClosePrevRaw10_lag_3_min', 'returnsClosePrevRaw10_lag_7_median',\n       'returnsClosePrevRaw10_lag_7_max', 'returnsClosePrevRaw10_lag_7_min',\n       'returnsClosePrevRaw10_lag_14_median',\n       'returnsClosePrevRaw10_lag_14_max', 'returnsClosePrevRaw10_lag_14_min',\n       'open_lag_3_median', 'open_lag_3_max', 'open_lag_3_min',\n       'open_lag_7_median', 'open_lag_7_max', 'open_lag_7_min',\n       'open_lag_14_median', 'open_lag_14_max', 'open_lag_14_min',\n       'close_lag_3_median', 'close_lag_3_max', 'close_lag_3_min',\n       'close_lag_7_median', 'close_lag_7_max', 'close_lag_7_min',\n       'close_lag_14_median', 'close_lag_14_max', 'close_lag_14_min',\n       'closeopentovolume', 'meanvolume', 'meanclose', 'stdclose', 'close_30EMA', 'close_26EMA', 'close_12EMA',\n       'MACD', 'MA_7MA', 'MA_7MA_std', 'MA_7MA_BB_high', 'MA_7MA_BB_low',\n       'RSI', 'beta', 'sin_quarter', 'sin_dayofweek', 'sin_month']\n</code></pre> <pre><code>from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\ndf_train[num_cols]=scaler.fit_transform(df_train[num_cols])\ndf_test[num_cols]=scaler.transform(df_test[num_cols])\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype float32, float64 were all converted to float64 by StandardScaler.\n  return self.partial_fit(X, y)\nC:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype float32, float64 were all converted to float64 by StandardScaler.\n  return self.fit(X, **fit_params).transform(X)\nC:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype float32, float64 were all converted to float64 by StandardScaler.\n  after removing the cwd from sys.path.\n</code></pre> <pre><code>df_train.to_csv('train.csv')\ndf_test.to_csv('test.csv')\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Market%20EDA/Two%20Sigma%20Market%20EDA/","title":"Two Sigma Stock Market Prediction","text":"<pre><code>import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nimport datetime\n\nimport plotly.figure_factory as ff\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n</code></pre> <pre><code>market_data=pd.read_csv('marketdata.csv')\n</code></pre> <pre><code>market_data.shape\n</code></pre> <pre><code>(4072956, 17)\n</code></pre> <pre><code>market_data.head(7)\n</code></pre> Unnamed: 0 time assetCode assetName volume close open returnsClosePrevRaw1 returnsOpenPrevRaw1 returnsClosePrevMktres1 returnsOpenPrevMktres1 returnsClosePrevRaw10 returnsOpenPrevRaw10 returnsClosePrevMktres10 returnsOpenPrevMktres10 returnsOpenNextMktres10 universe 0 0 2007-02-01 22:00:00+00:00 A.N Agilent Technologies Inc 2606900.0 32.19 32.17 0.005938 0.005312 NaN NaN -0.001860 0.000622 NaN NaN 0.034672 1.0 1 1 2007-02-01 22:00:00+00:00 AAI.N AirTran Holdings Inc 2051600.0 11.12 11.08 0.004517 -0.007168 NaN NaN -0.078708 -0.088066 NaN NaN 0.027803 0.0 2 2 2007-02-01 22:00:00+00:00 AAP.N Advance Auto Parts Inc 1164800.0 37.51 37.99 -0.011594 0.025648 NaN NaN 0.014332 0.045405 NaN NaN 0.024433 1.0 3 3 2007-02-01 22:00:00+00:00 AAPL.O Apple Inc 23747329.0 84.74 86.23 -0.011548 0.016324 NaN NaN -0.048613 -0.037182 NaN NaN -0.007425 1.0 4 4 2007-02-01 22:00:00+00:00 ABB.N ABB Ltd 1208600.0 18.02 18.01 0.011791 0.025043 NaN NaN 0.012929 0.020397 NaN NaN -0.017994 1.0 5 5 2007-02-01 22:00:00+00:00 ABC.N AmerisourceBergen Corp 1657300.0 52.37 52.40 -0.000191 0.008468 NaN NaN 0.089000 0.077746 NaN NaN 0.058680 1.0 6 6 2007-02-01 22:00:00+00:00 ABD.N ACCO Brands Corp 1186200.0 23.63 24.13 -0.020721 -0.007404 NaN NaN 0.005104 0.026809 NaN NaN -0.044285 0.0 <pre><code>market_data=market_data.iloc[:,1:]\n</code></pre> <pre><code>market_data.head(3)\n</code></pre> time assetCode assetName volume close open returnsClosePrevRaw1 returnsOpenPrevRaw1 returnsClosePrevMktres1 returnsOpenPrevMktres1 returnsClosePrevRaw10 returnsOpenPrevRaw10 returnsClosePrevMktres10 returnsOpenPrevMktres10 returnsOpenNextMktres10 universe 0 2007-02-01 22:00:00+00:00 A.N Agilent Technologies Inc 2606900.0 32.19 32.17 0.005938 0.005312 NaN NaN -0.001860 0.000622 NaN NaN 0.034672 1.0 1 2007-02-01 22:00:00+00:00 AAI.N AirTran Holdings Inc 2051600.0 11.12 11.08 0.004517 -0.007168 NaN NaN -0.078708 -0.088066 NaN NaN 0.027803 0.0 2 2007-02-01 22:00:00+00:00 AAP.N Advance Auto Parts Inc 1164800.0 37.51 37.99 -0.011594 0.025648 NaN NaN 0.014332 0.045405 NaN NaN 0.024433 1.0 <pre><code>market_data.isnull().sum()\n</code></pre> <pre><code>time                            0\nassetCode                       0\nassetName                       0\nvolume                          0\nclose                           0\nopen                            0\nreturnsClosePrevRaw1            0\nreturnsOpenPrevRaw1             0\nreturnsClosePrevMktres1     15980\nreturnsOpenPrevMktres1      15988\nreturnsClosePrevRaw10           0\nreturnsOpenPrevRaw10            0\nreturnsClosePrevMktres10    93010\nreturnsOpenPrevMktres10     93054\nreturnsOpenNextMktres10         0\nuniverse                        0\ndtype: int64\n</code></pre> <pre><code>market_data.nunique()\n</code></pre> <pre><code>time                           2498\nassetCode                      3780\nassetName                      3511\nvolume                      2392152\nclose                         55434\nopen                          46627\nreturnsClosePrevRaw1        2903234\nreturnsOpenPrevRaw1         2866950\nreturnsClosePrevMktres1     4056917\nreturnsOpenPrevMktres1      4056910\nreturnsClosePrevRaw10       3416981\nreturnsOpenPrevRaw10        3380334\nreturnsClosePrevMktres10    3979921\nreturnsOpenPrevMktres10     3979902\nreturnsOpenNextMktres10     4072956\nuniverse                          2\ndtype: int64\n</code></pre> <pre><code>market_data.tail(3)\n</code></pre> time assetCode assetName volume close open returnsClosePrevRaw1 returnsOpenPrevRaw1 returnsClosePrevMktres1 returnsOpenPrevMktres1 returnsClosePrevRaw10 returnsOpenPrevRaw10 returnsClosePrevMktres10 returnsOpenPrevMktres10 returnsOpenNextMktres10 universe 4072953 2016-12-30 22:00:00+00:00 ZNGA.O Zynga Inc 7396601.0 2.57 2.58 -0.011538 0.000000 -0.006004 -0.001034 -0.091873 -0.078571 -0.077252 -0.077188 0.011703 0.0 4072954 2016-12-30 22:00:00+00:00 ZTO.N Unknown 3146519.0 12.07 12.50 -0.029743 0.007252 -0.028460 0.006719 -0.065066 -0.042146 -0.078104 -0.043813 0.083367 1.0 4072955 2016-12-30 22:00:00+00:00 ZTS.N Zoetis Inc 1701204.0 53.53 53.64 -0.001678 0.003091 0.005060 0.002885 0.023127 0.028177 0.026566 0.028719 -0.016220 1.0 <pre><code>initial=market_data.memory_usage()\n</code></pre> <pre><code>market_data.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4072956 entries, 0 to 4072955\nData columns (total 16 columns):\ntime                        object\nassetCode                   object\nassetName                   object\nvolume                      float64\nclose                       float64\nopen                        float64\nreturnsClosePrevRaw1        float64\nreturnsOpenPrevRaw1         float64\nreturnsClosePrevMktres1     float64\nreturnsOpenPrevMktres1      float64\nreturnsClosePrevRaw10       float64\nreturnsOpenPrevRaw10        float64\nreturnsClosePrevMktres10    float64\nreturnsOpenPrevMktres10     float64\nreturnsOpenNextMktres10     float64\nuniverse                    float64\ndtypes: float64(13), object(3)\nmemory usage: 497.2+ MB\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Market%20EDA/Two%20Sigma%20Market%20EDA/#lets-change-the-datatype-of-variables-to-make-them-more-functionable-and-reduce-the-memory-usage","title":"Let's change the datatype of variables to make them more functionable and reduce the memory usage.","text":"<pre><code>market_data.open.dtype=='float64'\n</code></pre> <pre><code>True\n</code></pre> <pre><code>for columnname in market_data.columns:\n    if market_data[columnname].dtype=='float64':\n        market_data[columnname]=market_data[columnname].astype('float32')\n</code></pre> <pre><code>market_data['time'] =  pd.to_datetime(market_data['time'], format='%Y-%m-%d %H:%M:%S+00:00')\n</code></pre> <pre><code>market_data['assetName']=market_data['assetName'].astype('category')\nmarket_data['assetCode']=market_data['assetCode'].astype('category')\nmarket_data['universe'] = market_data['universe'].astype('bool')\nmarket_data.time=market_data.time.dt.date\n</code></pre> <pre><code>market_data.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4072956 entries, 0 to 4072955\nData columns (total 16 columns):\ntime                        object\nassetCode                   category\nassetName                   category\nvolume                      float32\nclose                       float32\nopen                        float32\nreturnsClosePrevRaw1        float32\nreturnsOpenPrevRaw1         float32\nreturnsClosePrevMktres1     float32\nreturnsOpenPrevMktres1      float32\nreturnsClosePrevRaw10       float32\nreturnsOpenPrevRaw10        float32\nreturnsClosePrevMktres10    float32\nreturnsOpenPrevMktres10     float32\nreturnsOpenNextMktres10     float32\nuniverse                    bool\ndtypes: bool(1), category(2), float32(12), object(1)\nmemory usage: 237.3+ MB\n</code></pre> <pre><code>print('Memory occupied before downcasting:',initial.sum())\nprint('Memory occupied after downcasting :',market_data.memory_usage().sum())\nprint('Total memory saved                :',round(market_data.memory_usage().sum()*100/initial.sum(), 2),'%')\n</code></pre> <pre><code>Memory occupied before downcasting: 521338448\nMemory occupied after downcasting : 248836404\nTotal memory saved                : 47.73 %\n</code></pre> <pre><code>print('The amount of memory saved=',market_data.memory_usage().sum()*100/initial.sum())\n</code></pre> <pre><code>The amount of memory saved= 47.73029976104889\n</code></pre> <p>Saved 250+ MB of memory!</p>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Market%20EDA/Two%20Sigma%20Market%20EDA/#exploratory-data-analysis","title":"Exploratory data analysis","text":"<pre><code>market_data.describe()\n</code></pre> volume close open returnsClosePrevRaw1 returnsOpenPrevRaw1 returnsClosePrevMktres1 returnsOpenPrevMktres1 returnsClosePrevRaw10 returnsOpenPrevRaw10 returnsClosePrevMktres10 returnsOpenPrevMktres10 returnsOpenNextMktres10 count 4.072956e+06 4.072956e+06 4.072956e+06 4.072956e+06 4.072956e+06 4.056976e+06 4.056968e+06 4.072956e+06 4.072956e+06 3.979946e+06 3.979902e+06 4.072956e+06 mean 2.665310e+06 3.971241e+01 3.971235e+01 5.473023e-04 9.569114e-03 1.738580e-04 9.309284e-03 5.232020e-03 1.423684e-02 1.638857e-03 1.481702e-02 1.405529e-02 std 7.687606e+06 4.228822e+01 4.261116e+01 3.697774e-02 7.084387e+00 3.270305e-02 6.968866e+00 8.872484e-02 7.123552e+00 7.517820e-02 7.285742e+00 7.242466e+00 min 0.000000e+00 7.000000e-02 1.000000e-02 -9.776464e-01 -9.998881e-01 -1.235622e+00 -6.158463e+02 -9.774034e-01 -9.998904e-01 -3.343277e+00 -1.375045e+03 -1.375045e+03 25% 4.657968e+05 1.725000e+01 1.725000e+01 -1.089241e-02 -1.108987e-02 -8.569246e-03 -1.002114e-02 -3.339148e-02 -3.337573e-02 -2.927584e-02 -2.962645e-02 -2.970509e-02 50% 9.821000e+05 3.030000e+01 3.029000e+01 3.373819e-04 3.824092e-04 -1.236127e-04 -3.356938e-06 5.160339e-03 5.165128e-03 8.329260e-04 1.126206e-03 1.044642e-03 75% 2.403165e+06 4.986000e+01 4.985000e+01 1.165695e-02 1.183612e-02 8.397528e-03 1.002712e-02 4.297606e-02 4.298507e-02 3.059022e-02 3.171535e-02 3.152750e-02 max 1.226791e+09 1.578130e+03 9.998990e+03 4.559245e+01 9.209000e+03 4.512244e+01 8.989207e+03 4.667181e+01 9.382000e+03 4.624971e+01 9.761338e+03 9.761338e+03 <p>Target Variable</p> <pre><code>market_data.returnsOpenNextMktres10.describe(percentiles=[0.01, 0.99])\n</code></pre> <pre><code>count    4.072956e+06\nmean     1.405529e-02\nstd      7.242466e+00\nmin     -1.375045e+03\n1%      -1.888052e-01\n50%      1.044642e-03\n99%      2.078779e-01\nmax      9.761338e+03\nName: returnsOpenNextMktres10, dtype: float64\n</code></pre> <pre><code>sns.FacetGrid(market_data, height=5) \\\n   .map(sns.distplot, \"returnsOpenNextMktres10\") \\\n   .add_legend();\nplt.show();\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning:\n\nUsing a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n</code></pre> <p></p> <pre><code>print('The minimum value of target variable is ',market_data.returnsOpenNextMktres10.min(),'and max value is',market_data.returnsOpenNextMktres10.max())\n</code></pre> <pre><code>The minimum value of target variable is  -1375.045 and max value is 9761.338\n</code></pre> <pre><code>market_data[market_data.returnsOpenNextMktres10&lt;=-1000]\n</code></pre> time assetCode assetName volume close open returnsClosePrevRaw1 returnsOpenPrevRaw1 returnsClosePrevMktres1 returnsOpenPrevMktres1 returnsClosePrevRaw10 returnsOpenPrevRaw10 returnsClosePrevMktres10 returnsOpenPrevMktres10 returnsOpenNextMktres10 universe 277852 2007-10-31 ATPG.O ATP Oil &amp; Gas Corp 431705.0 57.349998 54.349998 0.051136 5434.000000 0.033208 5347.354004 0.049789 -0.020721 0.044722 -0.008626 -1375.045044 False 279315 2007-11-01 ATPG.O ATP Oil &amp; Gas Corp 480803.0 56.419998 57.000000 -0.016216 0.048758 0.018749 169.846085 0.034660 0.054579 0.056293 0.047723 -1036.400757 False 282237 2007-11-05 ATPG.O ATP Oil &amp; Gas Corp 444899.0 55.220001 54.590000 -0.018311 -0.036534 -0.011079 -38.512146 0.024870 0.044785 0.028541 0.036277 -1225.593262 False 283697 2007-11-06 ATPG.O ATP Oil &amp; Gas Corp 817261.0 56.139999 55.889999 0.016661 0.023814 -0.000148 -1.826470 0.039437 0.037113 0.038767 0.041530 -1226.144287 False <pre><code>market_data[market_data.returnsOpenNextMktres10&gt;=1000]\n</code></pre> time assetCode assetName volume close open returnsClosePrevRaw1 returnsOpenPrevRaw1 returnsClosePrevMktres1 returnsOpenPrevMktres1 returnsClosePrevRaw10 returnsOpenPrevRaw10 returnsClosePrevMktres10 returnsOpenPrevMktres10 returnsOpenNextMktres10 universe 24133 2007-02-26 TEO.N Telecom Argentina SA 90900.0 22.840000 22.600000 -0.000438 -0.013100 0.000825 -0.008187 0.029757 0.015730 0.003909 0.008387 2170.789062 False 91022 2007-05-02 PBRa.N Petroleo Brasileiro SA Petrobras 1557742.0 90.209999 89.349998 0.006359 0.000896 -0.007562 -0.003053 -0.031978 -0.039970 -0.023168 -0.032057 9761.337891 True 164243 2007-07-13 RRR.N RSC Holdings Inc 440000.0 21.910000 21.500000 0.012945 0.023810 0.009417 0.028716 0.089508 0.072319 NaN NaN 1273.736328 False 205166 2007-08-22 EXH.N Archrock Inc 996500.0 78.250000 77.000000 -0.003185 -0.030227 -0.013629 -0.031566 0.013081 -0.002164 0.066657 0.036802 7860.473145 True 274925 2007-10-29 ATPG.O ATP Oil &amp; Gas Corp 307933.0 56.599998 55.730000 0.022953 0.009053 0.018828 -0.002208 0.027783 0.014379 0.029591 0.016414 3207.863037 False 554191 2008-07-23 RGC.N Regal Entertainment Group 1311640.0 16.180000 16.080000 0.011250 0.000000 0.008593 -0.014918 0.083724 0.041451 0.042770 0.035710 1736.454224 True 587369 2008-08-21 TX.N Ternium SA 818526.0 34.599998 34.700001 0.004938 0.015808 0.002730 0.013045 0.008159 -0.034233 0.008439 -0.038720 2672.020996 True 614639 2008-09-17 ABV.N Companhia de Bebidas das Americas Ambev 1159408.0 50.240002 53.330002 -0.078334 0.004331 -0.033347 -0.006008 -0.168212 -0.124158 -0.026640 -0.042231 2734.923828 True <pre><code>for i in range(90,101,1):\n    print('The ',i,'Th percentile is=',market_data.returnsOpenNextMktres10.quantile(i/100))\n</code></pre> <pre><code>The  90 Th percentile is= 0.07094716653227806\nThe  91 Th percentile is= 0.07588737159967426\nThe  92 Th percentile is= 0.08152703344821932\nThe  93 Th percentile is= 0.08805996291339398\nThe  94 Th percentile is= 0.09590333029627798\nThe  95 Th percentile is= 0.1055758111178875\nThe  96 Th percentile is= 0.11776522397994993\nThe  97 Th percentile is= 0.13463359102606773\nThe  98 Th percentile is= 0.15970668196678162\nThe  99 Th percentile is= 0.20787793472409272\nThe  100 Th percentile is= 9761.337890625\n</code></pre> <pre><code>for i in range(990,1001,1):\n    print('The ',i/10,'Th percentile is=',market_data.returnsOpenNextMktres10.quantile(i/1000))\n</code></pre> <pre><code>The  99.0 Th percentile is= 0.20787793472409272\nThe  99.1 Th percentile is= 0.21580771833658205\nThe  99.2 Th percentile is= 0.22496113300323461\nThe  99.3 Th percentile is= 0.23544716760516154\nThe  99.4 Th percentile is= 0.24807283088565074\nThe  99.5 Th percentile is= 0.26368587389588394\nThe  99.6 Th percentile is= 0.28374402821064126\nThe  99.7 Th percentile is= 0.3113138820230907\nThe  99.8 Th percentile is= 0.35328230112790016\nThe  99.9 Th percentile is= 0.43966944068672636\nThe  100.0 Th percentile is= 9761.337890625\n</code></pre> <pre><code>sns.distplot(market_data.returnsOpenNextMktres10.clip(-1,1))\nplt.show()\n</code></pre> <p></p> <p>Analyzing the stock with maximum target variable</p> <pre><code>data = []\nfor columnnames in ['close','open']:\n    asset_df = market_data[(market_data['assetName'] == 'ATP Oil &amp; Gas Corp')]\n    data.append(go.Scatter(\n        x = asset_df['time'].values,\n        y = asset_df[columnnames].values,\n        name = columnnames\n    ))\nlayout = go.Layout(dict(title = \"Closing Vs Opening of ATP Oil &amp; Gas Corp\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')\n</code></pre> <pre><code>data = []\nfor columnnames in ['close','open']:\n    asset_df = market_data[(market_data['assetName'] == 'Petroleo Brasileiro SA Petrobras')]\n    data.append(go.Scatter(\n        x = asset_df['time'].values,\n        y = asset_df[columnnames].values,\n        name = columnnames\n    ))\nlayout = go.Layout(dict(title = \"Closing Vs Opening of Petroleo Brasileiro SA Petrobras\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')\n</code></pre> <pre><code>data = []\nfor columnnames in ['volume']:\n    asset_df = market_data[(market_data['assetName'] == 'Petroleo Brasileiro SA Petrobras')]\n    data.append(go.Scatter(\n        x = asset_df['time'].values,\n        y = asset_df[columnnames].values,\n        name = columnnames\n    ))\nlayout = go.Layout(dict(title = \"Volume of Petroleo Brasileiro SA Petrobras\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')\n</code></pre> <p>AssetCode</p> <pre><code>market_data.assetCode.nunique()\n</code></pre> <pre><code>3780\n</code></pre> <pre><code>print('The market data is ranging from ',market_data.time.min(),'to ',market_data.time.max())\n</code></pre> <pre><code>The market data is ranging from  2007-02-01 to  2016-12-30\n</code></pre> <pre><code>market_data.assetCode.value_counts().head(5)\n</code></pre> <pre><code>KO.N      2498\nLSTR.O    2498\nMO.N      2498\nMMM.N     2498\nMMC.N     2498\nName: assetCode, dtype: int64\n</code></pre> <pre><code>market_data.assetName[market_data['assetCode']=='GLW.N'][0]\n</code></pre> <pre><code>'Corning Inc'\n</code></pre> <pre><code>market_data.assetCode.isna().sum()\n</code></pre> <pre><code>0\n</code></pre> <pre><code>market_data.assetName.value_counts().head()\n</code></pre> <pre><code>Unknown                             24279\nPetroleo Brasileiro SA Petrobras     4699\nComcast Corp                         4683\nRoyal Dutch Shell PLC                4616\nChubb Ltd                            4315\nName: assetName, dtype: int64\n</code></pre> <pre><code>market_data.assetCode[market_data['assetName']=='Unknown'].head()\n</code></pre> <pre><code>10      ABY.N\n17       AD.N\n245    CBSS.O\n261     CEI.N\n305    CMGb.N\nName: assetCode, dtype: category\nCategories (3780, object): [A.N, AA.N, AAI.N, AAL.O, ..., ZTS.N, ZU.O, ZUMZ.O, ZZ.N]\n</code></pre> <pre><code>np.shape(market_data.assetCode[market_data['assetName']=='Unknown'])[0]\n</code></pre> <pre><code>24279\n</code></pre> <pre><code>print('Mean closing of all the data is',market_data['close'].mean(),' and mean of the unknown data is ',market_data.close[market_data['assetName']=='Unknown'].mean())\n</code></pre> <pre><code>Mean closing of all the data is 39.712414  and mean of the unknown data is  25.359137\n</code></pre> <pre><code>data = [market_data['close'], market_data.close[market_data['assetName']=='Unknown']]\nfig, ax = plt.subplots()\nax.boxplot(data, 0, '')\nplt.title('Closing of All assetName Vs Unknown')\nplt.show()\n</code></pre> <p></p> <pre><code>market_data.groupby('assetCode')['close'].mean().reset_index().sort_values(by='close', ascending=False).head()\n</code></pre> assetCode close 1460 GOOGL.O 763.556580 1459 GOOG.O 743.822510 2564 PCLN.O 632.970459 747 CME.N 545.510437 1774 ISRG.O 386.257721 <pre><code>market_data.groupby('assetName')['close'].mean().reset_index().sort_values(by='close', ascending=False).head()\n</code></pre> assetName close 182 Alphabet Inc 753.689575 496 Booking Holdings Inc 632.970459 1702 Intuitive Surgical Inc 386.257721 270 Apple Inc 262.096436 193 Amazon.com Inc 260.670044 <pre><code>freqstocks=market_data.assetName.value_counts()[:1000].index.tolist()\nmarketfreq=market_data[market_data.assetName.isin(freqstocks)]\n</code></pre> <pre><code>marketfreq.groupby('assetName')['close'].std().sort_values(ascending=True)[:5]\n</code></pre> <pre><code>assetName\nUnited Microelectronics Corp       0.592034\nMFA Financial Inc                  0.873824\nASE Technology Holding Co Ltd      1.212521\nFranklin Street Properties Corp    1.324322\nCapstead Mortgage Corp             1.448474\nName: close, dtype: float32\n</code></pre> <pre><code>data=[]\nasset_df = market_data[(market_data['assetName'] == 'United Microelectronics Corp')]\n\ndata.append(go.Scatter(\n        x = asset_df['time'].values,\n        y = asset_df['close'].values,\n        name = 'United Microelectronics Corp'\n    ))\nlayout = go.Layout(dict(title = \"Closing prices of United Microelectronics Corp\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')\n</code></pre> <pre><code>market_data[market_data.assetName==market_data.groupby('assetName')['close'].std().sort_values(ascending=False).idxmin()]\n</code></pre> time assetCode assetName volume close open returnsClosePrevRaw1 returnsOpenPrevRaw1 returnsClosePrevMktres1 returnsOpenPrevMktres1 returnsClosePrevRaw10 returnsOpenPrevRaw10 returnsClosePrevMktres10 returnsOpenPrevMktres10 returnsOpenNextMktres10 universe 544300 2008-07-15 ESMK.O RG Steel Wheeling Steel Group LLC 470511.0 19.17 19.18 0.000522 -0.002600 NaN NaN 0.002615 0.001567 NaN NaN -0.017475 False 545833 2008-07-16 ESMK.O RG Steel Wheeling Steel Group LLC 846691.0 19.17 19.17 0.000000 -0.000521 -0.015489 -0.001104 0.002091 0.002615 NaN NaN -0.014007 False 547366 2008-07-17 ESMK.O RG Steel Wheeling Steel Group LLC 331262.0 19.17 19.25 0.000000 0.004173 -0.007111 0.003901 0.002091 0.006273 NaN NaN -0.001602 False 548902 2008-07-18 ESMK.O RG Steel Wheeling Steel Group LLC 211234.0 19.16 19.17 -0.000522 -0.004156 -0.000695 -0.004133 0.000522 0.000000 NaN NaN 0.007816 False <pre><code>for asset in ['Apple Inc','Alphabet Inc','Amazon.com Inc']:\n    print(asset)\n</code></pre> <pre><code>Apple Inc\nAlphabet Inc\nAmazon.com Inc\n</code></pre> <pre><code>data = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_data.groupby('time')['close'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].values,\n        y = price_df['close'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of closing prices by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),\n    annotations=[\n        dict(\n            x='2008-09-01',\n            y=82,\n            xref='x',\n            yref='y',\n            text='Collapse of Lehman Brothers',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2011-08-01',\n            y=85,\n            xref='x',\n            yref='y',\n            text='Black Monday',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2014-10-01',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Brazil crisis',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=-20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2016-01-01',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Oil prices crash',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        )\n    ])\npy.iplot(dict(data=data, layout=layout), filename='basic-line')\n</code></pre> <pre><code>data=[]\nfor asset in ['Apple Inc','Alphabet Inc','Amazon.com Inc']:\n    asset_df = market_data[(market_data['assetName'] == asset)]\n\n    data.append(go.Scatter(\n        x = asset_df['time'].values,\n        y = asset_df['close'].values,\n        name = asset\n    ))\nlayout = go.Layout(dict(title = \"Closing prices of Apple, Google and Amazon\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')\n</code></pre> <pre><code>market_data['close_to_open']=abs(market_data['close']-market_data['open'])*100/market_data['open']\n</code></pre> <pre><code>market_data['close_to_open']=abs(market_data['close']/market_data['open'])\n</code></pre> <pre><code>corr = market_data[['close','open','close_to_open','volume']].corr()\nsns.set(rc={'figure.figsize':(8.7,6.27)})\n\nsns.heatmap(corr, annot=True,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\nplt.show()\n</code></pre> <p></p>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Market%20EDA/Two%20Sigma%20Market%20EDA/#lets-see-how-the-values-are-corelated-to-each-other","title":"Let's see how the values are corelated to each other","text":"<pre><code>corr = market_data.corr()\nsns.set(rc={'figure.figsize':(15,12)})\n\nsns.heatmap(corr, annot=True,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\nplt.show()\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Market%20EDA/Two%20Sigma%20Market%20EDA/#cleaning-up-market-data","title":"Cleaning up market data","text":"<p>Let's take data post 2009 into consideration</p> <pre><code>market_data[market_data.time&gt;datetime.date(2009, 1, 1)].head(3)\n</code></pre> time assetCode assetName volume close open returnsClosePrevRaw1 returnsOpenPrevRaw1 returnsClosePrevMktres1 returnsOpenPrevMktres1 returnsClosePrevRaw10 returnsOpenPrevRaw10 returnsClosePrevMktres10 returnsOpenPrevMktres10 returnsOpenNextMktres10 universe close_to_open 732812 2009-01-02 A.N Agilent Technologies Inc 3030118.0 16.240000 15.600000 0.039028 0.045576 0.029112 0.042122 -0.005511 -0.037037 -0.026992 -0.033293 0.179633 True 1.041026 732813 2009-01-02 AAI.N AirTran Holdings Inc 1551494.0 4.510000 4.360000 0.015766 -0.035398 -0.018756 -0.047927 0.127500 0.141361 0.110937 0.144485 0.048476 False 1.034404 732814 2009-01-02 AAP.N Advance Auto Parts Inc 795900.0 34.139999 33.860001 0.014562 0.022652 -0.010692 0.009156 0.035283 0.047398 -0.005260 0.054363 0.029782 True 1.008269 <pre><code>print('We will loose ',4072956-np.shape(market_data[market_data.time&gt;datetime.date(2009, 1, 1)])[0],'rows data','which constitutes to ',732812*100/4072956,'percent')\n</code></pre> <pre><code>We will loose  732812 rows data which constitutes to  17.992141334205428 percent\n</code></pre> <pre><code>market_data=market_data[market_data.time&gt;datetime.date(2009, 1, 1)]\n</code></pre> <pre><code>market_data.shape\n</code></pre> <pre><code>(3340144, 17)\n</code></pre> <pre><code>market_data.describe()\n</code></pre> volume close open returnsClosePrevRaw1 returnsOpenPrevRaw1 returnsClosePrevMktres1 returnsOpenPrevMktres1 returnsClosePrevRaw10 returnsOpenPrevRaw10 returnsClosePrevMktres10 returnsOpenPrevMktres10 returnsOpenNextMktres10 close_to_open count 3.340144e+06 3.340144e+06 3.340144e+06 3.340144e+06 3.340144e+06 3.328182e+06 3.328174e+06 3.340144e+06 3.340144e+06 3.275909e+06 3.275865e+06 3.340144e+06 3.340144e+06 mean 2.639898e+06 4.032637e+01 4.031876e+01 7.889647e-04 8.404336e-04 1.649761e-04 3.655074e-04 7.935358e-03 7.992627e-03 1.044463e-03 1.528284e-03 1.158386e-03 1.000401e+00 std 7.816326e+06 4.458819e+01 4.491502e+01 3.692293e-02 1.050432e-01 3.364188e-02 9.419718e-02 8.539551e-02 1.317596e-01 7.415205e-02 1.253186e-01 1.252446e-01 2.940631e-02 min 0.000000e+00 7.000000e-02 8.000000e-02 -9.776464e-01 -9.959986e-01 -1.235622e+00 -7.734479e-01 -9.774034e-01 -8.946596e-01 -3.343277e+00 -2.843179e+00 -1.033950e+01 5.000505e-03 25% 4.584250e+05 1.694000e+01 1.694000e+01 -1.010101e-02 -1.025641e-02 -8.152630e-03 -9.478131e-03 -3.003003e-02 -3.000324e-02 -2.849500e-02 -2.884615e-02 -2.889140e-02 9.908416e-01 50% 9.758150e+05 3.030000e+01 3.029000e+01 5.096840e-04 5.924171e-04 -1.061538e-04 5.683446e-05 6.545763e-03 6.535948e-03 6.156432e-04 8.716749e-04 7.584366e-04 1.000286e+00 75% 2.392364e+06 5.055000e+01 5.053000e+01 1.129944e-02 1.147978e-02 7.998730e-03 9.610343e-03 4.329831e-02 4.323308e-02 2.899303e-02 3.003845e-02 2.973548e-02 1.009753e+00 max 1.226791e+09 1.578130e+03 9.998990e+03 4.559245e+01 1.859884e+02 4.512244e+01 1.636713e+02 4.667181e+01 1.899187e+02 4.624971e+01 1.881849e+02 1.881849e+02 3.382740e+01 <pre><code>market_data['close_to_open']=np.abs(market_data['close']/market_data['open'])\n</code></pre> <pre><code>print(f\"In {(market_data['close_to_open'] &gt;= 2).sum()} lines price increased by 100% or more.\")\nprint(f\"In {(market_data['close_to_open'] &lt;= 0.5).sum()} lines price decreased by 100% or more.\")\n</code></pre> <pre><code>In 6 lines price increased by 100% or more.\nIn 3 lines price decreased by 100% or more.\n</code></pre> <pre><code>market_data[market_data['close_to_open'] &gt;= 2]\n</code></pre> time assetCode assetName volume close open returnsClosePrevRaw1 returnsOpenPrevRaw1 returnsClosePrevMktres1 returnsOpenPrevMktres1 returnsClosePrevRaw10 returnsOpenPrevRaw10 returnsClosePrevMktres10 returnsOpenPrevMktres10 returnsOpenNextMktres10 universe close_to_open 1862465 2011-11-02 PGN.N Paragon Offshore PLC 3444214.0 0.736000 0.090000 7.177778 0.000000 6.980656 -0.006865 7.177778 0.000000 7.363568 0.000322 -0.095988 True 8.177777 3845015 2016-07-06 BBBY.O Bed Bath &amp; Beyond Inc 50303.0 123.449997 30.680000 1.921202 -0.295522 1.897847 -0.291610 1.849065 -0.303044 1.826249 -0.302549 -0.047101 True 4.023794 3845467 2016-07-06 FLEX.O Flex Ltd 175451.0 123.449997 16.440001 9.587479 0.405128 9.482847 0.404713 8.503464 0.254962 8.425788 0.255200 0.087592 True 7.509124 3845835 2016-07-06 MAT.O Mattel Inc 56994.0 123.449997 21.540001 2.904175 -0.315321 2.864919 -0.304480 2.842204 -0.334363 2.812233 -0.334071 -0.069237 True 5.731197 3846276 2016-07-06 SHLD.O Sears Holdings Corp 80940.0 123.470001 8.840000 8.512326 -0.351431 8.417828 -0.345786 7.781650 -0.370818 7.710716 -0.370630 0.059298 False 13.967195 3846636 2016-07-06 ZNGA.O Zynga Inc 418847.0 123.470001 3.650000 45.592453 0.403846 45.122437 0.401845 46.671814 0.420233 46.249714 0.420887 -0.045244 False 33.827396 <p>We can observe that there is some data error for BBBY FLEX MAT as three of them has the same closing price on same data which is not possible. Lets delete those 3 rows</p> <pre><code>market_data.drop([3845015,3845467,3845835], inplace=True)\n</code></pre> <pre><code>market_data[market_data['close_to_open'] &lt;= 0.5]\n</code></pre> time assetCode assetName volume close open returnsClosePrevRaw1 returnsOpenPrevRaw1 returnsClosePrevMktres1 returnsOpenPrevMktres1 returnsClosePrevRaw10 returnsOpenPrevRaw10 returnsClosePrevMktres10 returnsOpenPrevMktres10 returnsOpenNextMktres10 universe close_to_open 1127598 2010-01-04 TW.N Towers Watson &amp; Co 223136.0 50.00 9998.990234 -0.058470 185.988358 -0.056911 163.671295 -0.024316 189.918701 0.034162 188.184860 13.167586 False 0.005001 1879901 2011-11-16 RMBS.O Rambus Inc 17227592.0 7.11 17.860001 -0.605876 0.010181 -0.559494 0.006892 -0.595333 0.036564 -0.593534 0.006536 -0.000272 True 0.398096 3264631 2015-03-16 TECD.O Tech Data Corp 674385.0 56.59 263.799988 0.036447 3.868057 0.020318 3.781770 -0.066172 3.442573 -0.021922 3.284477 0.072616 False 0.214519 <pre><code>data = []\nfor columnnames in ['close','open']:\n    asset_df = market_data[(market_data['assetName'] == 'Towers Watson &amp; Co')]\n    data.append(go.Scatter(\n        x = asset_df['time'].values,\n        y = asset_df[columnnames].values,\n        name = columnnames\n    ))\nlayout = go.Layout(dict(title = \"closing Vs Opening of Towers Watson &amp; Co\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')\n</code></pre> <p>We can see that the value is totally absurd. Let's delete it.</p> <pre><code>market_data.drop([1127598], inplace=True)\n</code></pre> <pre><code>data = []\nfor columnnames in ['close','open']:\n    asset_df = market_data[(market_data['assetName'] == 'Rambus Inc')]\n    data.append(go.Scatter(\n        x = asset_df['time'].values,\n        y = asset_df[columnnames].values,\n        name = columnnames\n    ))\nlayout = go.Layout(dict(title = \"closing Vs Opening of Towers Watson &amp; Co\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')\n</code></pre> <pre><code>market_data=market_data.drop(['close_to_open'],axis=1)\n</code></pre> <pre><code>return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close','volume']\n</code></pre> <pre><code>assetname=market_data.assetName.unique()\n</code></pre> <pre><code>assetname\n</code></pre> <pre><code>[Agilent Technologies Inc, AirTran Holdings Inc, Advance Auto Parts Inc, Apple Inc, AllianceBernstein Holding LP, ..., MTGE Investment Corp, SiteOne Landscape Supply Inc, FCB Financial Holdings Inc, AMC Entertainment Holdings Inc, Calavo Growers Inc]\nLength: 3254\nCategories (3254, object): [Agilent Technologies Inc, AirTran Holdings Inc, Advance Auto Parts Inc, Apple Inc, ..., SiteOne Landscape Supply Inc, FCB Financial Holdings Inc, AMC Entertainment Holdings Inc, Calavo Growers Inc]\n</code></pre> <pre><code>market_data['volume_shift']=market_data.groupby('assetName')['volume'].shift(1)\n</code></pre> <pre><code>market_data['volume_change']=market_data.volume_shift/market_data.volume\n</code></pre> <pre><code>print(f\"In {(market_data['volume_change'] &gt;= 2).sum()} lines price increased by 100% or more.\")\nprint(f\"In {(market_data['volume_change'] &lt;= 0.5).sum()} lines price decreased by 100% or more.\")\n</code></pre> <pre><code>In 203289 lines price increased by 100% or more.\nIn 228387 lines price decreased by 100% or more.\n</code></pre> <pre><code>market_data=market_data.drop(['volume_shift','volume_change'],axis=1)\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Market%20EDA/Two%20Sigma%20Market%20EDA/#storing-the-processed-data","title":"Storing the processed data","text":"<pre><code>market_data.to_csv('marketdataprocesssed.csv')\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Modeling/Two%20Sigma%20Modeling/","title":"Applying models on data.","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.linear_model import LogisticRegression\nfrom lightgbm import LGBMClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nfrom xgboost import XGBClassifier\n</code></pre> <pre><code>traindata=pd.read_csv('train.csv')\n</code></pre> <pre><code>testdata=pd.read_csv('test.csv')\n</code></pre> <pre><code>traindata=traindata.iloc[:,1:]\ntestdata=testdata.iloc[:,1:]\n</code></pre> <pre><code>traindata.columns\n</code></pre> <pre><code>Index(['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsOpenNextMktres10', 'universe', 'urgency', 'bodySize',\n       'companyCount', 'marketCommentary', 'sentenceCount', 'wordCount',\n       'relevance', 'sentimentClass', 'sentimentNegative', 'sentimentNeutral',\n       'sentimentPositive', 'sentimentWordCount', 'noveltyCount3D',\n       'volumeCounts3D', 'headlinelength',\n       'returnsClosePrevMktres10_lag_3_median',\n       'returnsClosePrevMktres10_lag_3_max',\n       'returnsClosePrevMktres10_lag_3_min',\n       'returnsClosePrevMktres10_lag_7_median',\n       'returnsClosePrevMktres10_lag_7_max',\n       'returnsClosePrevMktres10_lag_7_min',\n       'returnsClosePrevMktres10_lag_14_median',\n       'returnsClosePrevMktres10_lag_14_max',\n       'returnsClosePrevMktres10_lag_14_min',\n       'returnsClosePrevRaw10_lag_3_median', 'returnsClosePrevRaw10_lag_3_max',\n       'returnsClosePrevRaw10_lag_3_min', 'returnsClosePrevRaw10_lag_7_median',\n       'returnsClosePrevRaw10_lag_7_max', 'returnsClosePrevRaw10_lag_7_min',\n       'returnsClosePrevRaw10_lag_14_median',\n       'returnsClosePrevRaw10_lag_14_max', 'returnsClosePrevRaw10_lag_14_min',\n       'open_lag_3_median', 'open_lag_3_max', 'open_lag_3_min',\n       'open_lag_7_median', 'open_lag_7_max', 'open_lag_7_min',\n       'open_lag_14_median', 'open_lag_14_max', 'open_lag_14_min',\n       'close_lag_3_median', 'close_lag_3_max', 'close_lag_3_min',\n       'close_lag_7_median', 'close_lag_7_max', 'close_lag_7_min',\n       'close_lag_14_median', 'close_lag_14_max', 'close_lag_14_min',\n       'closeopentovolume', 'meanvolume', 'meanclose', 'stdclose', 'dayofweek',\n       'quarter', 'month', 'year', 'close_30EMA', 'close_26EMA', 'close_12EMA',\n       'MACD', 'MA_7MA', 'MA_7MA_std', 'MA_7MA_BB_high', 'MA_7MA_BB_low',\n       'RSI', 'beta', 'sin_quarter', 'sin_dayofweek', 'sin_month'],\n      dtype='object')\n</code></pre> <pre><code>traindata['time']=pd.to_datetime(traindata['time'], format='%Y-%m-%d')\ntestdata['time']=pd.to_datetime(testdata['time'], format='%Y-%m-%d')\n</code></pre> <pre><code>universe_test = testdata['universe']\ntime_test = testdata['time']\nuniverse_train=traindata['universe']\ntime_train=traindata['time']\ny_train=(traindata.returnsOpenNextMktres10&gt;=0).astype('int')\ny_test=(testdata.returnsOpenNextMktres10&gt;=0).astype('int')\ny_train1=traindata.returnsOpenNextMktres10\ny_test1=testdata.returnsOpenNextMktres10\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Modeling/Two%20Sigma%20Modeling/#removing-news-data","title":"Removing news data","text":"<pre><code>cols=['volume','returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsClosePrevMktres10_lag_3_median',\n       'returnsClosePrevMktres10_lag_3_max',\n       'returnsClosePrevMktres10_lag_3_min',\n       'returnsClosePrevMktres10_lag_7_median',\n       'returnsClosePrevMktres10_lag_7_max',\n       'returnsClosePrevMktres10_lag_7_min',\n       'returnsClosePrevMktres10_lag_14_median',\n       'returnsClosePrevMktres10_lag_14_max',\n       'returnsClosePrevMktres10_lag_14_min',\n       'returnsClosePrevRaw10_lag_3_median', 'returnsClosePrevRaw10_lag_3_max',\n       'returnsClosePrevRaw10_lag_3_min', 'returnsClosePrevRaw10_lag_7_median',\n       'returnsClosePrevRaw10_lag_7_max', 'returnsClosePrevRaw10_lag_7_min',\n       'returnsClosePrevRaw10_lag_14_median',\n       'returnsClosePrevRaw10_lag_14_max', 'returnsClosePrevRaw10_lag_14_min',\n       'open_lag_3_median', 'open_lag_3_max', 'open_lag_3_min',\n       'open_lag_7_median', 'open_lag_7_max', 'open_lag_7_min',\n       'open_lag_14_median', 'open_lag_14_max', 'open_lag_14_min',\n       'close_lag_3_median', 'close_lag_3_max', 'close_lag_3_min',\n       'close_lag_7_median', 'close_lag_7_max', 'close_lag_7_min',\n       'close_lag_14_median', 'close_lag_14_max', 'close_lag_14_min',\n       'closeopentovolume', 'meanvolume', 'meanclose', 'stdclose','close_30EMA', 'close_26EMA', 'close_12EMA',\n       'MACD', 'MA_7MA', 'MA_7MA_std', 'MA_7MA_BB_high', 'MA_7MA_BB_low',\n       'RSI', 'beta', 'sin_quarter', 'sin_dayofweek', 'sin_month','year']\nX_train=traindata[cols]\nX_test=testdata[cols]\n</code></pre> <pre><code>corr = X_train.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n</code></pre> <pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1ea4062aac8&gt;\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Modeling/Two%20Sigma%20Modeling/#defining-custom-metric-functions","title":"Defining custom metric functions","text":"<pre><code>def sigma_score(pred,valid_data):\n    pred=np.clip(pred,-1,1)\n    if(len(pred)==len(y_test)):\n        df_time = time_test.factorize()[0]\n        universe=universe_test\n        labels=y_test1*universe\n    elif(len(pred)==len(y_train)):\n        df_time = time_train.factorize()[0]\n        universe=universe_train\n        labels=y_train1*universe\n    x_t = pred * labels\n    x_t_sum = x_t.groupby(df_time).sum()\n    score = x_t_sum.mean() / x_t_sum.std()\n    return 'sigma_score', score, True\ndef sigma_score_2(preds, valid_data):\n    preds=preds*2-1\n    if(len(preds)==len(y_train)):\n        df_time= time_train.factorize()[0]\n        labels=y_train1\n        x_t = preds*labels*universe_train\n#    df_time = valid_data.params['extra_time']\n#    labels = valid_data.get_label()\n    elif(len(preds)==len(y_test)):\n        df_time= time_test.factorize()[0]\n        labels=y_test1\n        x_t = preds*labels*universe_test\n\n   # labels=y_test\n #   preds=pd.DataFrame(preds).clip(-1,1).values\n  #  preds=preds.reshape(len(preds), )\n#    assert len(labels) == len(df_time)\n    #  * df_valid['universe'] -&gt; Here we take out the 'universe' term because we already keep only those equals to 1.\n\n    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n    # is a pd.Series and call `group_by`\n    x_t_sum = x_t.groupby(df_time).sum()\n    score = x_t_sum.mean() / x_t_sum.std()\n\n    return 'sigma_score', score, True\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Modeling/Two%20Sigma%20Modeling/#time-series-split","title":"Time series split","text":"<pre><code>tscv = TimeSeriesSplit(n_splits=2)\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Modeling/Two%20Sigma%20Modeling/#classifiers","title":"Classifiers","text":""},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Modeling/Two%20Sigma%20Modeling/#logistic-regression","title":"Logistic regression","text":"<pre><code>model=LogisticRegression()\nmodel.fit(X_train,y_train)\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n\n\n\n\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False)\n</code></pre> <pre><code>y_pred=model.predict_proba(X_test)\nsigma_score(y_pred[:,1]*2-1,y_test1.values)\n</code></pre> <pre><code>('sigma_score', 0.37416417744774055, True)\n</code></pre> <pre><code>clf = LogisticRegression()\n\nparam_grid = { \n    'C': [0.001,0.01,0.1,1,10],\n    'penalty':['l2']\n   }\n\nrfc = RandomizedSearchCV(estimator=clf, param_distributions =param_grid, cv= tscv,n_jobs=-1,scoring ='neg_log_loss')\nrfc.fit(X_train, y_train)\n</code></pre> <pre><code>y_pred=clf.predict_proba(X_test)\ny_pred=y_pred[:,1]*2-1\nsigma_score(y_pred,y_test1)\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Modeling/Two%20Sigma%20Modeling/#random-forest","title":"Random forest","text":"<pre><code>clf = RandomForestClassifier(n_jobs=-1)\nmodel=clf.fit(X_train,y_train)\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n</code></pre> <pre><code>y_pred=clf.predict_proba(X_test)\ny_pred=y_pred[:,1]*2-1\nsigma_score(y_pred,y_test1)\n</code></pre> <pre><code>('sigma_score', 0.3750540242407995, True)\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Modeling/Two%20Sigma%20Modeling/#calibrated-on-rf","title":"Calibrated on RF","text":"<pre><code>from sklearn.calibration import CalibratedClassifierCV\ncalibrated_clf = CalibratedClassifierCV(model, method='sigmoid', cv=tscv)\ncalibrated_clf.fit(X_train, y_train)\n</code></pre> <pre><code>CalibratedClassifierCV(base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False),\n            cv=TimeSeriesSplit(max_train_size=None, n_splits=2),\n            method='sigmoid')\n</code></pre> <pre><code>Ytrainpred=calibrated_clf.predict_proba(X_train)\n</code></pre> <pre><code>Ytestpred=calibrated_clf.predict_proba(X_test)\n</code></pre> <pre><code>ytrainpred=Ytrainpred[:,1]\nytestpred=Ytestpred[:,1]\n</code></pre> <pre><code>sigma_score(ytrainpred*2-1,y_train1.values)\n</code></pre> <pre><code>('sigma_score', 1.1045316341808502, True)\n</code></pre> <pre><code>sigma_score(ytestpred*2-1,y_train1.values)\n</code></pre> <pre><code>('sigma_score', 0.4862011486445091, True)\n</code></pre> <pre><code>feature_importances = pd.DataFrame(clf.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\n</code></pre> <pre><code>feature_importances.tail()\n</code></pre> importance sin_dayofweek 0.006310 sin_quarter 0.003783 stdclose 0.000000 meanclose 0.000000 meanvolume 0.000000"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Modeling/Two%20Sigma%20Modeling/#grid-search-on-rf","title":"Grid search on RF","text":"<pre><code>clf = RandomForestClassifier(n_jobs=-1)\n\nparam_grid = { \n    'n_estimators': [100,200,300,500],\n    'max_depth' : [4,5,6,7,8],\n}\n\nrfc = RandomizedSearchCV(estimator=clf, param_distributions =param_grid, cv= tscv,n_jobs=-1,scoring ='neg_log_loss')\nrfc.fit(X_train, y_train)\n</code></pre> <pre><code>RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=2),\n          error_score='raise-deprecating',\n          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False),\n          fit_params=None, iid='warn', n_iter=10, n_jobs=-1,\n          param_distributions={'n_estimators': [100, 200, 300, 500], 'max_depth': [4, 5, 6, 7, 8]},\n          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n          return_train_score='warn', scoring='neg_log_loss', verbose=0)\n</code></pre> <pre><code>rfc.best_params_\n</code></pre> <pre><code>{'n_estimators': 300, 'max_depth': 8}\n</code></pre> <pre><code>y_pred=rfc.predict_proba(X_train)*2-1\n</code></pre> <pre><code>y_pred=y_pred[:,1]\n</code></pre> <pre><code>sigma_score(y_pred,y_train1)\n</code></pre> <pre><code>('sigma_score', 0.7134924265006625, True)\n</code></pre> <pre><code>y_pred=rfc.predict_proba(X_test)*2-1\nsigma_score(y_pred[:,1],y_test1)\n</code></pre> <pre><code>('sigma_score', 0.5346658390165596, True)\n</code></pre> <pre><code>sns.distplot(y_pred[:,1])\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n\n\n\n\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x1ea4b178550&gt;\n</code></pre> <pre><code>confusion_matrix(y_test,y_pred[:,1]&gt;=0)\n</code></pre> <pre><code>array([[173933, 189148],\n       [136749, 217843]], dtype=int64)\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Modeling/Two%20Sigma%20Modeling/#calibrated-on-tuned-model","title":"Calibrated on tuned model","text":"<pre><code>from sklearn.calibration import CalibratedClassifierCV\nclf = RandomForestClassifier(n_estimators=300,max_depth=7)\ncalibrated_clf = CalibratedClassifierCV(clf, method='sigmoid', cv=tscv)\ncalibrated_clf.fit(X_train, y_train)\n</code></pre> <pre><code>CalibratedClassifierCV(base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=7, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False),\n            cv=TimeSeriesSplit(max_train_size=None, n_splits=2),\n            method='sigmoid')\n</code></pre> <pre><code>y_pred=calibrated_clf.predict_proba(X_test)\n</code></pre> <pre><code>sigma_score(y_pred[:,1]*2-1,y_test1)\n</code></pre> <pre><code>('sigma_score', 0.5485461670781893, True)\n</code></pre> <pre><code>y_pred=calibrated_clf.predict_proba(X_train)\nsigma_score(y_pred[:,1]*2-1,y_train1)\n</code></pre> <pre><code>('sigma_score', 0.6121033369983073, True)\n</code></pre> <pre><code>y_pred=y_pred[:,1]&gt;=0\n</code></pre> <pre><code>confusion_matrix(y_train,y_pred)\n</code></pre> <pre><code>array([[      0, 1280993],\n       [      0, 1341474]], dtype=int64)\n</code></pre> <pre><code>y_pred=calibrated_clf.predict_proba(X_train)\n</code></pre> <pre><code>y_pred=y_pred[:,1]*2-1\n</code></pre> <pre><code>sns.distplot(y_pred)\n</code></pre> <pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1f8569cdd30&gt;\n</code></pre> <pre><code>train_cols = X_train.columns.tolist()\n# Note: y data is expected to be a pandas Series, as we will use its group_by function in `sigma_score`\ndtrain = lgb.Dataset(X_train.values, y_train,feature_name=train_cols,free_raw_data=False)\ndvalid = lgb.Dataset(X_test.values, y_test,feature_name=train_cols, free_raw_data=False)\n</code></pre> <pre><code>params = {'boosting_type': 'gbdt',\n          'max_depth' : -1,\n          'objective': 'binary',\n          'nthread': 3, # Updated from nthread\n          'num_leaves': 64,\n          'learning_rate': 0.05,\n          'max_bin': 512,\n          'subsample_for_bin': 200,\n          'subsample': 1,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.8,\n          'reg_alpha': 5,\n          'reg_lambda': 10,\n          'min_split_gain': 0.5,\n          'min_child_weight': 1,\n          'min_child_samples': 5,\n          'scale_pos_weight': 1,\n          'num_class' : 1,\n          'metric' : 'binary_error'}\n\n# Create parameters to search\ngridParams = {\n    'learning_rate': [0.005],\n    'n_estimators': [40,150],\n    'num_leaves': [6,8,12],\n    'boosting_type' : ['gbdt'],\n    'objective' : ['binary'],\n    'random_state' : [501], # Updated from 'seed'\n    'colsample_bytree' : [0.65, 0.66],\n    'subsample' : [0.7,0.75],\n    'reg_alpha' : [1,1.2],\n    'reg_lambda' : [1,1.2,1.4],\n    }\n\n# Create classifier to use. Note that parameters have to be input manually\n# not as a dict!\nevals_result = {}\n\nmdl = lgb.LGBMClassifier(boosting_type= 'gbdt',\n          objective = 'binary',\n          n_jobs = 3, \n          silent = False,\n          max_depth = params['max_depth'],\n          max_bin = params['max_bin'],\n          subsample_for_bin = params['subsample_for_bin'],\n          subsample = params['subsample'],\n          subsample_freq = params['subsample_freq'],\n          min_split_gain = params['min_split_gain'],\n          min_child_weight = params['min_child_weight'],\n          min_child_samples = params['min_child_samples'],\n          scale_pos_weight = params['scale_pos_weight'])\n\n# To view the default model params:\nmdl.get_params().keys()\n\n# Create the grid\ngrid = RandomizedSearchCV(mdl, gridParams,\n                    verbose=0,\n                    cv=tscv,\n                    n_jobs=2,scoring='neg_log_loss')\n# Run the grid\ngrid.fit(X_train,y_train)\n\n# Print the best parameters found\nprint(grid.best_params_)\nprint(grid.best_score_)\n</code></pre> <pre><code>{'subsample': 0.75, 'reg_lambda': 1.2, 'reg_alpha': 1, 'random_state': 501, 'objective': 'binary', 'num_leaves': 12, 'n_estimators': 150, 'learning_rate': 0.005, 'colsample_bytree': 0.66, 'boosting_type': 'gbdt'}\n-0.6895867716593379\n</code></pre> <pre><code>x_1 = [0.19000424246380565, 2452, 212, 328, 202]\n\nparams_1 = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'learning_rate': 0.005,\n        'num_iteration': 500,\n        'n_estimators':150,\n        'colsample_bytree': 0.66, \n         'subsample': 0.75,\n        'verbose': 1,\n         'metric':None,\n    'reg_lambda': 1.2,\n    'reg_alpha': 1,\n    'random_state': 501,\n    'objective': 'binary',\n    'num_leaves': 12\n    }\n\n\ngbm_1 = lgb.train(params_1,\n        dtrain,\n        num_boost_round=1000,\n        valid_sets=dvalid,\n        feval=sigma_score_2,\n        early_stopping_rounds=30,\n        verbose_eval=25\n        )\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n\n\nTraining until validation scores don't improve for 30 rounds.\n[25]    valid_0's binary_logloss: 0.692633  valid_0's sigma_score: 0.0795633\n[50]    valid_0's binary_logloss: 0.691774  valid_0's sigma_score: 0.273122\n[75]    valid_0's binary_logloss: 0.691024  valid_0's sigma_score: 0.397076\n[100]   valid_0's binary_logloss: 0.690331  valid_0's sigma_score: 0.477671\n[125]   valid_0's binary_logloss: 0.689849  valid_0's sigma_score: 0.52121\n[150]   valid_0's binary_logloss: 0.689309  valid_0's sigma_score: 0.551341\n[175]   valid_0's binary_logloss: 0.688996  valid_0's sigma_score: 0.562054\n[200]   valid_0's binary_logloss: 0.688644  valid_0's sigma_score: 0.572705\n[225]   valid_0's binary_logloss: 0.688447  valid_0's sigma_score: 0.573533\n[250]   valid_0's binary_logloss: 0.688145  valid_0's sigma_score: 0.579904\n[275]   valid_0's binary_logloss: 0.687972  valid_0's sigma_score: 0.579298\nEarly stopping, best iteration is:\n[254]   valid_0's binary_logloss: 0.688098  valid_0's sigma_score: 0.580737\n</code></pre> <pre><code>plt.figure(figsize=(40,20))\nlgb.plot_importance(gbm_1,max_num_features=15)\nplt.show()\n</code></pre> <pre><code>&lt;Figure size 2880x1440 with 0 Axes&gt;\n</code></pre> <pre><code>y_pred=gbm_1.predict(X_train)\n</code></pre> <pre><code>sigma_score(y_pred*2-1,y_train1)\n</code></pre> <pre><code>('sigma_score', 0.6859160729981825, True)\n</code></pre> <pre><code>y_pred=gbm_1.predict(X_test)\n</code></pre> <pre><code>sigma_score(y_pred*2-1,y_test1)\n</code></pre> <pre><code>('sigma_score', 0.5807365552305024, True)\n</code></pre> <pre><code>sns.distplot(y_pred*2-1)\n</code></pre> <pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1f8875b84e0&gt;\n</code></pre> <pre><code>clf = lgb.LGBMClassifier(task= 'train',\n        boosting_type= 'gbdt',\n        objective='binary',\n        learning_rate= 0.005,\n        num_iteration= 500,\n        n_estimators=150,\n        colsample_bytree= 0.66, \n        subsample= 0.75,\n        verbose =1,\n         reg_lambda= 1.2,\n    reg_alpha= 1,\n    random_state= 501,\n    num_leaves= 12\n)\n</code></pre> <pre><code>calibrated_clf = CalibratedClassifierCV(clf, method='isotonic', cv=tscv)\ncalibrated_clf.fit(X_train, y_train)\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\nC:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n\n\n\n\n\nCalibratedClassifierCV(base_estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.66,\n        importance_type='split', learning_rate=0.005, max_depth=-1,\n        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n        n_estimators=150, n_jobs=-1, num_iteration=500, num_leaves=12,\n        objective='binary', random_state=501, reg_alpha=1, reg_lambda=1.2,\n        silent=True, subsample=0.75, subsample_for_bin=200000,\n        subsample_freq=0, task='train', verbose=1),\n            cv=TimeSeriesSplit(max_train_size=None, n_splits=2),\n            method='isotonic')\n</code></pre> <pre><code>Ytrainpred=calibrated_clf.predict_proba(X_train)\n</code></pre> <pre><code>Ytestpred=calibrated_clf.predict_proba(X_test)\n</code></pre> <pre><code>ytrainpred=Ytrainpred[:,1]\nytestpred=Ytestpred[:,1]\n</code></pre> <pre><code>sigma_score(ytrainpred*2-1,y_train1.values)\n</code></pre> <pre><code>('sigma_score', 0.6237107583636623, True)\n</code></pre> <pre><code>sigma_score(ytestpred*2-1,y_train1.values)\n</code></pre> <pre><code>('sigma_score', 0.5677100719773357, True)\n</code></pre> <pre><code>xgb = XGBClassifier(learning_rate=0.1, n_estimators=300, objective='binary:logistic',\n                    silent=True, nthread=2)\n</code></pre> <pre><code>model=xgb.fit(X_train,y_train)\n</code></pre> <pre><code>y_pred=xgb.predict_proba(X_test)\n</code></pre> <pre><code>sigma_score(y_pred[:,1]*2-1,y_test1)\n</code></pre> <pre><code>('sigma_score', 0.48366598850149906, True)\n</code></pre> <pre><code>y_pred=xgb.predict_proba(X_train)\nsigma_score(y_pred[:,1]*2-1,y_train1)\n</code></pre> <pre><code>('sigma_score', 0.8113740249829478, True)\n</code></pre> <pre><code>y_pred=y_pred[:,1]&gt;=0\n</code></pre> <pre><code>confusion_matrix(y_train,y_pred)\n</code></pre> <pre><code>array([[601906, 679087],\n       [447189, 894285]], dtype=int64)\n</code></pre> <pre><code>xgb = XGBClassifier(objective='binary:logistic')\n</code></pre> <pre><code>clf = RandomizedSearchCV(xgb,{'max_depth': [2,4,6],\n                    'n_estimators': [50,100,200]}, \n                    verbose=1, \n                    scoring='neg_log_loss',\n                   cv= tscv,n_jobs=-1\n                     )\n\nclf.fit(X_train,y_train)\nclf.best_score_, clf.best_params_\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:271: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n  % (grid_size, self.n_iter, grid_size), UserWarning)\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n\n\nFitting 2 folds for each of 9 candidates, totalling 18 fits\n\n\n[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed: 167.0min finished\n\n\n\n\n\n(-0.6870596950771145, {'n_estimators': 50, 'max_depth': 2})\n</code></pre> <pre><code>y_pred=clf.predict_proba(X_test)\n</code></pre> <pre><code>sigma_score(y_pred[:,1]*2-1,y_test1)\n</code></pre> <pre><code>('sigma_score', 0.5466463282705594, True)\n</code></pre> <pre><code>y_pred=clf.predict_proba(X_train)\nsigma_score(y_pred[:,1]*2-1,y_train1)\n</code></pre> <pre><code>('sigma_score', 0.6633845835678877, True)\n</code></pre> <pre><code>y_pred=y_pred[:,1]&gt;=0\n</code></pre> <pre><code>confusion_matrix(y_train,y_pred)\n</code></pre> <pre><code>array([[      0, 1280993],\n       [      0, 1341474]], dtype=int64)\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Modeling/Two%20Sigma%20Modeling/#calibrated-on-xg-boost","title":"Calibrated on XG BOOST","text":"<pre><code>xgb = XGBClassifier(objective='binary:logistic',n_estimators=50,max_depth=2)\n</code></pre> <pre><code>from sklearn.calibration import CalibratedClassifierCV\ncalibrated_clf = CalibratedClassifierCV(xgb, method='sigmoid', cv=tscv)\ncalibrated_clf.fit(X_train, y_train)\n</code></pre> <pre><code>CalibratedClassifierCV(base_estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=2, min_child_weight=1, missing=None, n_estimators=50,\n       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1),\n            cv=TimeSeriesSplit(max_train_size=None, n_splits=2),\n            method='sigmoid')\n</code></pre> <pre><code>Ytrainpred=calibrated_clf.predict_proba(X_train)\n</code></pre> <pre><code>Ytestpred=calibrated_clf.predict_proba(X_test)\n</code></pre> <pre><code>ytrainpred=Ytrainpred[:,1]\nytestpred=Ytestpred[:,1]\n</code></pre> <pre><code>sigma_score(ytrainpred*2-1,y_train1.values)\n</code></pre> <pre><code>('sigma_score', 0.5896574335387855, True)\n</code></pre> <pre><code>sigma_score(ytestpred*2-1,y_test1.values)\n</code></pre> <pre><code>('sigma_score', 0.549996122565269, True)\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20Modeling/Two%20Sigma%20Modeling/#using-regression","title":"Using regression","text":"<pre><code>train_cols = X_train.columns.tolist()\n\n# Note: y data is expected to be a pandas Series, as we will use its group_by function in `sigma_score`\ndtrain = lgb.Dataset(X_train.values, y_train1,feature_name=train_cols, free_raw_data=False)\ndvalid = lgb.Dataset(X_test.values, y_test1,feature_name=train_cols,free_raw_data=False)\n</code></pre> <pre><code>lgb_params = dict(\n    objective = 'regression_l1',\n    learning_rate = 0.005,\n    bagging_fraction = 0.75,\n    bagging_freq = 2,\n    feature_fraction = 0.5,\n    num_iteration= 500,\n    n_estimators=150,\n    colsample_bytree= 0.66, \n    subsample= 0.75,\n    verbose =1,\n    reg_lambda= 1.2,\n    reg_alpha= 1,\n    random_state= 501,\n    num_leaves= 12,\n    lambda_l1 = 0.5,\n    lambda_l2 = 1.0,\n    metric = 'None', # This will ignore the loss objetive and use sigma_score instead,\n    seed = 0 # Change for better luck! :)\n)\n</code></pre> <pre><code>evals_result = {}\nm = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dtrain,dvalid), valid_names=('train','valid'), verbose_eval=25, feval=sigma_score, evals_result=evals_result)\ndf_result = pd.DataFrame(evals_result['valid'])\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n\n\n[25]    train's sigma_score: 0.418809   valid's sigma_score: 0.117999\n[50]    train's sigma_score: 0.568597   valid's sigma_score: 0.307166\n[75]    train's sigma_score: 0.620927   valid's sigma_score: 0.421572\n[100]   train's sigma_score: 0.635875   valid's sigma_score: 0.482216\n[125]   train's sigma_score: 0.636294   valid's sigma_score: 0.512729\n[150]   train's sigma_score: 0.638908   valid's sigma_score: 0.531243\n[175]   train's sigma_score: 0.636937   valid's sigma_score: 0.5422\n[200]   train's sigma_score: 0.637198   valid's sigma_score: 0.547239\n[225]   train's sigma_score: 0.635802   valid's sigma_score: 0.551443\n[250]   train's sigma_score: 0.632717   valid's sigma_score: 0.554628\n[275]   train's sigma_score: 0.633287   valid's sigma_score: 0.554167\n[300]   train's sigma_score: 0.634656   valid's sigma_score: 0.556521\n[325]   train's sigma_score: 0.63648    valid's sigma_score: 0.556715\n[350]   train's sigma_score: 0.637532   valid's sigma_score: 0.55786\n[375]   train's sigma_score: 0.636781   valid's sigma_score: 0.558729\n[400]   train's sigma_score: 0.637117   valid's sigma_score: 0.558295\n[425]   train's sigma_score: 0.637578   valid's sigma_score: 0.558153\n[450]   train's sigma_score: 0.638075   valid's sigma_score: 0.558225\n[475]   train's sigma_score: 0.637123   valid's sigma_score: 0.558062\n[500]   train's sigma_score: 0.636048   valid's sigma_score: 0.557664\n</code></pre> <pre><code>y_pred=m.predict(X_test)\n</code></pre> <pre><code>sns.distplot(y_pred)\n</code></pre> <pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1f88893d978&gt;\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 2, figsize=(14, 14))\nlgb.plot_importance(m, ax=ax[0])\nlgb.plot_importance(m, ax=ax[1], importance_type='gain')\nfig.tight_layout()\n</code></pre> <pre><code>clf = RandomForestRegressor(n_estimators=200, max_depth=5,random_state=0,n_jobs=-1)\n</code></pre> <pre><code>model=clf.fit(X_train,y_train1)\n</code></pre> <pre><code>y_pred=model.predict(X_test)\n</code></pre> <pre><code>sigma_score(y_pred,y_test1)\n</code></pre> <pre><code>('sigma_score', 0.3476250043396745, True)\n</code></pre> <pre><code>feature_importances = pd.DataFrame(clf.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\n\nfeature_importances.head()\n</code></pre> importance returnsClosePrevMktres10_lag_14_min 0.267275 close_30EMA 0.132671 close_26EMA 0.131267 returnsOpenPrevMktres1 0.044182 returnsOpenPrevRaw10 0.037511 <pre><code>#model = RandomForestRegressor()\n#param_grid = { \n#    'n_estimators': [100,250,500],\n#    'max_depth' : [3,5,7],\n#}\n#rfc = GridSearchCV(estimator=model, param_distributions =param_grid, cv= tscv,n_jobs=2)\n#rfc.fit(X_train, y_train)\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:271: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n  % (grid_size, self.n_iter, grid_size), UserWarning)\n</code></pre> <pre><code>#rfc.best_params_\n</code></pre> <pre><code>#y_pred=rfc.predict(X_test)\n</code></pre> <pre><code>#sns.distplot(y_pred)\n</code></pre> <pre><code>#sigma_score(y_pred,y_train1)\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20News%20EDA/Two%20Sigma%20News%20EDA/","title":"News EDA","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n</code></pre> <pre><code>news_data=pd.read_csv('newsdata.csv')\n</code></pre> <pre><code>news_data.head()\n</code></pre> Unnamed: 0 time sourceTimestamp firstCreated sourceId headline urgency takeSequence provider subjects ... noveltyCount12H noveltyCount24H noveltyCount3D noveltyCount5D noveltyCount7D volumeCounts12H volumeCounts24H volumeCounts3D volumeCounts5D volumeCounts7D 0 0 2007-01-01 04:29:32+00:00 2007-01-01 04:29:32+00:00 2007-01-01 04:29:32+00:00 e58c6279551b85cf China's Daqing pumps 43.41 mln tonnes of oil i... 3 1 RTRS {'ENR', 'ASIA', 'CN', 'NGS', 'EMRG', 'RTRS', '... ... 0 0 0 0 0 0 0 3 6 7 1 1 2007-01-01 07:03:35+00:00 2007-01-01 07:03:34+00:00 2007-01-01 07:03:34+00:00 5a31c4327427f63f FEATURE-In kidnapping, finesse works best 3 1 RTRS {'FEA', 'CA', 'LATAM', 'MX', 'INS', 'ASIA', 'I... ... 1 1 1 1 1 1 1 3 3 3 2 2 2007-01-01 11:29:56+00:00 2007-01-01 11:29:56+00:00 2007-01-01 11:29:56+00:00 1cefd27a40fabdfe PRESS DIGEST - Wall Street Journal - Jan 1 3 1 RTRS {'RET', 'ENR', 'ID', 'BG', 'US', 'PRESS', 'IQ'... ... 0 0 0 0 0 0 0 5 11 17 3 3 2007-01-01 12:08:37+00:00 2007-01-01 12:08:37+00:00 2007-01-01 12:08:37+00:00 23768af19dc69992 PRESS DIGEST - New York Times - Jan 1 3 1 RTRS {'FUND', 'FIN', 'CA', 'SFWR', 'INS', 'PUB', 'B... ... 0 0 0 0 0 0 0 5 13 15 4 4 2007-01-01 12:08:37+00:00 2007-01-01 12:08:37+00:00 2007-01-01 12:08:37+00:00 23768af19dc69992 PRESS DIGEST - New York Times - Jan 1 3 1 RTRS {'FUND', 'FIN', 'CA', 'SFWR', 'INS', 'PUB', 'B... ... 0 0 0 0 0 0 0 0 0 0 <p>5 rows \u00d7 36 columns</p> <pre><code>news_data=news_data.iloc[:,1:]\n</code></pre> <pre><code>news_data.shape\n</code></pre> <pre><code>(9328750, 35)\n</code></pre> <pre><code>news_data.isnull().sum()\n</code></pre> <pre><code>time                          0\nsourceTimestamp               0\nfirstCreated                  0\nsourceId                      0\nheadline                  73960\nurgency                       0\ntakeSequence                  0\nprovider                      0\nsubjects                      0\naudiences                     0\nbodySize                      0\ncompanyCount                  0\nheadlineTag             6341993\nmarketCommentary              0\nsentenceCount                 0\nwordCount                     0\nassetCodes                    0\nassetName                     0\nfirstMentionSentence          0\nrelevance                     0\nsentimentClass                0\nsentimentNegative             0\nsentimentNeutral              0\nsentimentPositive             0\nsentimentWordCount            0\nnoveltyCount12H               0\nnoveltyCount24H               0\nnoveltyCount3D                0\nnoveltyCount5D                0\nnoveltyCount7D                0\nvolumeCounts12H               0\nvolumeCounts24H               0\nvolumeCounts3D                0\nvolumeCounts5D                0\nvolumeCounts7D                0\ndtype: int64\n</code></pre> <pre><code>news_data.nunique()\n</code></pre> <pre><code>time                    5245343\nsourceTimestamp         5228340\nfirstCreated            3441535\nsourceId                6340206\nheadline                5532378\nurgency                       3\ntakeSequence                 97\nprovider                     30\nsubjects                1733963\naudiences                 88488\nbodySize                  61200\ncompanyCount                 43\nheadlineTag                 162\nmarketCommentary              2\nsentenceCount               669\nwordCount                 10365\nassetCodes                10691\nassetName                  8902\nfirstMentionSentence        516\nrelevance                 28412\nsentimentClass                3\nsentimentNegative       1321080\nsentimentNeutral         582507\nsentimentPositive       1025372\nsentimentWordCount         6493\nnoveltyCount12H             501\nnoveltyCount24H             501\nnoveltyCount3D              501\nnoveltyCount5D              501\nnoveltyCount7D              501\nvolumeCounts12H            2565\nvolumeCounts24H            2570\nvolumeCounts3D             2587\nvolumeCounts5D             2598\nvolumeCounts7D             2928\ndtype: int64\n</code></pre> <pre><code>news_data.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9328750 entries, 0 to 9328749\nData columns (total 35 columns):\ntime                    object\nsourceTimestamp         object\nfirstCreated            object\nsourceId                object\nheadline                object\nurgency                 int64\ntakeSequence            int64\nprovider                object\nsubjects                object\naudiences               object\nbodySize                int64\ncompanyCount            int64\nheadlineTag             object\nmarketCommentary        bool\nsentenceCount           int64\nwordCount               int64\nassetCodes              object\nassetName               object\nfirstMentionSentence    int64\nrelevance               float64\nsentimentClass          int64\nsentimentNegative       float64\nsentimentNeutral        float64\nsentimentPositive       float64\nsentimentWordCount      int64\nnoveltyCount12H         int64\nnoveltyCount24H         int64\nnoveltyCount3D          int64\nnoveltyCount5D          int64\nnoveltyCount7D          int64\nvolumeCounts12H         int64\nvolumeCounts24H         int64\nvolumeCounts3D          int64\nvolumeCounts5D          int64\nvolumeCounts7D          int64\ndtypes: bool(1), float64(4), int64(19), object(11)\nmemory usage: 2.4+ GB\n</code></pre>"},{"location":"Programming/Reprint/TwoSigma_StockMarket%20Prediction_Reprint/Two%20Sigma%20News%20EDA/Two%20Sigma%20News%20EDA/#lets-change-the-datatype-of-variables-to-make-them-more-functionable-and-reduce-the-memory-usage","title":"Let's change the datatype of variables to make them more functionable and reduce the memory usage.","text":"<pre><code>for columnname in news_data.columns:\n    if news_data[columnname].dtype=='float64':\n        news_data[columnname]=news_data[columnname].astype('float32')\n    elif news_data[columnname].dtype=='int64':\n        news_data[columnname]=news_data[columnname].astype('int32')\nnews_data['urgency']=news_data['urgency'].astype('int8')\nnews_data['provider']=news_data['provider'].astype('category')\nnews_data['subjects']=news_data['subjects'].astype('category')\nnews_data['audiences']=news_data['audiences'].astype('category')\n</code></pre> <pre><code>news_data['time'] =  pd.to_datetime(news_data['time'], format='%Y-%m-%d %H:%M:%S+00:00')\nnews_data['sourceTimestamp'] =  pd.to_datetime(news_data['sourceTimestamp'], format='%Y-%m-%d %H:%M:%S+00:00')\nnews_data['firstCreated'] =  pd.to_datetime(news_data['firstCreated'], format='%Y-%m-%d %H:%M:%S+00:00')\n</code></pre> <pre><code>news_data.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9328750 entries, 0 to 9328749\nData columns (total 35 columns):\ntime                    datetime64[ns]\nsourceTimestamp         datetime64[ns]\nfirstCreated            datetime64[ns]\nsourceId                object\nheadline                object\nurgency                 int8\ntakeSequence            int32\nprovider                category\nsubjects                category\naudiences               category\nbodySize                int32\ncompanyCount            int32\nheadlineTag             object\nmarketCommentary        bool\nsentenceCount           int32\nwordCount               int32\nassetCodes              object\nassetName               object\nfirstMentionSentence    int32\nrelevance               float32\nsentimentClass          int32\nsentimentNegative       float32\nsentimentNeutral        float32\nsentimentPositive       float32\nsentimentWordCount      int32\nnoveltyCount12H         int32\nnoveltyCount24H         int32\nnoveltyCount3D          int32\nnoveltyCount5D          int32\nnoveltyCount7D          int32\nvolumeCounts12H         int32\nvolumeCounts24H         int32\nvolumeCounts3D          int32\nvolumeCounts5D          int32\nvolumeCounts7D          int32\ndtypes: bool(1), category(3), datetime64[ns](3), float32(4), int32(18), int8(1), object(5)\nmemory usage: 1.5+ GB\n</code></pre> <pre><code>news_data=news_data[news_data.time&gt;datetime.date(2009, 1, 1)]\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n'datetime.date' is coerced to a datetime. In the future pandas will\nnot coerce, and a TypeError will be raised. To retain the current\nbehavior, convert the 'datetime.date' to a datetime with\n'pd.Timestamp'.\n  \"\"\"Entry point for launching an IPython kernel.\n</code></pre> <pre><code>100-news_data.shape[0]*100/9328750\n</code></pre> <pre><code>16.306514806378132\n</code></pre> <pre><code>news_data.head(2)\n</code></pre> time sourceTimestamp firstCreated sourceId headline urgency takeSequence provider subjects audiences ... noveltyCount12H noveltyCount24H noveltyCount3D noveltyCount5D noveltyCount7D volumeCounts12H volumeCounts24H volumeCounts3D volumeCounts5D volumeCounts7D 1521194 2009-01-01 00:25:02 2009-01-01 00:25:01 2009-01-01 00:25:01 6bf957cb4d7b0ce1 Churchill Downs Incorporated, KHBPA, KTA Reach... 3 1 BSW {'ENTS', 'EQUE', 'CYCS', 'NEWR', 'LEI', 'RELX'... {'BSW', 'CNR'} ... 0 0 0 0 0 0 0 0 0 0 1521195 2009-01-01 00:25:03 2009-01-01 00:25:01 2009-01-01 00:25:01 aceb69df2660a6f5 Corgi International Limited Announces Financia... 3 1 BSW {'CYCP', 'RET', 'CYCS', 'ASIA', 'HK', 'NEWR', ... {'BSW', 'CNR', 'CNRA'} ... 0 0 0 0 0 0 0 0 0 0 <p>2 rows \u00d7 35 columns</p> <pre><code>news_data['urgency'].value_counts()\n</code></pre> <pre><code>3    6162567\n1    3166158\n2         25\nName: urgency, dtype: int64\n</code></pre> <pre><code>news_data.headline[news_data['urgency']==1].head(1).values\n</code></pre> <pre><code>array([\"INDIA'S HERO HONDA &lt;HROH.BO&gt; DEC VEHICLE SALES 252,462  UNITS VS 245,104 YEAR AGO \"],\n      dtype=object)\n</code></pre> <pre><code>news_data.headline[news_data['urgency']==2].head(1).values\n</code></pre> <pre><code>array(['BRIEF-Terrapin 3 Acquisition Corporation to buy Yatra Online, Inc.&lt;TRTL.O&gt;'],\n      dtype=object)\n</code></pre> <pre><code>news_data.headline[news_data['urgency']==3].head(1).values\n</code></pre> <pre><code>array([\"China's Daqing pumps 43.41 mln tonnes of oil in 06\"], dtype=object)\n</code></pre> <pre><code>news_data.assetCodes[news_data['urgency']==3].head(1).values\n</code></pre> <pre><code>array([\"{'0857.HK', '0857.F', '0857.DE', 'PTR.N'}\"], dtype=object)\n</code></pre> <pre><code>news_data.takeSequence.value_counts()[:10]\n</code></pre> <pre><code>1     6747890\n2      831166\n3      465806\n4      310821\n5      222402\n6      163805\n7      124725\n8       96287\n9       74009\n10      56180\nName: takeSequence, dtype: int64\n</code></pre> <pre><code>news_data[['sentimentClass','sentimentNegative', 'sentimentNeutral','sentimentPositive']].head()\n</code></pre> sentimentClass sentimentNegative sentimentNeutral sentimentPositive 0 -1 0.500739 0.419327 0.079934 1 -1 0.600082 0.345853 0.054064 2 -1 0.450049 0.295671 0.254280 3 -1 0.752917 0.162715 0.084368 4 -1 0.699274 0.209360 0.091366 <pre><code>    # Barplot on negative, neutral and positive columns.\nnews_data[['sentimentNegative', 'sentimentNeutral','sentimentPositive']].mean().plot(kind='bar')\nplt.title(\"News positivity chart\")\nplt.show()\n</code></pre> <p></p> <pre><code>news_data['delay_time']=news_data['time']-news_data['firstCreated']\n</code></pre> <pre><code>news_data['time']=news_data['time'].dt.date\n</code></pre> <pre><code>def plot_vs_time(data_frame, column, calculation='mean', span=10):\n    if calculation == 'mean':\n        group_temp = data_frame.groupby('time')[column].mean().reset_index()\n    if calculation == 'count':\n        group_temp = data_frame.groupby('time')[column].count().reset_index()\n    if calculation == 'nunique':\n        group_temp = data_frame.groupby('time')[column].nunique().reset_index()\n    fig = plt.figure(figsize=(10,3))\n    plt.plot(group_temp['time'], group_temp[column])\n    plt.xlabel('Time')\n    plt.ylabel(column)\n    plt.title('%s versus time' %column)\n</code></pre> <pre><code>plot_vs_time(news_data, 'sourceId', calculation='count', span=10)\nplt.title('News count vs time')\nplt.ylabel('Count')\n</code></pre> <pre><code>Text(0, 0.5, 'Count')\n</code></pre> <p></p> <pre><code>provider_count = news_data.groupby('provider')['sourceId'].count()\n</code></pre> <pre><code>provider_sort = provider_count.sort_values(ascending= False)\nprovider_sort[:10].plot.barh()\nplt.xlabel('Count')\nplt.ylabel('Provider')\nplt.title('Top 10 news provider')\nplt.gca().invert_yaxis()\ndel provider_count\n</code></pre> <p></p> <pre><code>sentimentWordRatio = news_data.groupby('sentimentWordCount')['relevance'].mean()\nplt.plot(sentimentWordRatio)\nplt.xlim(0,2000)\nplt.ylabel('Relevance')\nplt.xlabel('Sentiment word count')\nplt.title('Sentiment word count and relevance')\ndel sentimentWordRatio\n</code></pre> <p></p> <pre><code>for i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n    df_sentiment = news_data.loc[news_data['sentimentClass'] == i, 'assetName']\n    print(f'Top mentioned companies for {j} sentiment are:')\n    print(df_sentiment.value_counts().head(5))\n    print('')\n</code></pre> <pre><code>Top mentioned companies for negative sentiment are:\nJPMorgan Chase &amp; Co        24262\nApple Inc                  24148\nBank of America Corp       23007\nCitigroup Inc              22421\nGoldman Sachs Group Inc    20570\nName: assetName, dtype: int64\n\nTop mentioned companies for neutral sentiment are:\nHSBC Holdings PLC    20799\nBarclays PLC         15133\nDeutsche Bank AG     14840\nCredit Suisse AG     14632\nApple Inc            11921\nName: assetName, dtype: int64\n\nTop mentioned companies for positive sentiment are:\nApple Inc                20533\nBarclays PLC             19561\nGeneral Electric Co      16555\nRoyal Dutch Shell PLC    16369\nBoeing Co                15289\nName: assetName, dtype: int64\n</code></pre> <pre><code>news_data['headline']=news_data['headline'].fillna('')\n</code></pre> <pre><code>news_data['headline'].map(len).mean()\n</code></pre> <pre><code>77.71627344075407\n</code></pre> <pre><code>news_data['headline'].map(len).max()\n</code></pre> <pre><code>272\n</code></pre> <pre><code>news_data['headline'].map(len).min()\n</code></pre> <pre><code>0\n</code></pre> <pre><code>news_data[news_data.headline.map(len)==0]\n</code></pre> time sourceTimestamp firstCreated sourceId headline urgency takeSequence provider subjects audiences ... noveltyCount24H noveltyCount3D noveltyCount5D noveltyCount7D volumeCounts12H volumeCounts24H volumeCounts3D volumeCounts5D volumeCounts7D delay_time 1521402 2009-01-02 11:11:26 2009-01-02 11:11:26 2009-01-02 11:10:18 f704b7195d2630f4 3 1 RTRS {'FINS', 'US', 'BNK', 'RTRS', 'BSVC', 'RCH', '... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:01:08 1521447 2009-01-02 12:29:28 2009-01-02 12:29:28 2009-01-02 12:29:11 a2b08707144a2e70 3 1 RTRS {'CHEM', 'BMAT', 'US', 'COMC', 'CHE', 'RCH', '... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 56 176 176 00:00:17 1521462 2009-01-02 12:53:00 2009-01-02 12:53:00 2009-01-02 12:50:38 905209eb39d3b19c 3 1 RTRS {'EUROPE', 'ELC', 'WEU', 'TECH', 'US', 'SEMI',... {'UKI', 'PSC', 'U', 'NAW', 'E'} ... 0 0 0 0 2 2 2 2 2 00:02:22 1521463 2009-01-02 12:53:00 2009-01-02 12:53:00 2009-01-02 12:50:38 905209eb39d3b19c 3 1 RTRS {'EUROPE', 'ELC', 'WEU', 'TECH', 'US', 'SEMI',... {'UKI', 'PSC', 'U', 'NAW', 'E'} ... 0 0 0 0 1 1 1 1 1 00:02:22 1521601 2009-01-02 14:36:07 2009-01-02 14:36:07 2009-01-02 14:35:38 a5db1545921197df 3 1 RTRS {'SWIT', 'US', 'TECH', 'SOFW', 'RTRS', 'RCH', ... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:00:29 1521602 2009-01-02 14:38:22 2009-01-02 14:38:22 2009-01-02 14:37:48 7ca50e16d4a0ca9a 3 1 RTRS {'CMSS', 'MAC', 'INDS', 'US', 'ENVS', 'ISER', ... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 5 5 5 00:00:34 1522405 2009-01-05 06:17:11 2009-01-05 06:17:11 2009-01-05 06:16:54 c0ae3e6cd77ab29e 3 1 RTRS {'EUROPE', 'WEU', 'CH', 'FINS', 'INVS', 'BNK',... {'E', 'PSC'} ... 0 0 0 0 3 3 3 20 36 00:00:17 1522480 2009-01-05 08:33:19 2009-01-05 08:33:19 2009-01-05 08:09:04 15202c85fcd07c00 3 1 RTRS {'EUROPE', 'GB', 'MEMI', 'WEU', 'BMAT', 'MINE'... {'UKI', 'E', 'PSC'} ... 0 0 0 0 2 2 2 6 22 00:24:15 1522491 2009-01-05 09:08:24 2009-01-05 09:08:24 2009-01-05 09:07:58 2e2ec98f85fe15e2 3 1 RTRS {'EUROPE', 'WEU', 'FINS', 'BNK', 'RTRS', 'BSVC... {'UKI', 'E', 'PSC', 'U'} ... 0 0 0 0 7 8 14 28 39 00:00:26 1522619 2009-01-05 11:28:38 2009-01-05 11:28:38 2009-01-05 11:28:08 47506cf9f09aebd4 3 1 RTRS {'FINS', 'US', 'BNK', 'RTRS', 'BSVC', 'RCH', '... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 8 8 10 00:00:30 1522620 2009-01-05 11:28:53 2009-01-05 11:28:53 2009-01-05 10:55:18 da281f1d5d756c9f 3 1 RTRS {'WLES', 'US', 'TCOM', 'TEL', 'RTRS', 'RCH', '... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 2 2 2 3 3 00:33:35 1522621 2009-01-05 11:29:08 2009-01-05 11:29:08 2009-01-05 11:25:09 e52777aa6e613cb0 3 1 RTRS {'PHAD', 'HECA', 'US', 'CHE', 'RCH', 'RTRS', '... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:03:59 1522622 2009-01-05 11:29:23 2009-01-05 11:29:23 2009-01-05 11:25:53 f068551abd8b14c5 3 1 RTRS {'MACH', 'INDS', 'INDG', 'APL', 'US', 'RTRS', ... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:03:30 1522623 2009-01-05 11:29:36 2009-01-05 11:29:36 2009-01-05 11:26:32 8696504dd5f62925 3 1 RTRS {'CMSS', 'BUS', 'BSUP', 'INDS', 'US', 'ISER', ... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:03:04 1522654 2009-01-05 11:48:37 2009-01-05 11:48:37 2009-01-05 11:48:21 6fd2d146ece5f914 3 1 RTRS {'BIOT', 'HECA', 'DRU', 'US', 'RTRS', 'RCH', '... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:00:16 1522657 2009-01-05 11:58:00 2009-01-05 11:58:00 2009-01-05 11:56:22 e21d042a26117ecf 3 1 RTRS {'SPEC', 'CHEM', 'BMAT', 'US', 'CHE', 'RCH', '... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:01:38 1522775 2009-01-05 12:26:27 2009-01-05 12:26:27 2009-01-05 12:22:32 ca9c0f30a00488b0 3 1 RTRS {'BIOT', 'HECA', 'DRU', 'US', 'CHE', 'RCH', 'R... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:03:55 1522785 2009-01-05 12:31:35 2009-01-05 12:31:35 2009-01-05 12:30:22 fdca736d395e2267 3 1 RTRS {'ENT', 'PHON', 'WLES', 'US', 'TCOM', 'TEL', '... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 2 2 2 2 3 00:01:13 1522786 2009-01-05 12:31:35 2009-01-05 12:31:35 2009-01-05 12:30:22 fdca736d395e2267 3 1 RTRS {'ENT', 'PHON', 'WLES', 'US', 'TCOM', 'TEL', '... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 3 19 31 00:01:13 1522902 2009-01-05 13:08:52 2009-01-05 13:08:52 2009-01-05 13:08:36 9176994e470abcd4 3 1 RTRS {'SWIT', 'ITSE', 'BUS', 'US', 'TECH', 'RTRS', ... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 2 2 00:00:16 1522903 2009-01-05 13:09:34 2009-01-05 13:09:34 2009-01-05 13:08:46 16f5391544a74d3f 3 1 RTRS {'LEN', 'TECH', 'US', 'SEMI', 'CHPT', 'RTRS', ... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:00:48 1522904 2009-01-05 13:09:44 2009-01-05 13:09:44 2009-01-05 13:08:26 3c711535ae44784c 3 1 RTRS {'LEN', 'TECH', 'US', 'SEMI', 'CHPT', 'RTRS', ... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:01:18 1522906 2009-01-05 13:11:27 2009-01-05 13:11:27 2009-01-05 13:06:29 475b7ebf1dcb330a 3 1 RTRS {'FIN', 'INVT', 'FINS', 'US', 'RTRS', 'RCH', '... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 16 16 00:04:58 1522910 2009-01-05 13:13:47 2009-01-05 13:13:47 2009-01-05 13:08:03 a4043c683b6ed121 3 1 RTRS {'FOOD', 'FOBE', 'BEV', 'FOD', 'US', 'RTRS', '... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:05:44 1522915 2009-01-05 13:20:43 2009-01-05 13:20:43 2009-01-05 13:20:18 419634edaaf034bc 3 1 RTRS {'SWIT', 'BUS', 'US', 'TECH', 'SOFW', 'RTRS', ... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 4 4 00:00:25 1522916 2009-01-05 13:20:55 2009-01-05 13:20:55 2009-01-05 13:19:18 5dcec274008b4723 3 1 RTRS {'HECA', 'DRU', 'PHAG', 'US', 'RTRS', 'RCH', '... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:01:37 1522925 2009-01-05 13:25:54 2009-01-05 13:25:54 2009-01-05 13:25:10 a1e9e0e197098aef 3 1 RTRS {'RET', 'CYCS', 'SHOP', 'US', 'DEPT', 'RTRS', ... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 2 3 4 4 4 00:00:44 1522926 2009-01-05 13:26:33 2009-01-05 13:26:33 2009-01-05 13:26:18 f067c0bdca248b4a 3 1 RTRS {'RETE', 'RET', 'CYCS', 'SHOP', 'US', 'RTRS', ... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 1 1 1 00:00:15 1522959 2009-01-05 13:31:19 2009-01-05 13:31:19 2009-01-05 13:27:45 f02704fe2c2e805b 3 1 RTRS {'FUND', 'MTG', 'BANK', 'DBT', 'RESF', 'BACT',... {'T', 'PSC', 'U', 'D', 'M', 'NAW', 'E', 'NAT'} ... 0 0 0 0 10 10 14 58 67 00:03:34 1523020 2009-01-05 13:38:53 2009-01-05 13:38:53 2009-01-05 13:38:26 1d012ec7114973bb 3 1 RTRS {'RET', 'RETS', 'CYCS', 'SHOP', 'US', 'RTRS', ... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 1 1 10 10 11 00:00:27 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 6053506 2013-10-23 16:23:49 2013-10-23 16:23:48 2013-10-23 16:23:48 0b0f170e7621edd0 3 1 MKW {'LEN', 'FINS', 'US', 'BNK', 'CMPNY', 'AMERS',... {'CNR', 'MKW'} ... 0 0 0 0 0 0 0 1 1 00:00:01 6076071 2013-10-29 21:58:10 2013-10-29 21:58:09 2013-10-29 21:58:09 7150eb23d9d0c955 3 1 MKW {'RET', 'CYCS', 'RETA', 'SHOP', 'US', 'RSPC', ... {'CNR', 'MKW'} ... 0 0 0 1 0 0 0 1 2 00:00:01 6076081 2013-10-29 21:59:06 2013-10-29 21:59:06 2013-10-29 21:59:06 d6e7741677893d7c 3 1 MKW {'HPRD', 'HECA', 'HLTH', 'US', 'CMPNY', 'AMED'... {'CNR', 'MKW'} ... 0 0 0 0 0 0 0 0 2 00:00:00 6076206 2013-10-29 22:32:52 2013-10-29 22:32:52 2013-10-29 22:32:52 ef38f016d04280f1 3 1 MKW {'RET', 'CYCS', 'RGEN', 'SHOP', 'US', 'DEPT', ... {'CNR', 'MKW'} ... 0 0 0 0 5 5 13 21 23 00:00:00 6086754 2013-10-31 19:00:50 2013-10-31 19:00:50 2013-10-31 19:00:50 8ac30d9256e5c9a9 3 1 MKW {'US', 'AMERS', 'LEN'} {'CNR', 'MKW'} ... 4 5 5 5 3 7 13 14 17 00:00:00 6147648 2013-11-19 03:00:00 2013-11-19 03:00:00 2013-11-19 03:00:00 574cba1408fba1a1 3 1 MKW {'CYCP', 'CYCS', 'HBLD', 'BLD', 'US', 'CONS', ... {'CNR', 'MKW'} ... 0 0 0 0 2 3 3 37 37 00:00:00 6155144 2013-11-21 03:00:00 2013-11-21 03:00:00 2013-11-21 03:00:00 17e644fa418bc053 3 1 MKW {'LEN', 'FINS', 'COFS', 'US', 'CMPNY', 'BNK', ... {'CNR', 'MKW'} ... 0 0 0 0 3 3 3 3 11 00:00:00 6161193 2013-11-22 16:34:47 2013-11-22 16:34:47 2013-11-22 16:34:47 1e7fe1b2d97a6bc5 3 1 MKW {'CYCP', 'CYCS', 'HBLD', 'BLD', 'US', 'CONS', ... {'CNR', 'MKW'} ... 1 1 3 3 0 2 4 9 12 00:00:00 6162343 2013-11-23 03:00:00 2013-11-23 03:00:00 2013-11-23 03:00:00 075905b30ad8de69 3 1 MKW {'CYCS', 'US', 'ADVT', 'CMPNY', 'MDIA', 'CCOS'... {'CNR', 'MKW'} ... 0 0 0 0 2 2 3 4 4 00:00:00 6162345 2013-11-23 03:00:00 2013-11-23 03:00:00 2013-11-23 03:00:00 d4317379cfaf8855 3 1 MKW {'SWIT', 'ITSE', 'OFCE', 'COMP', 'TECH', 'US',... {'CNR', 'MKW'} ... 0 0 0 0 1 1 5 6 6 00:00:00 6172086 2013-11-28 03:00:00 2013-11-28 03:00:00 2013-11-28 03:00:00 1f04e8520c872acf 3 1 MKW {'COMP', 'TECH', 'US', 'CMPNY', 'TEEQ', 'AMERS... {'CNR', 'MKW'} ... 0 0 0 0 2 2 4 5 40 00:00:00 6174051 2013-11-29 18:00:00 2013-11-29 18:00:00 2013-11-29 18:00:00 bdf1e56e37848a92 3 1 MKW {'TRAN', 'SHP', 'INDS', 'US', 'SHIP', 'CMPNY',... {'CNR', 'MKW'} ... 0 0 0 0 0 0 0 8 8 00:00:00 6176722 2013-12-02 16:26:54 2013-12-02 16:26:53 2013-12-02 16:26:53 4ebdd3421cebf68c 3 1 MKW {'CYCS', 'US', 'ADVT', 'CMPNY', 'MDIA', 'CCOS'... {'CNR', 'MKW'} ... 0 0 2 2 0 0 0 5 5 00:00:01 6176727 2013-12-02 16:27:58 2013-12-02 16:27:58 2013-12-02 16:27:58 47162aed09e8a7ba 3 1 MKW {'COMP', 'TECH', 'US', 'CMPNY', 'TEEQ', 'AMERS... {'CNR', 'MKW'} ... 0 0 2 2 0 0 0 4 6 00:00:00 6176949 2013-12-02 17:40:40 2013-12-02 17:40:40 2013-12-02 17:40:40 b63075dbf8bb4965 3 1 MKW {'ENTS', 'RORA', 'TRAN', 'CYCS', 'INDS', 'LEI'... {'CNR', 'MKW'} ... 0 0 0 0 0 0 0 1 6 00:00:00 6177589 2013-12-02 21:05:06 2013-12-02 21:05:05 2013-12-02 21:05:05 81ebd3761914804c 3 1 MKW {'HECA', 'PHMR', 'MRCH', 'DRU', 'US', 'CMPNY',... {'CNR', 'MKW'} ... 0 0 0 0 0 0 0 0 1 00:00:01 6185072 2013-12-04 18:18:11 2013-12-04 18:18:11 2013-12-04 18:18:11 ebeb9de2a1b6d774 3 1 MKW {'FIN', 'WLES', 'FINS', 'US', 'TCOM', 'CMPNY',... {'CNR', 'MKW'} ... 0 0 0 0 0 0 0 0 4 00:00:00 6206146 2013-12-12 17:28:23 2013-12-12 17:28:23 2013-12-12 17:28:23 17350a38692ccab6 3 1 MKW {'HECA', 'PHMR', 'MRCH', 'DRU', 'INDS', 'US', ... {'CNR', 'MKW'} ... 0 0 0 0 0 0 2 2 3 00:00:00 6218155 2013-12-17 23:22:56 2013-12-17 23:22:56 2013-12-17 23:22:56 5476f3c69374adee 3 1 MKW {'MACH', 'MIN', 'ELCO', 'BMAT', 'INDS', 'INDG'... {'CNR', 'MKW'} ... 0 0 0 1 0 0 0 2 3 00:00:00 6218343 2013-12-18 03:00:00 2013-12-18 03:00:00 2013-12-18 03:00:00 6a24362c5897933e 3 1 MKW {'MACH', 'MIN', 'ELCO', 'BMAT', 'INDS', 'INDG'... {'CNR', 'MKW'} ... 0 0 0 0 2 2 2 4 5 00:00:00 6228369 2013-12-20 18:20:02 2013-12-20 18:20:02 2013-12-20 18:20:02 bbe40e19969a0a85 3 1 MKW {'OILG', 'ENR', 'OGTR', 'ENER', 'US', 'ELG', '... {'CNR', 'MKW'} ... 0 0 0 0 0 7 9 12 12 00:00:00 6232636 2013-12-24 17:02:16 2013-12-24 17:02:16 2013-12-24 17:02:16 f2c906e76c605240 3 1 MKW {'CMSS', 'CYCS', 'BUS', 'BSUP', 'INDS', 'US', ... {'CNR', 'MKW'} ... 0 0 0 0 0 0 0 0 0 00:00:00 6232637 2013-12-24 17:02:16 2013-12-24 17:02:16 2013-12-24 17:02:16 f2c906e76c605240 3 1 MKW {'CMSS', 'CYCS', 'BUS', 'BSUP', 'INDS', 'US', ... {'CNR', 'MKW'} ... 0 0 0 0 0 0 0 5 12 00:00:00 6232771 2013-12-24 20:24:53 2013-12-24 20:24:53 2013-12-24 20:24:53 6d5044b14207ed0b 3 1 MKW {'CHEM', 'HECA', 'PHMR', 'MRCH', 'BMAT', 'DRU'... {'CNR', 'MKW'} ... 0 0 0 0 0 0 0 0 0 00:00:00 6232778 2013-12-24 20:30:02 2013-12-24 20:30:02 2013-12-24 20:30:02 d80acc871f6ef6cf 3 1 MKW {'AUTO', 'CYCS', 'US', 'AUT', 'CMPNY', 'CARM',... {'CNR', 'MKW'} ... 0 0 2 2 4 5 8 11 30 00:00:00 6249966 2014-01-08 12:00:02 2014-01-08 12:00:01 2014-01-08 12:00:01 2c23550af6de94ef 3 1 MKW {'CMSS', 'BUS', 'INVT', 'BSUP', 'INDS', 'FINS'... {'CNR', 'MKW'} ... 1 1 1 1 1 1 1 1 1 00:00:01 6251660 2014-01-08 16:35:28 2014-01-08 16:35:28 2014-01-08 16:35:28 b8619613aecff939 3 1 MKW {'SWIT', 'ITSE', 'TECH', 'US', 'CMPNY', 'AMERS... {'CNR', 'MKW'} ... 0 1 1 2 0 0 1 1 3 00:00:00 6281987 2014-01-17 18:33:26 2014-01-17 18:33:25 2014-01-17 18:33:25 872e13f65f358130 3 1 MKW {'MACH', 'STEE', 'MIN', 'BMAT', 'INDS', 'INDG'... {'CNR', 'MKW'} ... 0 0 1 1 1 1 2 5 6 00:00:01 6283977 2014-01-20 15:51:35 2014-01-20 15:51:35 2014-01-20 15:51:35 14ba0663fd91df60 3 1 MKW {'MEDQ', 'HPRD', 'INS', 'HECA', 'INSR', 'PHMR'... {'CNR', 'MKW'} ... 0 0 0 0 0 0 0 2 9 00:00:00 6291391 2014-01-22 18:30:00 2014-01-22 18:30:00 2014-01-22 18:30:00 ca018b238c26789a 3 1 MKW {'LEN', 'FINS', 'US', 'BNK', 'CMPNY', 'AMERS',... {'CNR', 'MKW'} ... 0 0 0 0 0 0 0 0 0 00:00:00 <p>37530 rows \u00d7 36 columns</p> <pre><code>news_data[news_data.headline.map(len)==272]\n</code></pre> time sourceTimestamp firstCreated sourceId headline urgency takeSequence provider subjects audiences ... noveltyCount24H noveltyCount3D noveltyCount5D noveltyCount7D volumeCounts12H volumeCounts24H volumeCounts3D volumeCounts5D volumeCounts7D delay_time 3086030 2010-10-27 15:48:01 2010-10-27 15:48:01 2010-10-27 15:48:01 3511350c6faaed0a PRESS RELEASE - CHICAGO-BASED \\\\\"JAM THEATRICA... 1 1 RTRS {'RORA', 'TRAN', 'RRLF', 'INDS', 'US', 'RTRS',... {'E', 'U'} ... 0 0 0 0 7 7 7 7 7 0 days 3086404 2010-10-27 17:59:00 2010-10-27 17:59:00 2010-10-27 17:59:00 d19099a829f8c69d PRESS RELEASE - CHICAGO-BASED \\\\\"JAM THEATRICA... 1 1 RTRS {'RORA', 'TRAN', 'RRLF', 'INDS', 'US', 'RTRS',... {'E', 'U'} ... 0 0 0 0 8 8 8 8 8 0 days <p>2 rows \u00d7 36 columns</p> <pre><code>sns.distplot(news_data.headline.map(len))\n</code></pre> <pre><code>C:\\Users\\chinn\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n\n\n\n\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x2190e9a45c0&gt;\n</code></pre> <p></p> <pre><code>news_data.bodySize.min()\n</code></pre> <pre><code>0\n</code></pre> <pre><code>news_data.bodySize.max()\n</code></pre> <pre><code>122770\n</code></pre> <pre><code>sns.distplot(news_data.bodySize)\n</code></pre> <pre><code>KeyboardInterrupt\n</code></pre> <p></p> <pre><code>news_data[news_data.companyCount==news_data.companyCount.max()].head()\n</code></pre> time sourceTimestamp firstCreated sourceId headline urgency takeSequence provider subjects audiences ... noveltyCount24H noveltyCount3D noveltyCount5D noveltyCount7D volumeCounts12H volumeCounts24H volumeCounts3D volumeCounts5D volumeCounts7D delay_time 5951141 2013-09-12 2013-09-12 10:13:18 2013-09-12 10:13:18 d6de134f025a1ace U.S. RESEARCH ROUNDUP: MasterCard, Morgan Stan... 3 1 RTRS {'BLR', 'PPRO', 'INVB', 'FOBE', 'RUBB', 'HECA'... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 4 4 4 4 4 00:00:01 5951142 2013-09-12 2013-09-12 10:13:18 2013-09-12 10:13:18 d6de134f025a1ace U.S. RESEARCH ROUNDUP: MasterCard, Morgan Stan... 3 1 RTRS {'BLR', 'PPRO', 'INVB', 'FOBE', 'RUBB', 'HECA'... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 19 19 26 26 26 00:00:01 5951143 2013-09-12 2013-09-12 10:13:18 2013-09-12 10:13:18 d6de134f025a1ace U.S. RESEARCH ROUNDUP: MasterCard, Morgan Stan... 3 1 RTRS {'BLR', 'PPRO', 'INVB', 'FOBE', 'RUBB', 'HECA'... {'E', 'PSC', 'NAW', 'U'} ... 0 0 0 0 3 3 10 10 10 00:00:01 5951144 2013-09-12 2013-09-12 10:13:18 2013-09-12 10:13:18 d6de134f025a1ace U.S. RESEARCH ROUNDUP: MasterCard, Morgan Stan... 3 1 RTRS {'BLR', 'PPRO', 'INVB', 'FOBE', 'RUBB', 'HECA'... {'E', 'PSC', 'NAW', 'U'} ... 1 1 1 1 3 6 6 6 6 00:00:01 5951145 2013-09-12 2013-09-12 10:13:18 2013-09-12 10:13:18 d6de134f025a1ace U.S. RESEARCH ROUNDUP: MasterCard, Morgan Stan... 3 1 RTRS {'BLR', 'PPRO', 'INVB', 'FOBE', 'RUBB', 'HECA'... {'E', 'PSC', 'NAW', 'U'} ... 3 3 3 3 3 7 23 23 24 00:00:01 <p>5 rows \u00d7 36 columns</p> <pre><code>news_data['subjects'].head()\n</code></pre> <pre><code>1521194    {'ENTS', 'EQUE', 'CYCS', 'NEWR', 'LEI', 'RELX'...\n1521195    {'CYCP', 'RET', 'CYCS', 'ASIA', 'HK', 'NEWR', ...\n1521196    {'COEN', 'INDS', 'SG', 'RTRS', 'EMRG', 'AUTO',...\n1521197    {'COEN', 'INDS', 'SG', 'RTRS', 'EMRG', 'AUTO',...\n1521198    {'FUND', 'AUTO', 'CYCS', 'DBT', 'NEWS', 'WASH'...\nName: subjects, dtype: category\nCategories (1733963, object): [{'AAA', 'ABS', 'BANK', 'DBT', 'CEEU', 'WEU', '..., {'AAA', 'ABS', 'DBT', 'CEEU', 'WEU', 'CH', 'LE..., {'AAA', 'ABS', 'DBT', 'DE', 'CEEU', 'WEU', 'CD..., {'AAA', 'AFR', 'DBT', 'WEU', 'ZA', 'IGD', 'LEN..., ..., {'ZNC', 'MEMI', 'ASIA', 'BASMTL', 'MIN', 'COM'..., {'ZNC', 'MEMI', 'AU', 'ASIA', 'COM', 'BASMTL',..., {'ZNC', 'MEMI', 'AU', 'ASIA', 'COM', 'BASMTL',..., {'ZNC', 'RTRS', 'MCE', 'ASIA', 'COM', 'BASMTL'...]\n</code></pre> <pre><code>news_data['subjects'] = news_data['subjects'].apply(lambda x: x[1:-1].replace(\"'\", \"\"))\n</code></pre> <pre><code>news_data['subjects']=news_data['subjects'].str.split(\",\")\n</code></pre> <pre><code>columns_corr = ['urgency', 'takeSequence', 'companyCount','marketCommentary','sentenceCount',\\\n           'firstMentionSentence','relevance','sentimentClass','sentimentWordCount','noveltyCount24H',\\\n           'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(news_data[columns_corr].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')\n</code></pre> <pre><code>Text(0.5, 1.0, 'Pair-wise correlation')\n</code></pre> <p></p> <pre><code>news_data['headlinelength']=news_data['headline'].map(len)\n</code></pre> <pre><code>dropfeatures=['sourceTimestamp','firstCreated','sourceId','headline','takeSequence','provider','subjects','audiences','headlineTag','volumeCounts7D','volumeCounts5D','volumeCounts24H','noveltyCount7D','noveltyCount5D','noveltyCount24H']\n</code></pre> <pre><code>news_data.drop(dropfeatures, axis=1, inplace=True)\n</code></pre> <pre><code>news_data.to_csv('newsdatapreprocessed.csv')\n</code></pre>"}]}